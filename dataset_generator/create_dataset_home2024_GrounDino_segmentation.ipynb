{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset for @HOME2024 using GroundingDino and SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jabv/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageEnhance, ImageFilter, ImageFont\n",
    "from pycocotools import mask\n",
    "import json\n",
    "import yaml\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ultralytics\n",
    "import time\n",
    "import imutils\n",
    "import argparse\n",
    "\n",
    "#grounding imports----------------\n",
    "\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.vl_utils import create_positive_map_from_span\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize images in a folder to a specific size\n",
    "\n",
    "pathtofiles = \"/home/jabv/Desktop/prueba/imgs/\"\n",
    "pathtoimage = [f.path for f in os.scandir(pathtofiles) if f.is_dir()]\n",
    "size = 720\n",
    "\n",
    "#pathtoimage = os.listdir(pathtofiles)\n",
    "\n",
    "for filepath in pathtoimage:\n",
    "    folder = filepath + \"/\"\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(folder + filename)\n",
    "        img = img.resize((size, size))\n",
    "        img.save(folder + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto label con Segment anyting y modelo de YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manzana1.jpeg', 'manzana2.jpeg']\n",
      "Processing image: manzana1.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jabv/.local/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jabv/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/jabv/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box: 269, 319, 342, 417\n",
      "Bounding box: 338, 224, 405, 309\n",
      "File already exists, saving with _1\n",
      "Bounding box: 364, 319, 438, 418\n",
      "File already exists, saving with _2\n",
      "Processing image: manzana2.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jabv/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/jabv/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box: 371, 345, 448, 449\n",
      "Bounding box: 264, 407, 347, 502\n",
      "File already exists, saving with _1\n",
      "Bounding box: 292, 290, 367, 380\n",
      "File already exists, saving with _2\n",
      "['platano1.jpeg', 'platano2.jpeg', 'platano3.jpeg']\n",
      "Processing image: platano1.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 267, 325, 349, 520\n",
      "Bounding box: 315, 282, 440, 421\n",
      "File already exists, saving with _1\n",
      "Processing image: platano2.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 235, 308, 413, 621\n",
      "Bounding box: 392, 225, 503, 521\n",
      "File already exists, saving with _1\n",
      "Processing image: platano3.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 317, 360, 416, 467\n",
      "Bounding box: 345, 484, 478, 577\n",
      "File already exists, saving with _1\n",
      "['silla.jpg']\n",
      "Processing image: silla.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 498, 120, 676, 631\n",
      "Bounding box: 47, 121, 224, 631\n",
      "File already exists, saving with _1\n",
      "Bounding box: 349, 120, 526, 631\n",
      "File already exists, saving with _2\n",
      "Bounding box: 45, 115, 680, 634\n",
      "File already exists, saving with _2\n",
      "Bounding box: 198, 120, 375, 631\n",
      "File already exists, saving with _2\n",
      "['jarra3.jpeg', 'jarra2.jpeg', 'jarra1.jpeg']\n",
      "Processing image: jarra3.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 112, 468, 278, 704\n",
      "Processing image: jarra2.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 421, 409, 580, 620\n",
      "Processing image: jarra1.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 297, 107, 434, 327\n",
      "Bounding box: 0, 105, 593, 720\n",
      "File already exists, saving with _1\n",
      "['gatos.jpeg']\n",
      "Processing image: gatos.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 322, 62, 502, 675\n",
      "Bounding box: 0, 213, 90, 677\n",
      "File already exists, saving with _1\n",
      "Bounding box: 620, 187, 720, 678\n",
      "File already exists, saving with _2\n",
      "Bounding box: 65, 150, 228, 676\n",
      "File already exists, saving with _2\n",
      "Bounding box: 183, 106, 357, 679\n",
      "File already exists, saving with _2\n",
      "Bounding box: 477, 141, 649, 679\n",
      "File already exists, saving with _2\n",
      "['lata.jpg']\n",
      "Processing image: lata.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 175, 4, 547, 711\n"
     ]
    }
   ],
   "source": [
    "pathtofiles = \"/home/jabv/Desktop/prueba/imgs/\" #path to images to process\n",
    "resultspath = \"/home/jabv/Desktop/prueba/prueba_precut\" #path to save results all ready processed and segmented images\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n",
    "\n",
    "def load_image(image_path):\n",
    "\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")  # load image\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    image, _ = transform(image_pil, None)  # 3, h, w\n",
    "    return image_pil, image\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, cpu_only=False):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_grounding_output(model, image, caption, box_threshold, text_threshold=None, with_logits=True, cpu_only=False, token_spans=None):\n",
    "    assert text_threshold is not None or token_spans is not None, \"text_threshould and token_spans should not be None at the same time!\"\n",
    "    caption = caption.lower()\n",
    "    caption = caption.strip()\n",
    "    if not caption.endswith(\".\"):\n",
    "        caption = caption + \".\"\n",
    "    device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        print(\"Running model...\")\n",
    "        outputs = model(image[None], captions=[caption])\n",
    "    logits = outputs[\"pred_logits\"].sigmoid()[0]  # (nq, 256)\n",
    "    boxes = outputs[\"pred_boxes\"][0]  # (nq, 4)\n",
    "\n",
    "    # filter output\n",
    "    if token_spans is None:\n",
    "        logits_filt = logits.cpu().clone()\n",
    "        boxes_filt = boxes.cpu().clone()\n",
    "        filt_mask = logits_filt.max(dim=1)[0] > box_threshold\n",
    "        logits_filt = logits_filt[filt_mask]  # num_filt, 256\n",
    "        boxes_filt = boxes_filt[filt_mask]  # num_filt, 4\n",
    "\n",
    "        # get phrase\n",
    "        tokenlizer = model.tokenizer\n",
    "        tokenized = tokenlizer(caption)\n",
    "        # build pred\n",
    "        pred_phrases = []\n",
    "        for logit, box in zip(logits_filt, boxes_filt):\n",
    "            pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenlizer)\n",
    "            if with_logits:\n",
    "                pred_phrases.append(pred_phrase + f\"({str(logit.max().item())[:4]})\")\n",
    "            else:\n",
    "                pred_phrases.append(pred_phrase)\n",
    "    else:\n",
    "        # given-phrase mode\n",
    "        positive_maps = create_positive_map_from_span(\n",
    "            model.tokenizer(text_prompt),\n",
    "            token_span=token_spans\n",
    "        ).to(image.device) # n_phrase, 256\n",
    "\n",
    "        logits_for_phrases = positive_maps @ logits.T # n_phrase, nq\n",
    "        all_logits = []\n",
    "        all_phrases = []\n",
    "        all_boxes = []\n",
    "        for (token_span, logit_phr) in zip(token_spans, logits_for_phrases):\n",
    "            # get phrase\n",
    "            phrase = ' '.join([caption[_s:_e] for (_s, _e) in token_span])\n",
    "            # get mask\n",
    "            filt_mask = logit_phr > box_threshold\n",
    "            # filt box\n",
    "            all_boxes.append(boxes[filt_mask])\n",
    "            # filt logits\n",
    "            all_logits.append(logit_phr[filt_mask])\n",
    "            if with_logits:\n",
    "                logit_phr_num = logit_phr[filt_mask]\n",
    "                all_phrases.extend([phrase + f\"({str(logit.item())[:4]})\" for logit in logit_phr_num])\n",
    "            else:\n",
    "                all_phrases.extend([phrase for _ in range(len(filt_mask))])\n",
    "        boxes_filt = torch.cat(all_boxes, dim=0).cpu()\n",
    "        pred_phrases = all_phrases\n",
    "\n",
    "\n",
    "    return boxes_filt, pred_phrases\n",
    "\n",
    "# cfg\n",
    "config_file = \"/home/jabv/Desktop/home-vision/dataset_generator/groundingdino/config/GroundingDINO_SwinT_OGC.py\"  # change the path of the model config file\n",
    "\n",
    "#wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
    "checkpoint_path = \"/home/jabv/Desktop/home-vision/dataset_generator/groundingdino_swint_ogc.pth\"  # change the path of the model\n",
    "text_prompt = \"bannana\"\n",
    "output_dir = resultspath\n",
    "box_threshold = 0.3\n",
    "text_threshold = 0.25\n",
    "token_spans = None\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "sam_model = \"h\"\n",
    "\n",
    "#use sam model\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
    "if sam_model ==\"h\":\n",
    "  sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "  model_type = \"vit_h\"\n",
    "else:\n",
    "  sam_checkpoint = \"sam_vit_l_0b3195.pth\"\n",
    "  model_type = \"vit_l\"\n",
    "\n",
    "device = \"cuda\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "images=[]\n",
    "annotations=[]\n",
    "categories=[]\n",
    "\n",
    "img_id=0\n",
    "anno_id=0\n",
    "\n",
    "#check if results directory exists, else create it\n",
    "if not os.path.exists(resultspath):\n",
    "  os.makedirs(resultspath)\n",
    "\n",
    "#make a list of all the directories in the path\n",
    "pathtoimage = [f.path for f in os.scandir(pathtofiles) if f.is_dir()]\n",
    "\n",
    "#pathtoimage = os.listdir(pathtofiles)\n",
    "\n",
    "for filepath in pathtoimage:\n",
    "    imgPaths = os.listdir(filepath)\n",
    "    print(imgPaths)\n",
    "\n",
    "    i=0\n",
    "\n",
    "    for imgPath in imgPaths:\n",
    "        print(f\"Processing image: {imgPath}\")\n",
    "        img = imutils.resize(cv2.imread(f\"{filepath}/{imgPath}\"))\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "    #------------------------start grounding----------------------------------------------\n",
    "        #image_path = args.image_path\n",
    "\n",
    "        # load image\n",
    "        image_pil, image = load_image(f\"{filepath}/{imgPath}\")\n",
    "\n",
    "        # load model\n",
    "        model = load_model(config_file, checkpoint_path, cpu_only=False)\n",
    "\n",
    "        # set the text_threshold to None if token_spans is set.\n",
    "        if token_spans is not None:\n",
    "            text_threshold = None\n",
    "            print(\"Using token_spans. Set the text_threshold to None.\")\n",
    "\n",
    "        # run model\n",
    "        boxes_filt, pred_phrases = get_grounding_output(\n",
    "            model, image, text_prompt, box_threshold, text_threshold, cpu_only=False, token_spans=eval(f\"{token_spans}\")\n",
    "        )\n",
    "\n",
    "        #found bb dimensions\n",
    "\n",
    "        size = image_pil.size\n",
    "        pred_dict = {\n",
    "            \"boxes\": boxes_filt,\n",
    "            \"size\": [size[1], size[0]],  # H,W\n",
    "            \"labels\": pred_phrases,\n",
    "        }\n",
    "\n",
    "        H, W = pred_dict[\"size\"]\n",
    "        boxes = pred_dict[\"boxes\"]\n",
    "        labels = pred_dict[\"labels\"]\n",
    "        assert len(boxes) == len(labels), \"boxes and labels must have same length\"\n",
    "\n",
    "        draw = ImageDraw.Draw(image_pil)\n",
    "        mask = Image.new(\"L\", image_pil.size, 0)\n",
    "        mask_draw = ImageDraw.Draw(mask)\n",
    "\n",
    "        #change pil image to cv2 image\n",
    "        img = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
    "        img2 = img.copy()\n",
    "        # draw boxes and masks\n",
    "        for box, label in zip(boxes, labels):\n",
    "            # from 0..1 to 0..W, 0..H\n",
    "            box = box * torch.Tensor([W, H, W, H])\n",
    "            # from xywh to xyxy\n",
    "            box[:2] -= box[2:] / 2\n",
    "            box[2:] += box[:2]\n",
    "            # random color\n",
    "            color = tuple(np.random.randint(0, 255, size=1).tolist())\n",
    "            # draw\n",
    "            padding = 10\n",
    "            x0, y0, x1, y1 = box\n",
    "            x0, y0, x1, y1 = int(x0)-padding, int(y0)-padding, int(x1)+padding, int(y1)+padding\n",
    "\n",
    "            #validate if the bounding box is inside the image\n",
    "            if x0 < 0:\n",
    "                x0 = 0\n",
    "            if y0 < 0:\n",
    "                y0 = 0\n",
    "            if x1 > W:\n",
    "                x1 = W\n",
    "            if y1 > H:\n",
    "                y1 = H\n",
    "                \n",
    "            #draw rectangles\n",
    "            cv2.rectangle(img2, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "            draw.rectangle([x0, y0, x1, y1], outline=color, width=6)\n",
    "            # draw.text((x0, y0), str(label), fill=color)\n",
    "\n",
    "            font = ImageFont.load_default()\n",
    "            if hasattr(font, \"getbbox\"):\n",
    "                bbox = draw.textbbox((x0, y0), str(label), font)\n",
    "            else:\n",
    "                w, h = draw.textsize(str(label), font)\n",
    "                bbox = (x0, y0, w + x0, y0 + h)\n",
    "            # bbox = draw.textbbox((x0, y0), str(label))\n",
    "            draw.rectangle(bbox, fill=color)\n",
    "            draw.text((x0, y0), str(label), fill=\"white\")\n",
    "\n",
    "            mask_draw.rectangle([x0, y0, x1, y1], fill=255, width=6)\n",
    "        \n",
    "    # ----------------End grounding ---------------------------------------------------------   \n",
    "        \n",
    "    # ----------------Start SAM--------------------------------------------------------------  \n",
    "            \n",
    "            class_name = filepath.split(\"/\")[-1]\n",
    "            #print x0, y0, x1, y1\n",
    "            print(f\"Bounding box: {x0}, {y0}, {x1}, {y1}\")\n",
    "\n",
    "            sam_bounding_box = np.array([x0, y0, x1, y1])\n",
    "            ran_sam = False\n",
    "            #run sam\n",
    "            if ran_sam == False:\n",
    "                predictor.set_image(img)\n",
    "                ran_sam = True\n",
    "\n",
    "            mask, _, _ = predictor.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=sam_bounding_box,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "\n",
    "            mask, _, _ = predictor.predict(box=sam_bounding_box, multimask_output=False)\n",
    "\n",
    "            #Make png mask\n",
    "            contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # Your call to find the contours\n",
    "\n",
    "            # threshold input image using otsu thresholding as mask and refine with morphology\n",
    "            ret, pngmask = cv2.threshold(mask[0].astype(np.uint8), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU) \n",
    "            kernel = np.ones((9,9), np.uint8)\n",
    "            pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_CLOSE, kernel)\n",
    "            pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_OPEN, kernel)\n",
    "            result = img.copy()\n",
    "            result = cv2.cvtColor(result, cv2.COLOR_BGR2BGRA)\n",
    "            result[:, :, 3] = pngmask                           \n",
    "\n",
    "    # ----------------End SAM-----------------------------------------------------------------  \n",
    "            #cv2.imwrite(f\"{resultspath}/groundingcv2_{imgPath}\", img2)\n",
    "\n",
    "            #image_pil.save(f\"{resultspath}/grounding_{imgPath}\")\n",
    "\n",
    "            if not os.path.exists(f\"{resultspath}/{class_name}\"):\n",
    "                os.mkdir(f\"{resultspath}/{class_name}\")\n",
    "\n",
    "            file_path = f\"{resultspath}/{class_name}/{imgPath[:-4]}.png\"\n",
    "            if os.path.exists(file_path):\n",
    "                if os.path.exists(f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\"):\n",
    "                    print(\"File already exists, saving with _2\")\n",
    "                    cv2.imwrite(f\"{resultspath}/{class_name}/{imgPath[:-4]}_2.png\", result)\n",
    "                else:\n",
    "                    print(\"File already exists, saving with _1\")\n",
    "                    file_path = f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\"\n",
    "\n",
    "            cv2.imwrite(file_path, result)\n",
    "            i=i+1\n",
    "            ran_sam = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gatos..png started\n",
      "gatos..png done\n",
      "gatos._2.png started\n",
      "gatos._2.png done\n",
      "gatos._1.png started\n",
      "gatos._1.png done\n",
      "jarra3..png started\n",
      "jarra3..png done\n",
      "jarra2..png started\n",
      "jarra2..png done\n",
      "jarra1..png started\n",
      "jarra1..png done\n",
      "lata.png started\n",
      "lata.png done\n",
      "manzana1..png started\n",
      "manzana1..png done\n",
      "manzana2._1.png started\n",
      "manzana2._1.png done\n",
      "manzana1._2.png started\n",
      "manzana1._2.png done\n",
      "manzana2._2.png started\n",
      "manzana2._2.png done\n",
      "manzana1._1.png started\n",
      "manzana1._1.png done\n",
      "manzana2..png started\n",
      "manzana2..png done\n",
      "platano3..png started\n",
      "platano3..png done\n",
      "platano3._1.png started\n",
      "platano3._1.png done\n",
      "platano1._1.png started\n",
      "platano1._1.png done\n",
      "platano2..png started\n",
      "platano2..png done\n",
      "platano2._1.png started\n",
      "platano2._1.png done\n",
      "platano1..png started\n",
      "platano1..png done\n",
      "silla.png started\n",
      "silla.png done\n",
      "silla_1.png started\n",
      "silla_1.png done\n",
      "silla_2.png started\n",
      "silla_2.png done\n",
      "all done\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "datasetpath = \"/home/jabv/Desktop/prueba/prueba_precut/\"\n",
    "resultspath = \"/home/jabv/Desktop/prueba/prueba_fit/\"\n",
    "\n",
    "#create a result folder if it doesn't exist\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n",
    "    os.makedirs(resultspath+\"gatos/\")\n",
    "    os.makedirs(resultspath+\"jarra/\")\n",
    "    os.makedirs(resultspath+\"lata/\")\n",
    "    os.makedirs(resultspath+\"manzana/\")\n",
    "    os.makedirs(resultspath+\"platano/\")\n",
    "    os.makedirs(resultspath+\"silla/\")\n",
    "\n",
    "\n",
    "fg_folders = [\n",
    "    (\"gatos/\"),\n",
    "    (\"jarra/\"),\n",
    "    (\"lata/\"),\n",
    "    (\"manzana/\"),\n",
    "    (\"platano/\"),\n",
    "    (\"silla/\")\n",
    "]\n",
    "\n",
    "for folder in fg_folders:\n",
    "    for filename in os.listdir(f\"{datasetpath}{folder}\"):\n",
    "        try:\n",
    "            print(f\"{filename} started\")\n",
    "            myImage = Image.open(datasetpath+folder+filename)\n",
    "            black = Image.new('RGBA', myImage.size)\n",
    "            myImage = Image.composite(myImage, black, myImage)\n",
    "            #print(\"aqui\")\n",
    "            myCroppedImage = myImage.crop(myImage.getbbox())\n",
    "            myCroppedImage.save(f\"{resultspath}{folder}{filename}\")\n",
    "            print(f\"{filename} done\")\n",
    "        except:\n",
    "            print(f\"{filename} failed\")\n",
    "            continue\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the three folders containing the images\n",
    "datasetPath = \"/home/jabv/Desktop/prueba/prueba_fit\"\n",
    "fg_folders = [\n",
    "    (f\"{datasetPath}/gatos/\",\"gatos\" ),\n",
    "    (f\"{datasetPath}/jarra/\",\"jarra\" ),\n",
    "    (f\"{datasetPath}/lata/\",\"lata\" ),\n",
    "    (f\"{datasetPath}/manzana/\",\"manzana\" ),\n",
    "    (f\"{datasetPath}/platano/\",\"platano\" ),\n",
    "    (f\"{datasetPath}/silla/\",\"silla\" )\n",
    "\n",
    "]\n",
    "bg_folder = \"/home/jabv/Desktop/prueba/bg/\"\n",
    "output_folder = \"/home/jabv/Desktop/prueba/prueba_final/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gatos': 0, 'jarra': 1, 'lata': 2, 'manzana': 3, 'platano': 4, 'silla': 5}\n",
      "[{'id': 0, 'name': 'gatos'}, {'id': 1, 'name': 'jarra'}, {'id': 2, 'name': 'lata'}, {'id': 3, 'name': 'manzana'}, {'id': 4, 'name': 'platano'}, {'id': 5, 'name': 'silla'}]\n"
     ]
    }
   ],
   "source": [
    "objects_list = [\"gatos\", \"jarra\", \"lata\", \"manzana\", \"platano\", \"silla\"]\n",
    "annotations_ID = {}\n",
    "categories = []\n",
    "for i, object in enumerate(objects_list):\n",
    "    annotations_ID[object] = i\n",
    "    categories.append({\"id\": i, \"name\": object})\n",
    "\n",
    "print(annotations_ID)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of files in each of the three folders\n",
    "fg_files = {}\n",
    "for folder, category in fg_folders:\n",
    "    fg_files[category] = os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "trainfolder = output_folder + \"train/\"\n",
    "testfolder = output_folder + \"test/\"\n",
    "validfolder = output_folder + \"valid/\"\n",
    "os.mkdir(trainfolder)\n",
    "os.mkdir(testfolder)\n",
    "os.mkdir(validfolder)\n",
    "os.mkdir(trainfolder + \"images/\")\n",
    "os.mkdir(trainfolder + \"labels/\")\n",
    "os.mkdir(testfolder + \"images/\")\n",
    "os.mkdir(testfolder + \"labels/\")\n",
    "os.mkdir(validfolder + \"images/\")\n",
    "os.mkdir(validfolder + \"labels/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping jarra due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping silla due to size constraints\n",
      "Skipping lata due to size constraints\n",
      "Skipping gatos due to size constraints\n",
      "Skipping platano due to size constraints\n",
      "Skipping silla due to size constraints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import yaml\n",
    "\n",
    "images = []\n",
    "annotations = []\n",
    "annotations2 = []\n",
    "annot_csv = []\n",
    "\n",
    "img_id = int(0)\n",
    "anno_id = int(0)\n",
    "\n",
    "rescaling_min = 0.60\n",
    "rescaling_max = 0.70\n",
    "\n",
    "for j in range(200):\n",
    "    # Crear archivo de etiquetas vacío\n",
    "    with open(f'{trainfolder}labels/{img_id}.txt', 'w') as file:\n",
    "        pass\n",
    "    \n",
    "    # Seleccionar aleatoriamente cuántos objetos habrá en la imagen\n",
    "    num_objects = random.randint(1, 5)  # Asegúrate de tener al menos un objeto\n",
    "    \n",
    "    # Seleccionar imágenes de primer plano aleatoriamente\n",
    "    fg_categories = random.choices(objects_list, k=num_objects)\n",
    "    \n",
    "    fg_files_selected = []\n",
    "    for category in fg_categories:\n",
    "        fg_files_selected.append([category, random.choice(fg_files[category])])\n",
    "    \n",
    "    # Cargar las imágenes de primer plano seleccionadas\n",
    "    fg_imgs = []\n",
    "    for img in fg_files_selected:\n",
    "        folder = [f[0] for f in fg_folders if f[1] == img[0]][0]\n",
    "        fg_imgs.append([img[0], Image.open(folder + img[1]), folder + img[1]])\n",
    "    \n",
    "    # Cargar la imagen de fondo\n",
    "    bg_files = os.listdir(bg_folder)\n",
    "    bg_file = random.choice(bg_files)\n",
    "    bg_img = Image.open(bg_folder + bg_file)\n",
    "\n",
    "    occupied = []  # Lista para almacenar posiciones ocupadas\n",
    "\n",
    "    for img in fg_imgs:\n",
    "        fg_img = img[1]\n",
    "\n",
    "        # Redimensionar y rotar la imagen de primer plano\n",
    "        angle = random.randint(-5, 5)\n",
    "        scale = random.uniform(rescaling_min, rescaling_max)\n",
    "        fg_img = fg_img.rotate(angle, resample=Image.BICUBIC, expand=True)\n",
    "        fg_img = fg_img.resize((int(fg_img.width * scale), int(fg_img.height * scale)))\n",
    "        \n",
    "        # Aplicar mejoras a la imagen\n",
    "        fg_img = ImageEnhance.Brightness(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = ImageEnhance.Contrast(fg_img).enhance(random.uniform(0.9, 1.1))\n",
    "        fg_img = ImageEnhance.Color(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = fg_img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.0, 0.5)))\n",
    "        \n",
    "        # Comprobar si el objeto cabe en la imagen de fondo\n",
    "        if fg_img.width > bg_img.width or fg_img.height > bg_img.height:\n",
    "            print(f\"Skipping {img[0]} due to size constraints\")\n",
    "            continue  # Si la imagen de primer plano es más grande, saltar\n",
    "\n",
    "        max_x = bg_img.width - fg_img.width\n",
    "        max_y = bg_img.height - fg_img.height\n",
    "\n",
    "        # Intentar colocar el objeto sin superposición\n",
    "        for attempt in range(10):\n",
    "            x = random.randint(0, max_x)\n",
    "            y = random.randint(0, max_y)\n",
    "\n",
    "            # Comprobar superposición\n",
    "            overlap = False\n",
    "            for occupied_rect in occupied:\n",
    "                if (x < occupied_rect[0] + occupied_rect[2] and\n",
    "                    x + fg_img.width > occupied_rect[0] and\n",
    "                    y < occupied_rect[1] + occupied_rect[3] and\n",
    "                    y + fg_img.height > occupied_rect[1]):\n",
    "                    overlap = True\n",
    "                    break\n",
    "            \n",
    "            if not overlap:\n",
    "                occupied.append([x, y, fg_img.width, fg_img.height])  # Guardar posición ocupada\n",
    "                break\n",
    "        else:\n",
    "            continue  # Si no se encuentra un espacio, continuar\n",
    "\n",
    "        # Pegar la imagen de primer plano en el fondo\n",
    "        bg_img.paste(fg_img, (x, y), fg_img)\n",
    "\n",
    "        # Guardar anotaciones\n",
    "        x_center_ann = (x + fg_img.width / 2) / bg_img.width\n",
    "        y_center_ann = (y + fg_img.height / 2) / bg_img.height\n",
    "        width_ann = fg_img.width / bg_img.width\n",
    "        height_ann = fg_img.height / bg_img.height\n",
    "        \n",
    "        with open(f'{trainfolder}labels/{img_id}.txt', 'a') as f:\n",
    "            f.write(f\"{annotations_ID[img[0]]} {x_center_ann} {y_center_ann} {width_ann} {height_ann}\\n\")\n",
    "\n",
    "        annotations2.append({\n",
    "            \"id\": anno_id,\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": annotations_ID[img[0]],\n",
    "            \"bbox\": [x, y, fg_img.width, fg_img.height],\n",
    "            \"segmentation\": [],\n",
    "            \"area\": fg_img.height * fg_img.width,\n",
    "            \"iscrowd\": 0\n",
    "        })\n",
    "        annotations.append({\n",
    "            \"id\": anno_id,\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": annotations_ID[img[0]],\n",
    "            \"bbox\": [x, y, fg_img.width, fg_img.height],\n",
    "            \"segmentation\": [],\n",
    "            \"area\": fg_img.height * fg_img.width,\n",
    "            \"iscrowd\": 0\n",
    "        })\n",
    "        annot_csv.append([\"TRAIN\", output_folder + str(img_id) + \".jpg\", img[0], x / bg_img.width, y / bg_img.height, \"\", \"\", (x + fg_img.width) / bg_img.width, (y + fg_img.height) / bg_img.height])\n",
    "        anno_id += 1\n",
    "\n",
    "    # Guardar la imagen de fondo con los objetos pegados\n",
    "    bg_img.save(f\"{trainfolder}images/\" + str(img_id) + \".jpg\", quality=100)\n",
    "    images.append({\n",
    "        \"id\": img_id,\n",
    "        \"file_name\": str(img_id) + \".jpg\",\n",
    "        \"height\": bg_img.height,\n",
    "        \"width\": bg_img.width\n",
    "    })\n",
    "    img_id += 1\n",
    "\n",
    "# Crear data.yaml\n",
    "data = dict(\n",
    "    train=f\"{trainfolder}images\",\n",
    "    val=f\"{validfolder}images\",\n",
    "    test=f\"{validfolder}images\",\n",
    "    nc=len(annotations_ID),\n",
    "    names=list(annotations_ID.keys())\n",
    ")\n",
    "\n",
    "# Almacenar\n",
    "with open(f'{output_folder}data.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: 196.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 195.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 125.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 15.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 182.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 86.jpg\n",
      "Number of objects: 5\n",
      "Processing image: 29.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 175.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 186.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 158.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 129.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 14.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 17.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 157.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 9.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 118.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 57.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 96.jpg\n",
      "Number of objects: 4\n",
      "Processing image: 45.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 27.jpg\n",
      "Number of objects: 5\n",
      "Processing image: 33.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 69.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 8.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 54.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 98.jpg\n",
      "Number of objects: 4\n",
      "Processing image: 67.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 145.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 179.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 70.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 35.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 130.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 38.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 11.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 108.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 159.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 136.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 107.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 197.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 151.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 194.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 189.jpg\n",
      "Number of objects: 4\n",
      "Processing image: 128.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 26.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 82.jpg\n",
      "Number of objects: 4\n",
      "Processing image: 127.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 62.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 44.jpg\n",
      "Number of objects: 4\n",
      "Processing image: 199.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 141.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 87.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 55.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 106.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 42.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 50.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 13.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 94.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 113.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 32.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 21.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 20.jpg\n",
      "Number of objects: 4\n",
      "Processing image: 40.jpg\n",
      "Number of objects: 5\n",
      "Processing image: 122.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 78.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 60.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 110.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 43.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 22.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 47.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 103.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 183.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 30.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 23.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 193.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 0.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 152.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 148.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 53.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 104.jpg\n",
      "Number of objects: 4\n",
      "Processing image: 177.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 174.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 89.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 64.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 3.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 6.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 180.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 88.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 39.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 114.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 112.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 31.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 76.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 72.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 80.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 83.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 135.jpg\n",
      "Number of objects: 4\n",
      "Processing image: 101.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 58.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 154.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 150.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 142.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 61.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 181.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 63.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 56.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 79.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 115.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 168.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 167.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 163.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 92.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 90.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 74.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 100.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 49.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 95.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 46.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 185.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 184.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 41.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 146.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 25.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 190.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 68.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 138.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 37.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 120.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 28.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 52.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 16.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 91.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 24.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 4.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 34.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 105.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 48.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 116.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 170.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 171.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 169.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 51.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 66.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 192.jpg\n",
      "Number of objects: 4\n",
      "Processing image: 77.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 71.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 153.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 124.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 97.jpg\n",
      "Number of objects: 5\n",
      "Processing image: 65.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 191.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 164.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 1.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 2.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 131.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 109.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 149.jpg\n",
      "Number of objects: 5\n",
      "Processing image: 12.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 176.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 81.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 84.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 160.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 162.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 111.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 133.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 117.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 18.jpg\n",
      "Number of objects: 3\n",
      "Processing image: 73.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 10.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 161.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 166.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 188.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 147.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 5.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 19.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 36.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 156.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 143.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 126.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 85.jpg\n",
      "Number of objects: 4\n",
      "Processing image: 119.jpg\n",
      "Number of objects: 0\n",
      "Processing image: 140.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 139.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 93.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 165.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 144.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 121.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 123.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 155.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 102.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 134.jpg\n",
      "Number of objects: 5\n",
      "Processing image: 59.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 99.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 187.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 75.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 173.jpg\n",
      "Number of objects: 1\n",
      "Processing image: 7.jpg\n",
      "Number of objects: 5\n",
      "Processing image: 172.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 198.jpg\n",
      "Number of objects: 5\n",
      "Processing image: 132.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 137.jpg\n",
      "Number of objects: 2\n",
      "Processing image: 178.jpg\n",
      "Number of objects: 4\n"
     ]
    }
   ],
   "source": [
    "final_dataset = \"/home/jabv/Desktop/prueba/prueba_final/\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "sam_model = \"h\"\n",
    "\n",
    "#use sam model\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
    "if sam_model ==\"h\":\n",
    "  sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "  model_type = \"vit_h\"\n",
    "else:\n",
    "  sam_checkpoint = \"sam_vit_l_0b3195.pth\"\n",
    "  model_type = \"vit_l\"\n",
    "\n",
    "device = \"cuda\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "images=[]\n",
    "annotations=[]\n",
    "categories=[]\n",
    "\n",
    "img_id=0\n",
    "anno_id=0\n",
    "\n",
    "final_images = f\"{final_dataset}train/images/\"\n",
    "final_labels = f\"{final_dataset}train/labels/\"\n",
    "output_points_path = f\"{final_dataset}train/segmentation_points/\"\n",
    "\n",
    "# Crea el directorio de salida si no existe\n",
    "os.makedirs(output_points_path, exist_ok=True)\n",
    "torch.cuda.empty_cache()  # Limpia la memoria de la GPU\n",
    "\n",
    "\n",
    "for imgPath in os.listdir(final_images):\n",
    "    img_id = imgPath.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(f\"Processing image: {imgPath}\")\n",
    "    \n",
    "    image = cv2.imread(os.path.join(final_images, imgPath))\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Cuenta las líneas en el archivo de etiquetas\n",
    "    with open(f\"{final_labels}{img_id}.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "        num_objects = len(lines)\n",
    "        print(f\"Number of objects: {num_objects}\")\n",
    "\n",
    "    ran_sam = False\n",
    "\n",
    "    for line in lines:\n",
    "        values = line.split(\" \")\n",
    "        class_name = values[0]  # Nombre de la clase\n",
    "        class_index = int(class_name)  # Asegúrate de que sea un número si usas índices\n",
    "\n",
    "        x_center = float(values[1])\n",
    "        y_center = float(values[2])\n",
    "        width_bbox = float(values[3])\n",
    "        height_bbox = float(values[4])\n",
    "        \n",
    "        # Convertir coordenadas del centro a esquinas\n",
    "        x0 = int((x_center - width_bbox / 2) * width)\n",
    "        y0 = int((y_center - height_bbox / 2) * height)\n",
    "        x1 = int((x_center + width_bbox / 2) * width)\n",
    "        y1 = int((y_center + height_bbox / 2) * height)\n",
    "\n",
    "        sam_bounding_box = np.array([x0, y0, x1, y1])\n",
    "\n",
    "        if not ran_sam:\n",
    "            predictor.set_image(image)\n",
    "            ran_sam = True\n",
    "\n",
    "        mask, _, _ = predictor.predict(box=sam_bounding_box, multimask_output=False)\n",
    "\n",
    "        contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        points = []\n",
    "        for contour in contours:\n",
    "            for point in contour:\n",
    "                # Normaliza las coordenadas\n",
    "                norm_x = point[0][0] / width\n",
    "                norm_y = point[0][1] / height\n",
    "                points.append((norm_x, norm_y))\n",
    "\n",
    "        # Guarda los puntos en el formato solicitado\n",
    "        point_file_path = f\"{output_points_path}{img_id}.txt\"\n",
    "        with open(point_file_path, \"a\") as point_file:  # Cambiado a \"a\" para añadir al archivo\n",
    "            point_file.write(f\"{class_index} \" + \" \".join(f\"{p[0]:.3f} {p[1]:.3f}\" for p in points) + \"\\n\")\n",
    "\n",
    "#delete labels folder\n",
    "import shutil\n",
    "shutil.rmtree(f\"{final_dataset}train/labels/\")\n",
    "#change name of segmentation_points to labels\n",
    "os.rename(f\"{final_dataset}train/segmentation_points/\", f\"{final_dataset}train/labels/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SplitTrainValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size is now: 168\n",
      "Validation size is now: 16\n",
      "Test size is now: 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "validation = 0.1\n",
    "test = 0.1\n",
    "\n",
    "# Assumes test has 100% of data\n",
    "output_folder = \"/home/jabv/Desktop/prueba/prueba_final/\"\n",
    "trainfolder = output_folder + \"train/\"\n",
    "trainfolderimgs = trainfolder + \"images/\"\n",
    "trainfolderlabels = trainfolder + \"labels/\"\n",
    "testfolder = output_folder + \"test/\"\n",
    "testfolderimgs = testfolder + \"images/\"\n",
    "testfolderlabels = testfolder + \"labels/\"\n",
    "validfolder = output_folder + \"valid/\"\n",
    "validfolderimgs = validfolder + \"images/\"\n",
    "validfolderlabels = validfolder + \"labels/\"\n",
    "\n",
    "# Obtener la lista de archivos en las carpetas de imágenes y etiquetas\n",
    "img_files = os.listdir(trainfolderimgs)\n",
    "label_files = os.listdir(trainfolderlabels)\n",
    "\n",
    "# Filtrar solo los archivos que existen en ambas carpetas\n",
    "existing_files = [f for f in img_files if f[:-4] + '.txt' in label_files]\n",
    "\n",
    "fullSize = len(existing_files)\n",
    "validSize = int(fullSize * validation)\n",
    "testSize = int(fullSize * test)\n",
    "\n",
    "# Mezclar los archivos existentes\n",
    "random.shuffle(existing_files)\n",
    "\n",
    "# Mover archivos a la carpeta de validación\n",
    "for i in range(validSize):\n",
    "    filetomove = existing_files[i]\n",
    "    filetomovename = filetomove[:-4]\n",
    "\n",
    "    # Mover imágenes\n",
    "    shutil.move(f\"{trainfolderimgs}{filetomove}\", f\"{validfolderimgs}{filetomove}\")\n",
    "    # Mover etiquetas\n",
    "    shutil.move(f\"{trainfolderlabels}{filetomovename}.txt\", f\"{validfolderlabels}{filetomovename}.txt\")\n",
    "\n",
    "# Mover archivos a la carpeta de test\n",
    "for i in range(testSize):\n",
    "    filetomove = existing_files[validSize + i]\n",
    "    filetomovename = filetomove[:-4]\n",
    "\n",
    "    # Mover imágenes\n",
    "    shutil.move(f\"{trainfolderimgs}{filetomove}\", f\"{testfolderimgs}{filetomove}\")\n",
    "    # Mover etiquetas\n",
    "    shutil.move(f\"{trainfolderlabels}{filetomovename}.txt\", f\"{testfolderlabels}{filetomovename}.txt\")\n",
    "\n",
    "# Imprimir el tamaño de cada carpeta\n",
    "print(f\"Train size is now: {len(os.listdir(trainfolderimgs))}\")\n",
    "print(f\"Validation size is now: {len(os.listdir(validfolderimgs))}\")\n",
    "print(f\"Test size is now: {len(os.listdir(testfolderimgs))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
