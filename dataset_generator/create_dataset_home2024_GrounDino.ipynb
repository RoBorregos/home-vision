{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jabv/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageEnhance, ImageFilter, ImageFont\n",
    "from pycocotools import mask\n",
    "import json\n",
    "import yaml\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ultralytics\n",
    "import time\n",
    "import imutils\n",
    "import argparse\n",
    "\n",
    "#grounding imports----------------\n",
    "\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.vl_utils import create_positive_map_from_span\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto label con Segment anyting y modelo de YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['platano2.jpeg']\n",
      "Processing image: platano2.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jabv/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/jabv/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box: 339, 330, 583, 661\n",
      "Bounding box: 562, 241, 712, 555\n",
      "File already exists, saving with _1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    for i,box in enumerate(boxes_filt):\\n\\n        #box = i * torch.Tensor([W, H, W, H])\\n\\n       # Calcular las coordenadas de la bounding box\\n        x0, y0, x1, y1 = map(int, box.tolist() * np.array([W, H, W, H]))\\n        w, h = x1 - x0, y1 - y0\\n\\n        img_with_bbox = img.copy()\\n\\n        cv2.rectangle(img_with_bbox, (x0, y0), (x1, y1), (0, 255, 0), 2)\\n\\n        cv2.imwrite(os.path.join(resultspath, f\"bbox_{i}_{imgPath}\"), img_with_bbox)\\n\\n        #class_name = labels[i]\\n        ran_sam = False\\n        \\n        #sam_bounding_box = (np.array([x, y, x+w, y+h]))\\n        sam_bounding_box = np.array([x0, y0, x1, y1])\\n        #run sam\\n        if ran_sam == False:\\n\\n            predictor.set_image(img)\\n\\n            ran_sam = True\\n\\n        mask, _, _ = predictor.predict(\\n            point_coords=None,\\n            point_labels=None,\\n            box=sam_bounding_box,\\n            multimask_output=False,\\n        )\\n        mask, _, _ = predictor.predict(box=sam_bounding_box, multimask_output=False)\\n\\n        contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # Your call to find the contours\\n        # threshold input image using otsu thresholding as mask and refine with morphology\\n        ret, pngmask = cv2.threshold(mask[0].astype(np.uint8), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU) \\n        kernel = np.ones((9,9), np.uint8)\\n        pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_CLOSE, kernel)\\n        pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_OPEN, kernel)\\n\\n        # put mask into alpha channel of result\\n        result2 = img.copy()\\n        result = img.copy()\\n        result = cv2.cvtColor(result, cv2.COLOR_BGR2BGRA)\\n        result[:, :, 3] = pngmask\\n\\n        #put the bounding box in the image with the mask and x,y,w,h\\n        cv2.rectangle(result2, (x0, y0), (w+x0, h+y0), (0, 255, 0), 2)\\n        print(j)\\n        cv2.imwrite(f\"{resultspath}/{class_name}/{j}_bbox.png\", result2)\\n\\n        j=j+1\\n        #cv2.imwrite(f\"{resultspath}/{cutobject}_{i}.png\", result)\\n        #to save with same name as original file\\n        # if already exists, save with _1, _2, etc\\n\\n        if not os.path.exists(f\"{resultspath}/{class_name}\"):\\n            os.mkdir(f\"{resultspath}/{class_name}\")\\n\\n        if os.path.exists(f\"{resultspath}/{class_name}/{imgPath[:-4]}.png\"):\\n            if os.path.exists(f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\"):\\n                print(\"File already exists, saving with _2\")\\n                cv2.imwrite(f\"{resultspath}/{class_name}/{imgPath[:-4]}_2.png\", result)\\n            print(\"File already exists, saving with _1\")\\n            temppath = f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\"\\n            print(temppath)\\n            cv2.imwrite(f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\", result)\\n\\n        cv2.imwrite(f\"{resultspath}/{class_name}/{imgPath[:-4]}.png\", result)\\n        i=i+1\\n        ran_sam = False\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathtoimage = \"/home/jabv/Desktop/prueba/imgs\" #path to images to process\n",
    "resultspath = \"/home/jabv/Desktop/prueba/res\" #path to save results all ready processed and segmented images\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n",
    "\n",
    "def load_image(image_path):\n",
    "\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")  # load image\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    image, _ = transform(image_pil, None)  # 3, h, w\n",
    "    return image_pil, image\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, cpu_only=False):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_grounding_output(model, image, caption, box_threshold, text_threshold=None, with_logits=True, cpu_only=False, token_spans=None):\n",
    "    assert text_threshold is not None or token_spans is not None, \"text_threshould and token_spans should not be None at the same time!\"\n",
    "    caption = caption.lower()\n",
    "    caption = caption.strip()\n",
    "    if not caption.endswith(\".\"):\n",
    "        caption = caption + \".\"\n",
    "    device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        print(\"Running model...\")\n",
    "        outputs = model(image[None], captions=[caption])\n",
    "    logits = outputs[\"pred_logits\"].sigmoid()[0]  # (nq, 256)\n",
    "    boxes = outputs[\"pred_boxes\"][0]  # (nq, 4)\n",
    "\n",
    "    # filter output\n",
    "    if token_spans is None:\n",
    "        logits_filt = logits.cpu().clone()\n",
    "        boxes_filt = boxes.cpu().clone()\n",
    "        filt_mask = logits_filt.max(dim=1)[0] > box_threshold\n",
    "        logits_filt = logits_filt[filt_mask]  # num_filt, 256\n",
    "        boxes_filt = boxes_filt[filt_mask]  # num_filt, 4\n",
    "\n",
    "        # get phrase\n",
    "        tokenlizer = model.tokenizer\n",
    "        tokenized = tokenlizer(caption)\n",
    "        # build pred\n",
    "        pred_phrases = []\n",
    "        for logit, box in zip(logits_filt, boxes_filt):\n",
    "            pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenlizer)\n",
    "            if with_logits:\n",
    "                pred_phrases.append(pred_phrase + f\"({str(logit.max().item())[:4]})\")\n",
    "            else:\n",
    "                pred_phrases.append(pred_phrase)\n",
    "    else:\n",
    "        # given-phrase mode\n",
    "        positive_maps = create_positive_map_from_span(\n",
    "            model.tokenizer(text_prompt),\n",
    "            token_span=token_spans\n",
    "        ).to(image.device) # n_phrase, 256\n",
    "\n",
    "        logits_for_phrases = positive_maps @ logits.T # n_phrase, nq\n",
    "        all_logits = []\n",
    "        all_phrases = []\n",
    "        all_boxes = []\n",
    "        for (token_span, logit_phr) in zip(token_spans, logits_for_phrases):\n",
    "            # get phrase\n",
    "            phrase = ' '.join([caption[_s:_e] for (_s, _e) in token_span])\n",
    "            # get mask\n",
    "            filt_mask = logit_phr > box_threshold\n",
    "            # filt box\n",
    "            all_boxes.append(boxes[filt_mask])\n",
    "            # filt logits\n",
    "            all_logits.append(logit_phr[filt_mask])\n",
    "            if with_logits:\n",
    "                logit_phr_num = logit_phr[filt_mask]\n",
    "                all_phrases.extend([phrase + f\"({str(logit.item())[:4]})\" for logit in logit_phr_num])\n",
    "            else:\n",
    "                all_phrases.extend([phrase for _ in range(len(filt_mask))])\n",
    "        boxes_filt = torch.cat(all_boxes, dim=0).cpu()\n",
    "        pred_phrases = all_phrases\n",
    "\n",
    "\n",
    "    return boxes_filt, pred_phrases\n",
    "\n",
    "# cfg\n",
    "config_file = \"/home/jabv/Desktop/home-vision/dataset_generator/groundingdino/config/GroundingDINO_SwinT_OGC.py\"  # change the path of the model config file\n",
    "\n",
    "#wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
    "checkpoint_path = \"/home/jabv/Desktop/home-vision/dataset_generator/groundingdino_swint_ogc.pth\"  # change the path of the model\n",
    "text_prompt = \"bannana\"\n",
    "output_dir = resultspath\n",
    "box_threshold = 0.3\n",
    "text_threshold = 0.25\n",
    "token_spans = None\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "sam_model = \"h\"\n",
    "\n",
    "#use sam model\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
    "if sam_model ==\"h\":\n",
    "  sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "  model_type = \"vit_h\"\n",
    "else:\n",
    "  sam_checkpoint = \"sam_vit_l_0b3195.pth\"\n",
    "  model_type = \"vit_l\"\n",
    "\n",
    "device = \"cuda\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "images=[]\n",
    "annotations=[]\n",
    "categories=[]\n",
    "\n",
    "img_id=0\n",
    "anno_id=0\n",
    "\n",
    "#check if results directory exists, else create it\n",
    "if not os.path.exists(resultspath):\n",
    "  os.makedirs(resultspath)\n",
    "\n",
    "\n",
    "imgPaths = os.listdir(pathtoimage)\n",
    "print(imgPaths)\n",
    "\n",
    "i=0\n",
    "\n",
    "for imgPath in imgPaths:\n",
    "    print(f\"Processing image: {imgPath}\")\n",
    "    img = imutils.resize(cv2.imread(f\"{pathtoimage}/{imgPath}\"))\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "#------------------------start grounding----------------------------------------------\n",
    "    #image_path = args.image_path\n",
    "\n",
    "    # load image\n",
    "    image_pil, image = load_image(f\"{pathtoimage}/{imgPath}\")\n",
    "\n",
    "    # load model\n",
    "    model = load_model(config_file, checkpoint_path, cpu_only=False)\n",
    "\n",
    "    # set the text_threshold to None if token_spans is set.\n",
    "    if token_spans is not None:\n",
    "        text_threshold = None\n",
    "        print(\"Using token_spans. Set the text_threshold to None.\")\n",
    "\n",
    "    # run model\n",
    "    boxes_filt, pred_phrases = get_grounding_output(\n",
    "        model, image, text_prompt, box_threshold, text_threshold, cpu_only=False, token_spans=eval(f\"{token_spans}\")\n",
    "    )\n",
    "\n",
    "    #found bb dimensions\n",
    "\n",
    "    size = image_pil.size\n",
    "    pred_dict = {\n",
    "        \"boxes\": boxes_filt,\n",
    "        \"size\": [size[1], size[0]],  # H,W\n",
    "        \"labels\": pred_phrases,\n",
    "    }\n",
    "\n",
    "    H, W = pred_dict[\"size\"]\n",
    "    boxes = pred_dict[\"boxes\"]\n",
    "    labels = pred_dict[\"labels\"]\n",
    "    assert len(boxes) == len(labels), \"boxes and labels must have same length\"\n",
    "\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    mask = Image.new(\"L\", image_pil.size, 0)\n",
    "    mask_draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    #change pil image to cv2 image\n",
    "    img = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
    "    img2 = img.copy()\n",
    "    # draw boxes and masks\n",
    "    for box, label in zip(boxes, labels):\n",
    "        # from 0..1 to 0..W, 0..H\n",
    "        box = box * torch.Tensor([W, H, W, H])\n",
    "        # from xywh to xyxy\n",
    "        box[:2] -= box[2:] / 2\n",
    "        box[2:] += box[:2]\n",
    "        # random color\n",
    "        color = tuple(np.random.randint(0, 255, size=1).tolist())\n",
    "        # draw\n",
    "        padding = 10\n",
    "        x0, y0, x1, y1 = box\n",
    "        x0, y0, x1, y1 = int(x0)-padding, int(y0)-padding, int(x1)+padding, int(y1)+padding\n",
    "\n",
    "        #draw rectangles\n",
    "        cv2.rectangle(img2, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "        draw.rectangle([x0, y0, x1, y1], outline=color, width=6)\n",
    "        # draw.text((x0, y0), str(label), fill=color)\n",
    "\n",
    "        font = ImageFont.load_default()\n",
    "        if hasattr(font, \"getbbox\"):\n",
    "            bbox = draw.textbbox((x0, y0), str(label), font)\n",
    "        else:\n",
    "            w, h = draw.textsize(str(label), font)\n",
    "            bbox = (x0, y0, w + x0, y0 + h)\n",
    "        # bbox = draw.textbbox((x0, y0), str(label))\n",
    "        draw.rectangle(bbox, fill=color)\n",
    "        draw.text((x0, y0), str(label), fill=\"white\")\n",
    "\n",
    "        mask_draw.rectangle([x0, y0, x1, y1], fill=255, width=6)\n",
    "    \n",
    "# ----------------End grounding ---------------------------------------------------------   \n",
    "    \n",
    "# ----------------Start SAM--------------------------------------------------------------  \n",
    "        \n",
    "        class_name = text_prompt\n",
    "        #print x0, y0, x1, y1\n",
    "        print(f\"Bounding box: {x0}, {y0}, {x1}, {y1}\")\n",
    "\n",
    "        sam_bounding_box = np.array([x0, y0, x1, y1])\n",
    "        ran_sam = False\n",
    "        #run sam\n",
    "        if ran_sam == False:\n",
    "            predictor.set_image(img)\n",
    "            ran_sam = True\n",
    "\n",
    "        mask, _, _ = predictor.predict(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            box=sam_bounding_box,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        mask, _, _ = predictor.predict(box=sam_bounding_box, multimask_output=False)\n",
    "\n",
    "        #Make png mask\n",
    "        contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # Your call to find the contours\n",
    "\n",
    "        # threshold input image using otsu thresholding as mask and refine with morphology\n",
    "        ret, pngmask = cv2.threshold(mask[0].astype(np.uint8), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU) \n",
    "        kernel = np.ones((9,9), np.uint8)\n",
    "        pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_CLOSE, kernel)\n",
    "        pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_OPEN, kernel)\n",
    "        result = img.copy()\n",
    "        result = cv2.cvtColor(result, cv2.COLOR_BGR2BGRA)\n",
    "        result[:, :, 3] = pngmask                           \n",
    "\n",
    "# ----------------End SAM-----------------------------------------------------------------  \n",
    "        cv2.imwrite(f\"{resultspath}/groundingcv2_{imgPath}\", img2)\n",
    "\n",
    "        image_pil.save(f\"{resultspath}/grounding_{imgPath}\")\n",
    "\n",
    "        if not os.path.exists(f\"{resultspath}/{class_name}\"):\n",
    "            os.mkdir(f\"{resultspath}/{class_name}\")\n",
    "\n",
    "        file_path = f\"{resultspath}/{class_name}/{imgPath[:-4]}.png\"\n",
    "        if os.path.exists(file_path):\n",
    "            if os.path.exists(f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\"):\n",
    "                print(\"File already exists, saving with _2\")\n",
    "                cv2.imwrite(f\"{resultspath}/{class_name}/{imgPath[:-4]}_2.png\", result)\n",
    "            else:\n",
    "                print(\"File already exists, saving with _1\")\n",
    "                file_path = f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\"\n",
    "\n",
    "        cv2.imwrite(file_path, result)\n",
    "        i=i+1\n",
    "        ran_sam = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "datasetpath = \"DS_letras_cut/\"\n",
    "resultspath = \"DS_letras/\"\n",
    "\n",
    "#create a result folder if it doesn't exist\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n",
    "    os.makedirs(resultspath+\"A/\")\n",
    "    os.makedirs(resultspath+\"B/\")\n",
    "    os.makedirs(resultspath+\"C/\")\n",
    "    os.makedirs(resultspath+\"D/\")\n",
    "    os.makedirs(resultspath+\"E/\")\n",
    "    os.makedirs(resultspath+\"F/\")\n",
    "    os.makedirs(resultspath+\"G/\")\n",
    "    os.makedirs(resultspath+\"H/\")\n",
    "    os.makedirs(resultspath+\"I/\")\n",
    "    os.makedirs(resultspath+\"verde/\")\n",
    "    os.makedirs(resultspath+\"amarillo/\")\n",
    "    os.makedirs(resultspath+\"azul/\")\n",
    "    os.makedirs(resultspath+\"rojo/\")\n",
    "\n",
    "fg_folders = [\n",
    "    (\"A/\"),\n",
    "    (\"B/\"),\n",
    "    (\"C/\"),\n",
    "    (\"D/\"),\n",
    "    (\"E/\"),\n",
    "    (\"F/\"),\n",
    "    (\"G/\"),\n",
    "    (\"H/\"),\n",
    "    (\"I/\"),\n",
    "    (\"verde/\"),\n",
    "    (\"amarillo/\"),\n",
    "    (\"azul/\"),\n",
    "    (\"rojo/\")\n",
    "]\n",
    "\n",
    "for folder in fg_folders:\n",
    "    for filename in os.listdir(f\"{datasetpath}{folder}\"):\n",
    "        try:\n",
    "            print(f\"{filename} started\")\n",
    "            myImage = Image.open(datasetpath+folder+filename)\n",
    "            black = Image.new('RGBA', myImage.size)\n",
    "            myImage = Image.composite(myImage, black, myImage)\n",
    "            print(\"aqui\")\n",
    "            myCroppedImage = myImage.crop(myImage.getbbox())\n",
    "            myCroppedImage.save(f\"{resultspath}{folder}{filename}\")\n",
    "            print(f\"{filename} done\")\n",
    "        except:\n",
    "            print(f\"{filename} failed\")\n",
    "            continue\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the three folders containing the images\n",
    "datasetPath = \"/home/jabv/Desktop/DS_letras_cropped\"\n",
    "fg_folders = [\n",
    "    (f\"{datasetPath}/A/\",\"A\" ),\n",
    "    (f\"{datasetPath}/B/\",\"B\" ),\n",
    "    (f\"{datasetPath}/C/\",\"C\" ),\n",
    "    (f\"{datasetPath}/D/\",\"D\" ),\n",
    "    (f\"{datasetPath}/E/\",\"E\" ),\n",
    "    (f\"{datasetPath}/F/\",\"F\" ),\n",
    "    (f\"{datasetPath}/G/\",\"G\" ),\n",
    "    (f\"{datasetPath}/H/\",\"H\" ),\n",
    "    (f\"{datasetPath}/I/\",\"I\" ),\n",
    "    (f\"{datasetPath}/verde/\",\"verde\" ),\n",
    "    (f\"{datasetPath}/azul/\",\"azul\" ),\n",
    "    (f\"{datasetPath}/amarillo/\",\"amarillo\" ),\n",
    "    (f\"{datasetPath}/rojo/\",\"rojo\" )\n",
    "]\n",
    "bg_folder = \"/home/jabv/Desktop/bg_imgs/\"\n",
    "output_folder = \"/home/jabv/Desktop/DS_letras/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'verde': 9, 'azul': 10, 'amarillo': 11, 'rojo': 12}\n",
      "[{'id': 0, 'name': 'A'}, {'id': 1, 'name': 'B'}, {'id': 2, 'name': 'C'}, {'id': 3, 'name': 'D'}, {'id': 4, 'name': 'E'}, {'id': 5, 'name': 'F'}, {'id': 6, 'name': 'G'}, {'id': 7, 'name': 'H'}, {'id': 8, 'name': 'I'}, {'id': 9, 'name': 'verde'}, {'id': 10, 'name': 'azul'}, {'id': 11, 'name': 'amarillo'}, {'id': 12, 'name': 'rojo'}]\n"
     ]
    }
   ],
   "source": [
    "objects_list = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"verde\", \"azul\", \"amarillo\", \"rojo\"]\n",
    "annotations_ID = {}\n",
    "categories = []\n",
    "for i, object in enumerate(objects_list):\n",
    "    annotations_ID[object] = i\n",
    "    categories.append({\"id\": i, \"name\": object})\n",
    "\n",
    "print(annotations_ID)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of files in each of the three folders\n",
    "fg_files = {}\n",
    "for folder, category in fg_folders:\n",
    "    fg_files[category] = os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(output_folder)\n",
    "trainfolder = output_folder + \"train/\"\n",
    "testfolder = output_folder + \"test/\"\n",
    "validfolder = output_folder + \"valid/\"\n",
    "os.mkdir(trainfolder)\n",
    "os.mkdir(testfolder)\n",
    "os.mkdir(validfolder)\n",
    "os.mkdir(trainfolder + \"images/\")\n",
    "os.mkdir(trainfolder + \"labels/\")\n",
    "os.mkdir(testfolder + \"images/\")\n",
    "os.mkdir(testfolder + \"labels/\")\n",
    "os.mkdir(validfolder + \"images/\")\n",
    "os.mkdir(validfolder + \"labels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\n",
    "annotations=[]\n",
    "annotations2=[]\n",
    "annot_csv=[]\n",
    "\n",
    "img_id=int(0)\n",
    "anno_id=int(0)\n",
    "\n",
    "rescaling_min = 0.20\n",
    "rescaling_max = 0.70\n",
    "\n",
    "# Ratios at which these values will be modified\n",
    "brightness_ratio = 0.05\n",
    "saturation_ratio = 0.05\n",
    "hue_ratio = 0.02\n",
    "\n",
    "for j in range(20000):\n",
    "    #create empty label file\n",
    "    with open(f'{trainfolder}labels/{img_id}.txt', 'w') as file:\n",
    "        pass\n",
    "    #select hramdomly how many objects will be in an image\n",
    "    num_objects = random.randint(0, 5)\n",
    "    #print(\"number of objects\",num_objects)\n",
    "    # Select random foreground images from the three folders, with replacement\n",
    "    fg_categories = random.choices(objects_list, k=num_objects)\n",
    "    \n",
    "    fg_files_selected = []\n",
    "    for category in fg_categories:\n",
    "        fg_files_selected.append([category,random.choice(fg_files[category])])\n",
    "    #print(\"seleccion\",fg_files_selected)\n",
    "    # Load the selected foreground images using Pillow\n",
    "    fg_imgs = []\n",
    "    for img in fg_files_selected:\n",
    "        folder = [f[0] for f in fg_folders if f[1] == img[0]][0]\n",
    "        fg_imgs.append([img[0],Image.open(folder + img[1]),folder+img[1]])\n",
    "\n",
    "    # Randomly resize and rotate the foreground images using Pillow's transform module\n",
    "    # img[0] is category, img[1] is image, img[2] is path\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "        angle = random.randint(-5, 5)\n",
    "        scale = random.uniform(rescaling_min, rescaling_max)\n",
    "        fg_img = fg_img.rotate(angle, resample=Image.BICUBIC, expand=True)\n",
    "        fg_img = fg_img.resize((int(fg_img.width * scale), int(fg_img.height * scale)))\n",
    "        fg_img = ImageEnhance.Brightness(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = ImageEnhance.Contrast(fg_img).enhance(random.uniform(0.9, 1.1))\n",
    "        fg_img = ImageEnhance.Color(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = fg_img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.0, 0.5)))\n",
    "\n",
    "\n",
    "        img[1] = fg_img\n",
    "\n",
    "    # Load the background image using Pillow\n",
    "    bg_files = os.listdir(bg_folder)\n",
    "    bg_file = random.choice(bg_files)\n",
    "    bg_img = Image.open(bg_folder + bg_file)\n",
    "\n",
    "    # Define the maximum overlap as a percentage\n",
    "    max_overlap_pct = 10\n",
    "\n",
    "    # Define an array to keep track of occupied areas\n",
    "    occupied = np.zeros((bg_img.height, bg_img.width))\n",
    "\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "\n",
    "        # Calculate the maximum overlap area\n",
    "        max_overlap_area = (fg_img.width * fg_img.height)\n",
    "\n",
    "        seg_img = img[1]\n",
    "\n",
    "\n",
    "        # Convert the image to a NumPy array\n",
    "        img_arr = np.array(seg_img)\n",
    "        # Create a binary mask of the non-transparent pixels\n",
    "        mask = img_arr[:, :, 3] != 0\n",
    "\n",
    "        # Convert the mask to a COCO format segmentation\n",
    "        segmentation = []\n",
    "        for i in range(mask.shape[0]):\n",
    "            for j in range(mask.shape[1]):\n",
    "                if mask[i, j]:\n",
    "                    segmentation.append(j)\n",
    "                    segmentation.append(i)\n",
    "        segmentation = [segmentation]\n",
    "\n",
    "        # Calculate the area of the segmentation\n",
    "        area = 0\n",
    "        for i in range(len(segmentation[0]) // 2):\n",
    "            x1 = segmentation[0][2 * i]\n",
    "            y1 = segmentation[0][2 * i + 1]\n",
    "            x2 = segmentation[0][(2 * i + 2) % len(segmentation[0])]\n",
    "            y2 = segmentation[0][(2 * i + 3) % len(segmentation[0])]\n",
    "            area += x1 * y2 - x2 * y1\n",
    "        area = abs(area) / 2\n",
    "        \n",
    "        # Draw the segmentation onto a copy of the original image\n",
    "        #image_copy = image.copy()\n",
    "        #cv2.fillPoly(image_copy, aux_segmentation, color=(0, 255, 0))\n",
    "\n",
    "        # Display the image with segmentation overlay\n",
    "        #cv2.imshow('Image with Segmentation', image_copy)\n",
    "        #cv2.waitKey(0)\n",
    "        #cv2.destroyAllWindows()\n",
    "\n",
    "        # Calculate the maximum allowed position for the top-left corner\n",
    "        max_x = bg_img.width - fg_img.width\n",
    "        max_y = bg_img.height - fg_img.height\n",
    "        area = fg_img.width * fg_img.height\n",
    "\n",
    "        # Generate a random location until an unoccupied area is found that meets the overlap limit\n",
    "        total_area = bg_img.width * bg_img.height\n",
    "        overlap_area = total_area\n",
    "        \n",
    "        while overlap_area / area > max_overlap_pct / 100:\n",
    "            try:\n",
    "                x = random.randint(0, max_x)\n",
    "                y = random.randint(0, max_y)\n",
    "            except:\n",
    "                x = random.randint(0, abs(max_x))\n",
    "                y = random.randint(0, abs(max_y))\n",
    "\n",
    "            # Calculate the overlap area\n",
    "            overlap_area = np.sum(occupied[y:y+fg_img.height, x:x+fg_img.width])\n",
    "\n",
    "            # Check if the area is unoccupied and the overlap limit is not exceeded\n",
    "            if (max_overlap_area) >= overlap_area/10:\n",
    "                break\n",
    "            if i==10:\n",
    "                continue\n",
    "        \n",
    "        for i in range(0, len(segmentation[0])):\n",
    "            if i % 2:\n",
    "                segmentation[0][i]=int(segmentation[0][i]+y)\n",
    "            else :\n",
    "                segmentation[0][i]=int(segmentation[0][i]+x)\n",
    "        # Update the occupied array\n",
    "        occupied[y:y+fg_img.height, x:x+fg_img.width] = 1\n",
    "\n",
    "        bg_img.paste(fg_img, (x, y), fg_img)\n",
    "        x_center_ann = (x+fg_img.width/2) / bg_img.width\n",
    "        y_center_ann = (y+fg_img.height/2) / bg_img.height\n",
    "        width_ann = fg_img.width / bg_img.width\n",
    "        height_ann = fg_img.height / bg_img.height\n",
    "        with open(f'{trainfolder}labels/{img_id}.txt', 'a') as f:\n",
    "            f.write(f\"{annotations_ID[img[0]]} {x_center_ann} {y_center_ann} {width_ann} {height_ann}\\n\")\n",
    "        annotations2.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": segmentation,\"area\": area,\"iscrowd\": 0})\n",
    "        annotations.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": [],\"area\": fg_img.height*fg_img.width,\"iscrowd\": 0})\n",
    "        annot_csv.append([\"TRAIN\", output_folder + str(img_id)+\".jpg\", img[0], x/bg_img.width, y/bg_img.height,\"\",\"\",(x+fg_img.width)/bg_img.width, (y+fg_img.height)/bg_img.height])\n",
    "        anno_id=anno_id+1\n",
    "        #draw = ImageDraw.Draw(bg_img)\n",
    "        #fdraw.rectangle((x, y, x+fg_img.width, y+fg_img.height), outline='red', width=3)\n",
    "    bg_img.save(f\"{trainfolder}images/\"+str(img_id)+\".jpg\", quality=100)\n",
    "    images.append({\"id\": img_id, \"file_name\": str(img_id)+\".jpg\",\"height\": bg_img.height,\"width\": bg_img.width})\n",
    "    img_id=img_id+1\n",
    "    #print(img_id)\n",
    "\n",
    "#making data.yaml\n",
    "data = dict(\n",
    "    train = f\"{trainfolder}images\",\n",
    "    val = f\"{validfolder}images\",\n",
    "    test = f\"{validfolder}images\",\n",
    "    nc = len(annotations_ID),\n",
    "    names = list(annotations_ID.keys())\n",
    "    )\n",
    "#storing\n",
    "with open(f'{output_folder}data.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
