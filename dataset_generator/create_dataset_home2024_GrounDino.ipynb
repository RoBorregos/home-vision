{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset for @HOME2024 using GroundingDino and SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipywidgets in /home/emilianh/.local/lib/python3.10/site-packages (8.1.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/emilianh/.local/lib/python3.10/site-packages (from ipywidgets) (8.24.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/emilianh/.local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/emilianh/.local/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /home/emilianh/.local/lib/python3.10/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /home/emilianh/.local/lib/python3.10/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: matplotlib-inline in /home/emilianh/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/emilianh/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/emilianh/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: decorator in /home/emilianh/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/emilianh/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/emilianh/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /home/emilianh/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /home/emilianh/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.11.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/emilianh/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in /home/emilianh/.local/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/emilianh/.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/emilianh/.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: pure-eval in /home/emilianh/.local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook qtconsole run server\n",
      "troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageEnhance, ImageFilter, ImageFont, ExifTags\n",
    "from pycocotools import mask\n",
    "import json\n",
    "import yaml\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ultralytics\n",
    "import time\n",
    "import imutils\n",
    "import argparse\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#grounding imports----------------\n",
    "\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.vl_utils import create_positive_map_from_span\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63e12f10e5a432d95a64a230b7b9234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resizing images:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m orientation \u001b[38;5;129;01min\u001b[39;00m ExifTags\u001b[38;5;241m.\u001b[39mTAGS\u001b[38;5;241m.\u001b[39mkeys() : \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ExifTags\u001b[38;5;241m.\u001b[39mTAGS[orientation]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrientation\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mbreak\u001b[39;00m \n\u001b[0;32m---> 17\u001b[0m exif\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getexif\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m())\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m   exif[orientation] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m : \n\u001b[1;32m     20\u001b[0m     img\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mrotate(\u001b[38;5;241m180\u001b[39m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "#resize images in a folder to a specific size\n",
    "pathtofiles = \"datasets/bags\"\n",
    "pathtoimage = [f.path for f in os.scandir(pathtofiles) if f.is_dir()]\n",
    "if len(pathtoimage) == 0:\n",
    "    print(\"No folders found in the directory\")\n",
    "new_width= 720\n",
    "\n",
    "resize_progress = tqdm(total=len(pathtoimage), desc=\"Resizing images\")\n",
    "for filepath in pathtoimage:\n",
    "    folder = filepath + \"/\"\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(folder + filename)\n",
    "        # Correct orientation\n",
    "        for orientation in ExifTags.TAGS.keys() : \n",
    "            if ExifTags.TAGS[orientation]=='Orientation' : break \n",
    "        \n",
    "        exif=dict(img._getexif().items())\n",
    "\n",
    "        if   exif[orientation] == 3 : \n",
    "            img=img.rotate(180, expand=True)\n",
    "        elif exif[orientation] == 6 : \n",
    "            img=img.rotate(270, expand=True)\n",
    "        elif exif[orientation] == 8 : \n",
    "            img=img.rotate(90, expand=True)\n",
    "        \n",
    "        # Resize\n",
    "        aspect_ratio = img.height / img.width\n",
    "        new_height = int(new_width * aspect_ratio)\n",
    "        img = img.resize((new_width, new_height))\n",
    "        img.save(folder + filename)\n",
    "        resize_progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto label con Segment anyting y modelo de YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SAM\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# SELECT MODEL\n",
    "sam_model = \"b\"\n",
    "\n",
    "#use sam model\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
    "if sam_model ==\"h\":\n",
    "  sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "  model_type = \"vit_h\"\n",
    "elif sam_model ==\"l\":\n",
    "  sam_checkpoint = \"sam_vit_l_0b3195.pth\"\n",
    "  model_type = \"vit_l\"\n",
    "elif sam_model ==\"b\":\n",
    "  sam_checkpoint = \"sam_vit_b_01ec64.pth\"\n",
    "  model_type = \"vit_b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINO setup\n",
    "# cfg\n",
    "config_file = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"  # change the path of the model config file\n",
    "\n",
    "#wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
    "checkpoint_path = \"GroundingDINO/groundingdino_swint_ogc.pth\"  # change the path of the model\n",
    "text_prompt = \"bag\"\n",
    "\n",
    "def load_image(image_path):\n",
    "\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")  # load image\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    image, _ = transform(image_pil, None)  # 3, h, w\n",
    "    return image_pil, image\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, cpu_only=False):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    # print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_grounding_output(model, image, caption, box_threshold, text_threshold=None, with_logits=True, cpu_only=False, token_spans=None):\n",
    "    assert text_threshold is not None or token_spans is not None, \"text_threshould and token_spans should not be None at the same time!\"\n",
    "    caption = caption.lower()\n",
    "    caption = caption.strip()\n",
    "    if not caption.endswith(\".\"):\n",
    "        caption = caption + \".\"\n",
    "    device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        # print(\"Running model...\")\n",
    "        outputs = model(image[None], captions=[caption])\n",
    "    logits = outputs[\"pred_logits\"].sigmoid()[0]  # (nq, 256)\n",
    "    boxes = outputs[\"pred_boxes\"][0]  # (nq, 4)\n",
    "\n",
    "    # filter output\n",
    "    if token_spans is None:\n",
    "        logits_filt = logits.cpu().clone()\n",
    "        boxes_filt = boxes.cpu().clone()\n",
    "        filt_mask = logits_filt.max(dim=1)[0] > box_threshold\n",
    "        logits_filt = logits_filt[filt_mask]  # num_filt, 256\n",
    "        boxes_filt = boxes_filt[filt_mask]  # num_filt, 4\n",
    "\n",
    "        # get phrase\n",
    "        tokenlizer = model.tokenizer\n",
    "        tokenized = tokenlizer(caption)\n",
    "        # build pred\n",
    "        pred_phrases = []\n",
    "        for logit, box in zip(logits_filt, boxes_filt):\n",
    "            pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenlizer)\n",
    "            if with_logits:\n",
    "                pred_phrases.append(pred_phrase + f\"({str(logit.max().item())[:4]})\")\n",
    "            else:\n",
    "                pred_phrases.append(pred_phrase)\n",
    "    else:\n",
    "        # given-phrase mode\n",
    "        positive_maps = create_positive_map_from_span(\n",
    "            model.tokenizer(text_prompt),\n",
    "            token_span=token_spans\n",
    "        ).to(image.device) # n_phrase, 256\n",
    "\n",
    "        logits_for_phrases = positive_maps @ logits.T # n_phrase, nq\n",
    "        all_logits = []\n",
    "        all_phrases = []\n",
    "        all_boxes = []\n",
    "        for (token_span, logit_phr) in zip(token_spans, logits_for_phrases):\n",
    "            # get phrase\n",
    "            phrase = ' '.join([caption[_s:_e] for (_s, _e) in token_span])\n",
    "            # get mask\n",
    "            filt_mask = logit_phr > box_threshold\n",
    "            # filt box\n",
    "            all_boxes.append(boxes[filt_mask])\n",
    "            # filt logits\n",
    "            all_logits.append(logit_phr[filt_mask])\n",
    "            if with_logits:\n",
    "                logit_phr_num = logit_phr[filt_mask]\n",
    "                all_phrases.extend([phrase + f\"({str(logit.item())[:4]})\" for logit in logit_phr_num])\n",
    "            else:\n",
    "                all_phrases.extend([phrase for _ in range(len(filt_mask))])\n",
    "        boxes_filt = torch.cat(all_boxes, dim=0).cpu()\n",
    "        pred_phrases = all_phrases\n",
    "\n",
    "\n",
    "    return boxes_filt, pred_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNNING DINO + SEGMENTATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathtofiles = \"datasets/bags\" #path to images to process\n",
    "resultspath = \"datasets/bags_precut\" #path to save results all ready processed and segmented images\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 directories in the path\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7b5bc5a60b4ead8505465289d9004d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images in datasets/bags/yellow_bag:   0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emilianh/.local/lib/python3.10/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emilianh/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:1052: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/emilianh/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/emilianh/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f646ca0c164de5985d1c589a863cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images in datasets/bags/red_bag:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0cb7dfc7264a688af0697397253b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images in datasets/bags/red_bag_precut: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa4e5a49b294ba49f01e5e14d515dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images in datasets/bags/green_bag:   0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "output_dir = resultspath\n",
    "box_threshold = 0.3\n",
    "text_threshold = 0.25\n",
    "token_spans = None\n",
    "\n",
    "device = \"cuda\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "images=[]\n",
    "annotations=[]\n",
    "categories=[]\n",
    "\n",
    "img_id=0\n",
    "anno_id=0\n",
    "\n",
    "#check if results directory exists, else create it\n",
    "if not os.path.exists(resultspath):\n",
    "  os.makedirs(resultspath)\n",
    "\n",
    "#make a list of all the directories in the path\n",
    "pathtoimage = [f.path for f in os.scandir(pathtofiles) if f.is_dir()]\n",
    "print(f\"Found {len(pathtoimage)} directories in the path\")\n",
    "\n",
    "#pathtoimage = os.listdir(pathtofiles)\n",
    "\n",
    "for filepath in pathtoimage:\n",
    "    imgPaths = os.listdir(filepath)\n",
    "    # print(imgPaths)\n",
    "\n",
    "    i=0\n",
    "    progress_bar = tqdm(total=len(imgPaths), desc=f\"Processing images in {filepath}\")\n",
    "    for imgPath in imgPaths:\n",
    "        # print(f\"Processing image: {imgPath}\")\n",
    "        progress_bar.update(1)\n",
    "        img = imutils.resize(cv2.imread(f\"{filepath}/{imgPath}\"))\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "    #------------------------start grounding----------------------------------------------\n",
    "        #image_path = args.image_path\n",
    "\n",
    "        # load image\n",
    "        image_pil, image = load_image(f\"{filepath}/{imgPath}\")\n",
    "\n",
    "        # load model\n",
    "        model = load_model(config_file, checkpoint_path, cpu_only=False)\n",
    "\n",
    "        # set the text_threshold to None if token_spans is set.\n",
    "        if token_spans is not None:\n",
    "            text_threshold = None\n",
    "            # print(\"Using token_spans. Set the text_threshold to None.\")\n",
    "\n",
    "        # run model\n",
    "        boxes_filt, pred_phrases = get_grounding_output(\n",
    "            model, image, text_prompt, box_threshold, text_threshold, cpu_only=False, token_spans=eval(f\"{token_spans}\")\n",
    "        )\n",
    "\n",
    "        #found bb dimensions\n",
    "\n",
    "        size = image_pil.size\n",
    "        pred_dict = {\n",
    "            \"boxes\": boxes_filt,\n",
    "            \"size\": [size[1], size[0]],  # H,W\n",
    "            \"labels\": pred_phrases,\n",
    "        }\n",
    "\n",
    "        H, W = pred_dict[\"size\"]\n",
    "        boxes = pred_dict[\"boxes\"]\n",
    "        labels = pred_dict[\"labels\"]\n",
    "        assert len(boxes) == len(labels), \"boxes and labels must have same length\"\n",
    "\n",
    "        draw = ImageDraw.Draw(image_pil)\n",
    "        mask = Image.new(\"L\", image_pil.size, 0)\n",
    "        mask_draw = ImageDraw.Draw(mask)\n",
    "\n",
    "        #change pil image to cv2 image\n",
    "        img = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
    "        img2 = img.copy()\n",
    "        # draw boxes and masks\n",
    "        x0_max = 0\n",
    "        y0_max = 0\n",
    "        x1_min = np.max(np.array(img))\n",
    "        y1_min = np.max(np.array(img))\n",
    "        for box, label in zip(boxes, labels):\n",
    "            # from 0..1 to 0..W, 0..H\n",
    "            box = box * torch.Tensor([W, H, W, H])\n",
    "            # from xywh to xyxy\n",
    "            box[:2] -= box[2:] / 2\n",
    "            box[2:] += box[:2]\n",
    "            # random color\n",
    "            color = tuple(np.random.randint(0, 255, size=1).tolist())\n",
    "            # draw\n",
    "            padding = 10\n",
    "            x0, y0, x1, y1 = box\n",
    "            x0, y0, x1, y1 = int(x0)-padding, int(y0)-padding, int(x1)+padding, int(y1)+padding\n",
    "\n",
    "            #validate if the bounding box is inside the image\n",
    "            if x0 < 0:\n",
    "                x0 = 0\n",
    "            if y0 < 0:\n",
    "                y0 = 0\n",
    "            if x1 > W:\n",
    "                x1 = W\n",
    "            if y1 > H:\n",
    "                y1 = H\n",
    "                \n",
    "            #draw rectangles\n",
    "            cv2.rectangle(img2, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "            draw.rectangle([x0, y0, x1, y1], outline=color, width=6)\n",
    "            # draw.text((x0, y0), str(label), fill=color)\n",
    "\n",
    "            font = ImageFont.load_default()\n",
    "            if hasattr(font, \"getbbox\"):\n",
    "                bbox = draw.textbbox((x0, y0), str(label), font)\n",
    "            else:\n",
    "                w, h = draw.textsize(str(label), font)\n",
    "                bbox = (x0, y0, w + x0, y0 + h)\n",
    "            # bbox = draw.textbbox((x0, y0), str(label))\n",
    "            draw.rectangle(bbox, fill=color)\n",
    "            draw.text((x0, y0), str(label), fill=\"white\")\n",
    "\n",
    "            mask_draw.rectangle([x0, y0, x1, y1], fill=255, width=6)\n",
    "        \n",
    "    # ----------------End grounding ---------------------------------------------------------   \n",
    "        \n",
    "    # ----------------Start SAM--------------------------------------------------------------  \n",
    "            \n",
    "            class_name = filepath.split(\"/\")[-1]\n",
    "            #print x0, y0, x1, y1\n",
    "            # print(f\"Bounding box: {x0}, {y0}, {x1}, {y1}\")\n",
    "            \n",
    "            #obtener el mas pequeÃ±o de los bounding boxes\n",
    "            \n",
    "            if x0 > x0_max and y0 > y0_max and x1 < x1_min and y1 < y1_min:\n",
    "                x0_max = x0\n",
    "                y0_max = y0\n",
    "                x1_min = x1\n",
    "                y1_min = y1\n",
    "                \n",
    "            sam_bounding_box = np.array([x0, y0, x1, y1])\n",
    "            ran_sam = False\n",
    "            #run sam\n",
    "            if ran_sam == False:\n",
    "                predictor.set_image(img)\n",
    "                ran_sam = True\n",
    "\n",
    "            mask, _, _ = predictor.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=sam_bounding_box,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "\n",
    "            mask, _, _ = predictor.predict(box=sam_bounding_box, multimask_output=False)\n",
    "\n",
    "            #Make png mask\n",
    "            contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # Your call to find the contours\n",
    "\n",
    "            # threshold input image using otsu thresholding as mask and refine with morphology\n",
    "            ret, pngmask = cv2.threshold(mask[0].astype(np.uint8), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU) \n",
    "            kernel = np.ones((9,9), np.uint8)\n",
    "            pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_CLOSE, kernel)\n",
    "            pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_OPEN, kernel)\n",
    "            result = img.copy()\n",
    "            result = cv2.cvtColor(result, cv2.COLOR_BGR2BGRA)\n",
    "            result[:, :, 3] = pngmask                           \n",
    "\n",
    "    # ----------------End SAM-----------------------------------------------------------------  \n",
    "            #cv2.imwrite(f\"{resultspath}/groundingcv2_{imgPath}\", img2)\n",
    "\n",
    "            #image_pil.save(f\"{resultspath}/grounding_{imgPath}\")\n",
    "\n",
    "            if not os.path.exists(f\"{resultspath}/{class_name}\"):\n",
    "                os.mkdir(f\"{resultspath}/{class_name}\")\n",
    "\n",
    "            file_path = f\"{resultspath}/{class_name}/{imgPath[:-4]}.png\"\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "                if x0_max == x0 and y0_max == y0 and x1_min == x1 and y1_min == y1:\n",
    "                    cv2.imwrite(file_path, result)\n",
    "            else:\n",
    "                cv2.imwrite(file_path, result)\n",
    "            i=i+1\n",
    "            ran_sam = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNG PREPROCESSING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating result folder:  datasets/bags_png_dataset/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2d302a3a64475d9b9d115ce0b9de1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images from object: red_bag:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c938306af01d43ca8a123e75622183c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images from object: green_bag:   0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0618ed3819e4a1e8079317b30f70c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images from object: yellow_bag:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "precut_dataset_path = \"datasets/bags_precut/\"\n",
    "png_dataset_result = \"datasets/bags_png_dataset/\"\n",
    "\n",
    "# Name of object MUST be the same as the folder name it is contained inside the png_datasetpath\n",
    "object_names = [\"red_bag\",\n",
    "                 \"green_bag\",\n",
    "                 \"yellow_bag\"]\n",
    "\n",
    "#create a result folder if it doesn't exist\n",
    "if not os.path.exists(png_dataset_result):\n",
    "    print(\"Creating result folder: \", png_dataset_result)\n",
    "    os.makedirs(png_dataset_result)\n",
    "    for object_name in object_names:\n",
    "        os.makedirs(os.path.join(png_dataset_result, object_name))\n",
    "else:\n",
    "    for object_name in object_names:\n",
    "        if not os.path.exists(os.path.join(png_dataset_result, object_name)):\n",
    "            os.makedirs(os.path.join(png_dataset_result, object_name))\n",
    "\n",
    "for object_name in object_names:\n",
    "    progress_bar = tqdm(total=len(os.listdir(os.path.join(precut_dataset_path, object_name))), desc=f\"Processing images from object: {object_name}\")\n",
    "    for filename in os.listdir(os.path.join(precut_dataset_path, object_name)):\n",
    "        progress_bar.update(1)\n",
    "        try:\n",
    "            filepath = os.path.join(precut_dataset_path, object_name, filename)\n",
    "            myImage = Image.open(filepath)\n",
    "            black = Image.new('RGBA', myImage.size)\n",
    "            myImage = Image.composite(myImage, black, myImage)\n",
    "            myCroppedImage = myImage.crop(myImage.getbbox())\n",
    "            save_filepath = os.path.join(png_dataset_result, object_name, filename)\n",
    "            myCroppedImage.save(save_filepath)\n",
    "            #print(f\"{filename} done\")\n",
    "        except:\n",
    "            #print(f\"{filename} failed\")\n",
    "            continue\n",
    "print(\"All done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINING DATASET ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('datasets/bags_png_dataset/red_bag', 'red_bag'), ('datasets/bags_png_dataset/green_bag', 'green_bag'), ('datasets/bags_png_dataset/yellow_bag', 'yellow_bag')]\n"
     ]
    }
   ],
   "source": [
    "# Define the paths to the three folders containing the images\n",
    "png_dataset = \"datasets/bags_png_dataset/\"\n",
    "\n",
    "# Define paths to each png folder, which will be used as foregrounds (fg)\n",
    "# has to be a list of tuples, (folder_path, object_name)\n",
    "fg_folders = [(os.path.join(png_dataset, object_name), object_name) for object_name in object_names]\n",
    "print(fg_folders)\n",
    "\n",
    "# Define the path to the backgrounds folder\n",
    "bg_folder = \"datasets/backgrounds\"\n",
    "\n",
    "# Define the path to the output folder, which will be a YOLO-formatted dataset\n",
    "output_folder = \"datasets/bags_yolo_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red_bag': 0, 'green_bag': 1, 'yellow_bag': 2}\n",
      "[{'id': 0, 'name': 'red_bag'}, {'id': 1, 'name': 'green_bag'}, {'id': 2, 'name': 'yellow_bag'}]\n"
     ]
    }
   ],
   "source": [
    "annotations_ID = {}\n",
    "categories = []\n",
    "for i, object in enumerate(object_names):\n",
    "    annotations_ID[object] = i\n",
    "    categories.append({\"id\": i, \"name\": object})\n",
    "\n",
    "print(annotations_ID)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of files in each of the three folders\n",
    "fg_files = {}\n",
    "for folder, category in fg_folders:\n",
    "    fg_files[category] = os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check files are loaded correctly\n",
    "fg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "trainfolder = os.path.join(output_folder, \"train\")\n",
    "testfolder = os.path.join(output_folder, \"test\")\n",
    "validfolder = os.path.join(output_folder, \"valid\")\n",
    "\n",
    "os.mkdir(trainfolder)\n",
    "os.mkdir(testfolder)\n",
    "os.mkdir(validfolder)\n",
    "os.mkdir(os.path.join(trainfolder, \"images\"))\n",
    "os.mkdir(os.path.join(trainfolder, \"labels\"))\n",
    "os.mkdir(os.path.join(testfolder, \"images\"))\n",
    "os.mkdir(os.path.join(testfolder, \"labels\"))\n",
    "os.mkdir(os.path.join(validfolder, \"images\"))\n",
    "os.mkdir(os.path.join(validfolder, \"labels\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET GENERATION ###\n",
    "Define the dataset augmentations, transformations and size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d2804f253e4783ac2c28a949f0c0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating images:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images=[]\n",
    "annotations=[]\n",
    "annotations2=[]\n",
    "annot_csv=[]\n",
    "\n",
    "img_id=int(0)\n",
    "anno_id=int(0)\n",
    "\n",
    "rescaling_min = 0.20\n",
    "rescaling_max = 0.70\n",
    "\n",
    "# Maximum ratio at which these values will be modified\n",
    "brightness_ratio = 0.05 # e.g. Brightness will be increased or decreased by up to brightness_ratio * 100%\n",
    "saturation_ratio = 0.05\n",
    "hue_ratio = 0.02\n",
    "\n",
    "TOTAL_IMAGES = 50\n",
    "\n",
    "progress_bar = tqdm(total=TOTAL_IMAGES, desc=\"Generating images\")\n",
    "for j in range(TOTAL_IMAGES):\n",
    "    #create empty label file\n",
    "    label_file = os.path.join(trainfolder, \"labels\", f\"{img_id}.txt\")\n",
    "    with open(label_file, 'w') as file:\n",
    "        pass\n",
    "    #select hramdomly how many objects will be in an image\n",
    "    num_objects = random.randint(0, 5)\n",
    "    #print(\"number of objects\",num_objects)\n",
    "    # Select random foreground images from the three folders, with replacement\n",
    "    fg_categories = random.choices(object_names, k=num_objects)\n",
    "    \n",
    "    fg_files_selected = []\n",
    "    for category in fg_categories:\n",
    "        fg_files_selected.append([category,random.choice(fg_files[category])])\n",
    "    #print(\"seleccion\",fg_files_selected)\n",
    "    # Load the selected foreground images using Pillow\n",
    "    fg_imgs = []\n",
    "    for img in fg_files_selected:\n",
    "        folder = [f[0] for f in fg_folders if f[1] == img[0]][0]\n",
    "        image_file = os.path.join(folder, img[1])\n",
    "        fg_imgs.append([img[0],Image.open(image_file),image_file])\n",
    "\n",
    "    # Randomly resize and rotate the foreground images using Pillow's transform module\n",
    "    # img[0] is category, img[1] is image, img[2] is path\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "        angle = random.randint(-5, 5)\n",
    "        scale = random.uniform(rescaling_min, rescaling_max)\n",
    "        fg_img = fg_img.rotate(angle, resample=Image.BICUBIC, expand=True)\n",
    "        fg_img = fg_img.resize((int(fg_img.width * scale), int(fg_img.height * scale)))\n",
    "        fg_img = ImageEnhance.Brightness(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = ImageEnhance.Contrast(fg_img).enhance(random.uniform(0.9, 1.1))\n",
    "        fg_img = ImageEnhance.Color(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = fg_img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.0, 0.5)))\n",
    "\n",
    "\n",
    "        img[1] = fg_img\n",
    "\n",
    "    # Load the background image using Pillow\n",
    "    bg_files = os.listdir(bg_folder)\n",
    "    bg_file = random.choice(bg_files)\n",
    "    bg_img = Image.open(os.path.join(bg_folder, bg_file))\n",
    "\n",
    "    # Define the maximum overlap as a percentage\n",
    "    max_overlap_pct = 10\n",
    "\n",
    "    # Define an array to keep track of occupied areas\n",
    "    occupied = np.zeros((bg_img.height, bg_img.width))\n",
    "\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "\n",
    "        # Calculate the maximum overlap area\n",
    "        max_overlap_area = (fg_img.width * fg_img.height)\n",
    "\n",
    "        seg_img = img[1]\n",
    "\n",
    "\n",
    "        # Convert the image to a NumPy array\n",
    "        img_arr = np.array(seg_img)\n",
    "        # Create a binary mask of the non-transparent pixels\n",
    "        mask = img_arr[:, :, 3] != 0\n",
    "\n",
    "        # Convert the mask to a COCO format segmentation\n",
    "        segmentation = []\n",
    "        for i in range(mask.shape[0]):\n",
    "            for j in range(mask.shape[1]):\n",
    "                if mask[i, j]:\n",
    "                    segmentation.append(j)\n",
    "                    segmentation.append(i)\n",
    "        segmentation = [segmentation]\n",
    "\n",
    "        # Calculate the area of the segmentation\n",
    "        area = 0\n",
    "        for i in range(len(segmentation[0]) // 2):\n",
    "            x1 = segmentation[0][2 * i]\n",
    "            y1 = segmentation[0][2 * i + 1]\n",
    "            x2 = segmentation[0][(2 * i + 2) % len(segmentation[0])]\n",
    "            y2 = segmentation[0][(2 * i + 3) % len(segmentation[0])]\n",
    "            area += x1 * y2 - x2 * y1\n",
    "        area = abs(area) / 2\n",
    "        \n",
    "        # Draw the segmentation onto a copy of the original image\n",
    "        #image_copy = image.copy()\n",
    "        #cv2.fillPoly(image_copy, aux_segmentation, color=(0, 255, 0))\n",
    "\n",
    "        # Display the image with segmentation overlay\n",
    "        #cv2.imshow('Image with Segmentation', image_copy)\n",
    "        #cv2.waitKey(0)\n",
    "        #cv2.destroyAllWindows()\n",
    "\n",
    "        # Calculate the maximum allowed position for the top-left corner\n",
    "        max_x = bg_img.width - fg_img.width\n",
    "        max_y = bg_img.height - fg_img.height\n",
    "        area = fg_img.width * fg_img.height\n",
    "\n",
    "        # Generate a random location until an unoccupied area is found that meets the overlap limit\n",
    "        total_area = bg_img.width * bg_img.height\n",
    "        overlap_area = total_area\n",
    "        \n",
    "        while overlap_area / area > max_overlap_pct / 100:\n",
    "            try:\n",
    "                x = random.randint(0, max_x)\n",
    "                y = random.randint(0, max_y)\n",
    "            except:\n",
    "                x = random.randint(0, abs(max_x))\n",
    "                y = random.randint(0, abs(max_y))\n",
    "\n",
    "            # Calculate the overlap area\n",
    "            overlap_area = np.sum(occupied[y:y+fg_img.height, x:x+fg_img.width])\n",
    "\n",
    "            # Check if the area is unoccupied and the overlap limit is not exceeded\n",
    "            if (max_overlap_area) >= overlap_area/10:\n",
    "                break\n",
    "            if i==10:\n",
    "                continue\n",
    "        \n",
    "        for i in range(0, len(segmentation[0])):\n",
    "            if i % 2:\n",
    "                segmentation[0][i]=int(segmentation[0][i]+y)\n",
    "            else :\n",
    "                segmentation[0][i]=int(segmentation[0][i]+x)\n",
    "        # Update the occupied array\n",
    "        occupied[y:y+fg_img.height, x:x+fg_img.width] = 1\n",
    "\n",
    "        bg_img.paste(fg_img, (x, y), fg_img)\n",
    "        x_center_ann = (x+fg_img.width/2) / bg_img.width\n",
    "        y_center_ann = (y+fg_img.height/2) / bg_img.height\n",
    "        width_ann = fg_img.width / bg_img.width\n",
    "        height_ann = fg_img.height / bg_img.height\n",
    "        with open(label_file, 'a') as f:\n",
    "            f.write(f\"{annotations_ID[img[0]]} {x_center_ann} {y_center_ann} {width_ann} {height_ann}\\n\")\n",
    "        annotations2.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": segmentation,\"area\": area,\"iscrowd\": 0})\n",
    "        annotations.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": [],\"area\": fg_img.height*fg_img.width,\"iscrowd\": 0})\n",
    "        annot_csv.append([\"TRAIN\", output_folder + str(img_id)+\".jpg\", img[0], x/bg_img.width, y/bg_img.height,\"\",\"\",(x+fg_img.width)/bg_img.width, (y+fg_img.height)/bg_img.height])\n",
    "        anno_id=anno_id+1\n",
    "        #draw = ImageDraw.Draw(bg_img)\n",
    "        #fdraw.rectangle((x, y, x+fg_img.width, y+fg_img.height), outline='red', width=3)\n",
    "    final_image_path = os.path.join(trainfolder, \"images\", f\"{img_id}.jpg\")\n",
    "    bg_img.save(final_image_path, quality=100)\n",
    "    images.append({\"id\": img_id, \"file_name\": str(img_id)+\".jpg\",\"height\": bg_img.height,\"width\": bg_img.width})\n",
    "    img_id=img_id+1\n",
    "    progress_bar.update(1)\n",
    "    #print(img_id)\n",
    "\n",
    "#making data.yaml\n",
    "data = dict(\n",
    "    train = f\"{trainfolder}images\",\n",
    "    val = f\"{validfolder}images\",\n",
    "    test = f\"{validfolder}images\",\n",
    "    nc = len(annotations_ID),\n",
    "    names = list(annotations_ID.keys())\n",
    "    )\n",
    "#storing\n",
    "with open(f'{output_folder}data.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT TRAIN AND VALIDATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size is now: 40\n",
      "Validation size is now: 5\n",
      "Test size is now: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "validation = 0.1\n",
    "test = 0.1\n",
    "\n",
    "# Assumes test has 100% of data\n",
    "output_folder = \"datasets/bags_yolo_dataset/\"\n",
    "trainfolder = os.path.join(output_folder, \"train\")\n",
    "testfolder = os.path.join(output_folder, \"test\")\n",
    "validfolder = os.path.join(output_folder, \"valid\")\n",
    "\n",
    "trainfolderimgs = os.path.join(trainfolder, \"images\")\n",
    "trainfolderlabels = os.path.join(trainfolder, \"labels\")\n",
    "testfolderimgs = os.path.join(testfolder, \"images\")\n",
    "testfolderlabels = os.path.join(testfolder, \"labels\")\n",
    "validfolderimgs = os.path.join(validfolder, \"images\")\n",
    "validfolderlabels = os.path.join(validfolder, \"labels\")\n",
    "\n",
    "fullSize = len(os.listdir(trainfolderimgs))\n",
    "validSize = int(fullSize * validation)\n",
    "testSize = int(fullSize * test)\n",
    "\n",
    "for i in range(validSize):\n",
    "    filelist = os.listdir(trainfolderimgs)\n",
    "    #randomize file list, to not pick files in order\n",
    "    random.shuffle(filelist)\n",
    "    filetomove = filelist[i]\n",
    "    #take out .jpg, .png, etc\n",
    "    filetomovename = filetomove[:-4]\n",
    "    #move images\n",
    "    shutil.move(os.path.join(trainfolderimgs, filetomove), os.path.join(validfolderimgs, filetomove))\n",
    "    #move labels\n",
    "    shutil.move(os.path.join(trainfolderlabels, f\"{filetomovename}.txt\"), os.path.join(validfolderlabels, f\"{filetomovename}.txt\"))\n",
    "for i in range(testSize):\n",
    "    filetomove = os.listdir(trainfolderimgs)[i]\n",
    "    #take out .jpg, .png, etc\n",
    "    filetomovename = filetomove[:-4]\n",
    "    #move images\n",
    "    shutil.move(os.path.join(trainfolderimgs, filetomove), os.path.join(testfolderimgs, filetomove))\n",
    "    #move labels\n",
    "    shutil.move(os.path.join(trainfolderlabels, f\"{filetomovename}.txt\"), os.path.join(testfolderlabels, f\"{filetomovename}.txt\"))\n",
    "\n",
    "#Validation\n",
    "print(f\"Train size is now: {len(os.listdir(trainfolderimgs))}\")\n",
    "print(f\"Validation size is now: {len(os.listdir(validfolderimgs))}\")\n",
    "print(f\"Test size is now: {len(os.listdir(testfolderimgs))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
