{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jabv/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageEnhance, ImageFilter\n",
    "from pycocotools import mask\n",
    "import json\n",
    "import yaml\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ultralytics\n",
    "import time\n",
    "import imutils\n",
    "import argparse\n",
    "\n",
    "#grounding imports----------------\n",
    "\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.vl_utils import create_positive_map_from_span\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto label con Segment anyting y modelo de YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manzana1.jpeg', 'manzana2.jpeg', 'platano1.jpeg', 'lata.jpg', 'platano2.jpeg', 'platano3.jpeg', 'jarra3.jpeg', 'jarra2.jpeg', 'jarra1.jpeg', 'silla.jpg', 'gatos.jpeg']\n",
      "Processing image: manzana1.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jabv/.local/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jabv/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/jabv/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0,y0,x1,y1:\n",
      "Box: (0, 0, 0, 0)\n",
      "x,y,w,h:\n",
      "Box: (0, 0, 0, 0)\n",
      "x0,y0,x1,y1:\n",
      "Box: (512, 384, 1536, 1152)\n",
      "x,y,w,h:\n",
      "Box: (512, 384, 2048, 1536)\n",
      "File already exists, saving with _1\n",
      "/home/jabv/Desktop/prueba/res/object/manzana1._1.png\n",
      "x0,y0,x1,y1:\n",
      "Box: (1024, 768, 3072, 2304)\n",
      "x,y,w,h:\n",
      "Box: (1024, 768, 4096, 3072)\n",
      "File already exists, saving with _2\n",
      "File already exists, saving with _1\n",
      "/home/jabv/Desktop/prueba/res/object/manzana1._1.png\n",
      "Processing image: manzana2.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jabv/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/jabv/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0,y0,x1,y1:\n",
      "Box: (0, 0, 0, 0)\n",
      "x,y,w,h:\n",
      "Box: (0, 0, 0, 0)\n",
      "x0,y0,x1,y1:\n",
      "Box: (512, 384, 1536, 1152)\n",
      "x,y,w,h:\n",
      "Box: (512, 384, 2048, 1536)\n",
      "File already exists, saving with _1\n",
      "/home/jabv/Desktop/prueba/res/object/manzana2._1.png\n",
      "x0,y0,x1,y1:\n",
      "Box: (1024, 768, 3072, 2304)\n",
      "x,y,w,h:\n",
      "Box: (1024, 768, 4096, 3072)\n",
      "File already exists, saving with _2\n",
      "File already exists, saving with _1\n",
      "/home/jabv/Desktop/prueba/res/object/manzana2._1.png\n",
      "Processing image: platano1.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "x0,y0,x1,y1:\n",
      "Box: (0, 0, 0, 0)\n",
      "x,y,w,h:\n",
      "Box: (0, 0, 0, 0)\n",
      "x0,y0,x1,y1:\n",
      "Box: (512, 384, 1536, 1152)\n",
      "x,y,w,h:\n",
      "Box: (512, 384, 2048, 1536)\n",
      "File already exists, saving with _1\n",
      "/home/jabv/Desktop/prueba/res/object/platano1._1.png\n",
      "x0,y0,x1,y1:\n",
      "Box: (1024, 768, 3072, 2304)\n",
      "x,y,w,h:\n",
      "Box: (1024, 768, 4096, 3072)\n",
      "File already exists, saving with _2\n",
      "File already exists, saving with _1\n",
      "/home/jabv/Desktop/prueba/res/object/platano1._1.png\n",
      "Processing image: lata.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "x0,y0,x1,y1:\n",
      "Box: (0, 0, 0, 0)\n",
      "x,y,w,h:\n",
      "Box: (0, 0, 0, 0)\n",
      "Processing image: platano2.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "x0,y0,x1,y1:\n",
      "Box: (0, 0, 0, 0)\n",
      "x,y,w,h:\n",
      "Box: (0, 0, 0, 0)\n",
      "x0,y0,x1,y1:\n",
      "Box: (512, 384, 1536, 1152)\n",
      "x,y,w,h:\n",
      "Box: (512, 384, 2048, 1536)\n",
      "File already exists, saving with _1\n",
      "/home/jabv/Desktop/prueba/res/object/platano2._1.png\n",
      "Processing image: platano3.jpeg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "x0,y0,x1,y1:\n",
      "Box: (0, 0, 0, 0)\n",
      "x,y,w,h:\n",
      "Box: (0, 0, 0, 0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jabv/Desktop/home-vision/dataset_generator/create_dataset_home2024_GrounDino.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jabv/Desktop/home-vision/dataset_generator/create_dataset_home2024_GrounDino.ipynb#W2sZmlsZQ%3D%3D?line=260'>261</a>\u001b[0m \u001b[39m#run sam\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jabv/Desktop/home-vision/dataset_generator/create_dataset_home2024_GrounDino.ipynb#W2sZmlsZQ%3D%3D?line=261'>262</a>\u001b[0m \u001b[39mif\u001b[39;00m ran_sam \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jabv/Desktop/home-vision/dataset_generator/create_dataset_home2024_GrounDino.ipynb#W2sZmlsZQ%3D%3D?line=262'>263</a>\u001b[0m     predictor\u001b[39m.\u001b[39;49mset_image(img)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jabv/Desktop/home-vision/dataset_generator/create_dataset_home2024_GrounDino.ipynb#W2sZmlsZQ%3D%3D?line=263'>264</a>\u001b[0m     ran_sam \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jabv/Desktop/home-vision/dataset_generator/create_dataset_home2024_GrounDino.ipynb#W2sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m mask, _, _ \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jabv/Desktop/home-vision/dataset_generator/create_dataset_home2024_GrounDino.ipynb#W2sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m     point_coords\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jabv/Desktop/home-vision/dataset_generator/create_dataset_home2024_GrounDino.ipynb#W2sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m     point_labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jabv/Desktop/home-vision/dataset_generator/create_dataset_home2024_GrounDino.ipynb#W2sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m     box\u001b[39m=\u001b[39msam_bounding_box,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jabv/Desktop/home-vision/dataset_generator/create_dataset_home2024_GrounDino.ipynb#W2sZmlsZQ%3D%3D?line=269'>270</a>\u001b[0m     multimask_output\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jabv/Desktop/home-vision/dataset_generator/create_dataset_home2024_GrounDino.ipynb#W2sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/segment_anything/predictor.py:60\u001b[0m, in \u001b[0;36mSamPredictor.set_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     57\u001b[0m input_image_torch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(input_image, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m input_image_torch \u001b[39m=\u001b[39m input_image_torch\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()[\u001b[39mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m---> 60\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_torch_image(input_image_torch, image\u001b[39m.\u001b[39;49mshape[:\u001b[39m2\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/segment_anything/predictor.py:89\u001b[0m, in \u001b[0;36mSamPredictor.set_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(transformed_image\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:])\n\u001b[1;32m     88\u001b[0m input_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpreprocess(transformed_image)\n\u001b[0;32m---> 89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mimage_encoder(input_image)\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_image_set \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[39m=\u001b[39m blk(x)\n\u001b[1;32m    114\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneck(x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[39m=\u001b[39m window_partition(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(x)\n\u001b[1;32m    175\u001b[0m \u001b[39m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[39m=\u001b[39m (q \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale) \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[39m=\u001b[39m add_decomposed_rel_pos(attn, q, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrel_pos_h, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrel_pos_w, (H, W), (H, W))\n\u001b[1;32m    236\u001b[0m attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[39m=\u001b[39m (attn \u001b[39m@\u001b[39m v)\u001b[39m.\u001b[39mview(B, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, H, W, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mreshape(B, H, W, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/segment_anything/modeling/image_encoder.py:349\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    347\u001b[0m q_h, q_w \u001b[39m=\u001b[39m q_size\n\u001b[1;32m    348\u001b[0m k_h, k_w \u001b[39m=\u001b[39m k_size\n\u001b[0;32m--> 349\u001b[0m Rh \u001b[39m=\u001b[39m get_rel_pos(q_h, k_h, rel_pos_h)\n\u001b[1;32m    350\u001b[0m Rw \u001b[39m=\u001b[39m get_rel_pos(q_w, k_w, rel_pos_w)\n\u001b[1;32m    352\u001b[0m B, _, dim \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/segment_anything/modeling/image_encoder.py:322\u001b[0m, in \u001b[0;36mget_rel_pos\u001b[0;34m(q_size, k_size, rel_pos)\u001b[0m\n\u001b[1;32m    319\u001b[0m k_coords \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(k_size)[\u001b[39mNone\u001b[39;00m, :] \u001b[39m*\u001b[39m \u001b[39mmax\u001b[39m(q_size \u001b[39m/\u001b[39m k_size, \u001b[39m1.0\u001b[39m)\n\u001b[1;32m    320\u001b[0m relative_coords \u001b[39m=\u001b[39m (q_coords \u001b[39m-\u001b[39m k_coords) \u001b[39m+\u001b[39m (k_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mmax\u001b[39m(q_size \u001b[39m/\u001b[39m k_size, \u001b[39m1.0\u001b[39m)\n\u001b[0;32m--> 322\u001b[0m \u001b[39mreturn\u001b[39;00m rel_pos_resized[relative_coords\u001b[39m.\u001b[39;49mlong()]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pathtoimage = \"/home/jabv/Desktop/prueba/imgs\" #path to images to process\n",
    "resultspath = \"/home/jabv/Desktop/prueba/res\" #path to save results all ready processed and segmented images\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n",
    "\n",
    "'''\n",
    "def plot_boxes_to_image(image_pil, tgt):\n",
    "    H, W = tgt[\"size\"]\n",
    "    boxes = tgt[\"boxes\"]\n",
    "    labels = tgt[\"labels\"]\n",
    "    assert len(boxes) == len(labels), \"boxes and labels must have same length\"\n",
    "\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    mask = Image.new(\"L\", image_pil.size, 0)\n",
    "    mask_draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    # draw boxes and masks\n",
    "    for box, label in zip(boxes, labels):\n",
    "        # from 0..1 to 0..W, 0..H   \n",
    "        box = box * torch.Tensor([W, H, W, H])\n",
    "        # from xywh to xyxy\n",
    "        box[:2] -= box[2:] / 2\n",
    "        box[2:] += box[:2]\n",
    "        # random color\n",
    "        color = tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "        # draw\n",
    "        x0, y0, x1, y1 = box\n",
    "        x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n",
    "\n",
    "        draw.rectangle([x0, y0, x1, y1], outline=color, width=6)\n",
    "        # draw.text((x0, y0), str(label), fill=color)\n",
    "\n",
    "        font = ImageFont.load_default()\n",
    "        if hasattr(font, \"getbbox\"):\n",
    "            bbox = draw.textbbox((x0, y0), str(label), font)\n",
    "        else:\n",
    "            w, h = draw.textsize(str(label), font)\n",
    "            bbox = (x0, y0, w + x0, y0 + h)\n",
    "        # bbox = draw.textbbox((x0, y0), str(label))\n",
    "        draw.rectangle(bbox, fill=color)\n",
    "        draw.text((x0, y0), str(label), fill=\"white\")\n",
    "\n",
    "        mask_draw.rectangle([x0, y0, x1, y1], fill=255, width=6)\n",
    "\n",
    "    return image_pil, mask\n",
    "'''\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    # load image\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")  # load image\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    image, _ = transform(image_pil, None)  # 3, h, w\n",
    "    return image_pil, image\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, cpu_only=False):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_grounding_output(model, image, caption, box_threshold, text_threshold=None, with_logits=True, cpu_only=False, token_spans=None):\n",
    "    assert text_threshold is not None or token_spans is not None, \"text_threshould and token_spans should not be None at the same time!\"\n",
    "    caption = caption.lower()\n",
    "    caption = caption.strip()\n",
    "    if not caption.endswith(\".\"):\n",
    "        caption = caption + \".\"\n",
    "    device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        print(\"Running model...\")\n",
    "        outputs = model(image[None], captions=[caption])\n",
    "    logits = outputs[\"pred_logits\"].sigmoid()[0]  # (nq, 256)\n",
    "    boxes = outputs[\"pred_boxes\"][0]  # (nq, 4)\n",
    "\n",
    "    # filter output\n",
    "    if token_spans is None:\n",
    "        logits_filt = logits.cpu().clone()\n",
    "        boxes_filt = boxes.cpu().clone()\n",
    "        filt_mask = logits_filt.max(dim=1)[0] > box_threshold\n",
    "        logits_filt = logits_filt[filt_mask]  # num_filt, 256\n",
    "        boxes_filt = boxes_filt[filt_mask]  # num_filt, 4\n",
    "\n",
    "        # get phrase\n",
    "        tokenlizer = model.tokenizer\n",
    "        tokenized = tokenlizer(caption)\n",
    "        # build pred\n",
    "        pred_phrases = []\n",
    "        for logit, box in zip(logits_filt, boxes_filt):\n",
    "            pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenlizer)\n",
    "            if with_logits:\n",
    "                pred_phrases.append(pred_phrase + f\"({str(logit.max().item())[:4]})\")\n",
    "            else:\n",
    "                pred_phrases.append(pred_phrase)\n",
    "    else:\n",
    "        # given-phrase mode\n",
    "        positive_maps = create_positive_map_from_span(\n",
    "            model.tokenizer(text_prompt),\n",
    "            token_span=token_spans\n",
    "        ).to(image.device) # n_phrase, 256\n",
    "\n",
    "        logits_for_phrases = positive_maps @ logits.T # n_phrase, nq\n",
    "        all_logits = []\n",
    "        all_phrases = []\n",
    "        all_boxes = []\n",
    "        for (token_span, logit_phr) in zip(token_spans, logits_for_phrases):\n",
    "            # get phrase\n",
    "            phrase = ' '.join([caption[_s:_e] for (_s, _e) in token_span])\n",
    "            # get mask\n",
    "            filt_mask = logit_phr > box_threshold\n",
    "            # filt box\n",
    "            all_boxes.append(boxes[filt_mask])\n",
    "            # filt logits\n",
    "            all_logits.append(logit_phr[filt_mask])\n",
    "            if with_logits:\n",
    "                logit_phr_num = logit_phr[filt_mask]\n",
    "                all_phrases.extend([phrase + f\"({str(logit.item())[:4]})\" for logit in logit_phr_num])\n",
    "            else:\n",
    "                all_phrases.extend([phrase for _ in range(len(filt_mask))])\n",
    "        boxes_filt = torch.cat(all_boxes, dim=0).cpu()\n",
    "        pred_phrases = all_phrases\n",
    "\n",
    "\n",
    "    return boxes_filt, pred_phrases\n",
    "\n",
    "# cfg\n",
    "config_file = \"/home/jabv/Desktop/home-vision/dataset_generator/groundingdino/config/GroundingDINO_SwinT_OGC.py\"  # change the path of the model config file\n",
    "checkpoint_path = \"/home/jabv/Desktop/home-vision/dataset_generator/groundingdino_swint_ogc.pth\"  # change the path of the model\n",
    "text_prompt = \"algo\"\n",
    "output_dir = resultspath\n",
    "box_threshold = 0.3\n",
    "text_threshold = 0.25\n",
    "token_spans = None\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "sam_model = \"h\"\n",
    "\n",
    "#use sam model\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
    "if sam_model ==\"h\":\n",
    "  sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "  model_type = \"vit_h\"\n",
    "else:\n",
    "  sam_checkpoint = \"sam_vit_l_0b3195.pth\"\n",
    "  model_type = \"vit_l\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Creating annotation in COCO format\n",
    "#{\"id\": 0, \"file_name\": \"0.jpg\", \"height\": 480, \"width\": 736}\n",
    "images=[]\n",
    "annotations=[]\n",
    "categories=[]\n",
    "\n",
    "img_id=0\n",
    "anno_id=0\n",
    "\n",
    "#check if results directory exists, else create it\n",
    "if not os.path.exists(resultspath):\n",
    "  os.makedirs(resultspath)\n",
    "\n",
    "\n",
    "imgPaths = os.listdir(pathtoimage)\n",
    "print(imgPaths)\n",
    "\n",
    "i=0\n",
    "\n",
    "for imgPath in imgPaths:\n",
    "    print(f\"Processing image: {imgPath}\")\n",
    "    img = imutils.resize(cv2.imread(f\"{pathtoimage}/{imgPath}\"))\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    #------------------------start grounding----------------------------------------------\n",
    "    #image_path = args.image_path\n",
    "\n",
    "    # load image\n",
    "    image_pil, image = load_image(f\"{pathtoimage}/{imgPath}\")\n",
    "    # image_pil = Image.fromarray(img)\n",
    "    # image = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "    # image = image.unsqueeze(0)\n",
    "\n",
    "    # load model\n",
    "    model = load_model(config_file, checkpoint_path, cpu_only=False)\n",
    "\n",
    "    # visualize raw image\n",
    "    #image_pil.save(os.path.join(output_dir, \"raw_image.jpg\"))\n",
    "\n",
    "    # set the text_threshold to None if token_spans is set.\n",
    "    if token_spans is not None:\n",
    "        text_threshold = None\n",
    "        print(\"Using token_spans. Set the text_threshold to None.\")\n",
    "\n",
    "\n",
    "    # run model\n",
    "    boxes_filt, pred_phrases = get_grounding_output(\n",
    "        model, image, text_prompt, box_threshold, text_threshold, cpu_only=False, token_spans=eval(f\"{token_spans}\")\n",
    "    )\n",
    "\n",
    "    # visualize pred\n",
    "    size = image_pil.size\n",
    "    pred_dict = {\n",
    "        \"boxes\": boxes_filt,\n",
    "        \"size\": [size[1], size[0]],  # H,W\n",
    "        \"labels\": pred_phrases,\n",
    "    }\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    #image_with_box = plot_boxes_to_image(image_pil, pred_dict)[0]\n",
    "    #image_with_box.save(os.path.join(output_dir, \"pred.jpg\"))\n",
    "    H, W = pred_dict[\"size\"]\n",
    "    boxes = pred_dict[\"boxes\"]\n",
    "    labels = pred_dict[\"labels\"]\n",
    "\n",
    "    # ----------------End grounding ---------------------------------------------------------   \n",
    "    class_name = \"object\"\n",
    "    for i in range(len(boxes)):\n",
    "\n",
    "        box = i * torch.Tensor([W, H, W, H])\n",
    "        # from xywh to xyxy\n",
    "        box[:2] -= box[2:] / 2\n",
    "        box[2:] += box[:2]\n",
    "        \n",
    "        x0, y0, x1, y1 = box\n",
    "        x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n",
    "\n",
    "        x,y,w,h = x0, y0, x1+x0, y1+y0\n",
    "        print (\"x0,y0,x1,y1:\")\n",
    "        print (f\"Box: {x0, y0, x1, y1}\")\n",
    "\n",
    "        print (\"x,y,w,h:\")\n",
    "        print (f\"Box: {x,y,w,h}\")\n",
    "        #class_name = labels[i]\n",
    "        ran_sam = False\n",
    "        \n",
    "        #sam_bounding_box = (np.array([x, y, x+w, y+h]))\n",
    "        sam_bounding_box = (np.array([box[0], box[1], box[2], box[3]]))\n",
    "        #run sam\n",
    "        if ran_sam == False:\n",
    "            predictor.set_image(img)\n",
    "            ran_sam = True\n",
    "\n",
    "        mask, _, _ = predictor.predict(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            box=sam_bounding_box,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # Your call to find the contours\n",
    "        # threshold input image using otsu thresholding as mask and refine with morphology\n",
    "        ret, pngmask = cv2.threshold(mask[0].astype(np.uint8), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU) \n",
    "        kernel = np.ones((9,9), np.uint8)\n",
    "        pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_CLOSE, kernel)\n",
    "        pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # put mask into alpha channel of result\n",
    "        result = img.copy()\n",
    "        result = cv2.cvtColor(result, cv2.COLOR_BGR2BGRA)\n",
    "        result[:, :, 3] = pngmask\n",
    "        #put the bounding box in the image with the mask and x,y,w,h\n",
    "        cv2.rectangle(result, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        #cv2.imwrite(f\"{resultspath}/{cutobject}_{i}.png\", result)\n",
    "        #to save with same name as original file\n",
    "        # if already exists, save with _1, _2, etc\n",
    "\n",
    "        if not os.path.exists(f\"{resultspath}/{class_name}\"):\n",
    "            os.mkdir(f\"{resultspath}/{class_name}\")\n",
    "\n",
    "        if os.path.exists(f\"{resultspath}/{class_name}/{imgPath[:-4]}.png\"):\n",
    "            if os.path.exists(f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\"):\n",
    "                print(\"File already exists, saving with _2\")\n",
    "                cv2.imwrite(f\"{resultspath}/{class_name}/{imgPath[:-4]}_2.png\", result)\n",
    "            print(\"File already exists, saving with _1\")\n",
    "            temppath = f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\"\n",
    "            print(temppath)\n",
    "            cv2.imwrite(f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\", result)\n",
    "\n",
    "        cv2.imwrite(f\"{resultspath}/{class_name}/{imgPath[:-4]}.png\", result)\n",
    "        i=i+1\n",
    "        ran_sam = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "datasetpath = \"DS_letras_cut/\"\n",
    "resultspath = \"DS_letras/\"\n",
    "\n",
    "#create a result folder if it doesn't exist\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n",
    "    os.makedirs(resultspath+\"A/\")\n",
    "    os.makedirs(resultspath+\"B/\")\n",
    "    os.makedirs(resultspath+\"C/\")\n",
    "    os.makedirs(resultspath+\"D/\")\n",
    "    os.makedirs(resultspath+\"E/\")\n",
    "    os.makedirs(resultspath+\"F/\")\n",
    "    os.makedirs(resultspath+\"G/\")\n",
    "    os.makedirs(resultspath+\"H/\")\n",
    "    os.makedirs(resultspath+\"I/\")\n",
    "    os.makedirs(resultspath+\"verde/\")\n",
    "    os.makedirs(resultspath+\"amarillo/\")\n",
    "    os.makedirs(resultspath+\"azul/\")\n",
    "    os.makedirs(resultspath+\"rojo/\")\n",
    "\n",
    "fg_folders = [\n",
    "    (\"A/\"),\n",
    "    (\"B/\"),\n",
    "    (\"C/\"),\n",
    "    (\"D/\"),\n",
    "    (\"E/\"),\n",
    "    (\"F/\"),\n",
    "    (\"G/\"),\n",
    "    (\"H/\"),\n",
    "    (\"I/\"),\n",
    "    (\"verde/\"),\n",
    "    (\"amarillo/\"),\n",
    "    (\"azul/\"),\n",
    "    (\"rojo/\")\n",
    "]\n",
    "\n",
    "for folder in fg_folders:\n",
    "    for filename in os.listdir(f\"{datasetpath}{folder}\"):\n",
    "        try:\n",
    "            print(f\"{filename} started\")\n",
    "            myImage = Image.open(datasetpath+folder+filename)\n",
    "            black = Image.new('RGBA', myImage.size)\n",
    "            myImage = Image.composite(myImage, black, myImage)\n",
    "            print(\"aqui\")\n",
    "            myCroppedImage = myImage.crop(myImage.getbbox())\n",
    "            myCroppedImage.save(f\"{resultspath}{folder}{filename}\")\n",
    "            print(f\"{filename} done\")\n",
    "        except:\n",
    "            print(f\"{filename} failed\")\n",
    "            continue\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the three folders containing the images\n",
    "datasetPath = \"/home/jabv/Desktop/DS_letras_cropped\"\n",
    "fg_folders = [\n",
    "    (f\"{datasetPath}/A/\",\"A\" ),\n",
    "    (f\"{datasetPath}/B/\",\"B\" ),\n",
    "    (f\"{datasetPath}/C/\",\"C\" ),\n",
    "    (f\"{datasetPath}/D/\",\"D\" ),\n",
    "    (f\"{datasetPath}/E/\",\"E\" ),\n",
    "    (f\"{datasetPath}/F/\",\"F\" ),\n",
    "    (f\"{datasetPath}/G/\",\"G\" ),\n",
    "    (f\"{datasetPath}/H/\",\"H\" ),\n",
    "    (f\"{datasetPath}/I/\",\"I\" ),\n",
    "    (f\"{datasetPath}/verde/\",\"verde\" ),\n",
    "    (f\"{datasetPath}/azul/\",\"azul\" ),\n",
    "    (f\"{datasetPath}/amarillo/\",\"amarillo\" ),\n",
    "    (f\"{datasetPath}/rojo/\",\"rojo\" )\n",
    "]\n",
    "bg_folder = \"/home/jabv/Desktop/bg_imgs/\"\n",
    "output_folder = \"/home/jabv/Desktop/DS_letras/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'verde': 9, 'azul': 10, 'amarillo': 11, 'rojo': 12}\n",
      "[{'id': 0, 'name': 'A'}, {'id': 1, 'name': 'B'}, {'id': 2, 'name': 'C'}, {'id': 3, 'name': 'D'}, {'id': 4, 'name': 'E'}, {'id': 5, 'name': 'F'}, {'id': 6, 'name': 'G'}, {'id': 7, 'name': 'H'}, {'id': 8, 'name': 'I'}, {'id': 9, 'name': 'verde'}, {'id': 10, 'name': 'azul'}, {'id': 11, 'name': 'amarillo'}, {'id': 12, 'name': 'rojo'}]\n"
     ]
    }
   ],
   "source": [
    "objects_list = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"verde\", \"azul\", \"amarillo\", \"rojo\"]\n",
    "annotations_ID = {}\n",
    "categories = []\n",
    "for i, object in enumerate(objects_list):\n",
    "    annotations_ID[object] = i\n",
    "    categories.append({\"id\": i, \"name\": object})\n",
    "\n",
    "print(annotations_ID)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of files in each of the three folders\n",
    "fg_files = {}\n",
    "for folder, category in fg_folders:\n",
    "    fg_files[category] = os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(output_folder)\n",
    "trainfolder = output_folder + \"train/\"\n",
    "testfolder = output_folder + \"test/\"\n",
    "validfolder = output_folder + \"valid/\"\n",
    "os.mkdir(trainfolder)\n",
    "os.mkdir(testfolder)\n",
    "os.mkdir(validfolder)\n",
    "os.mkdir(trainfolder + \"images/\")\n",
    "os.mkdir(trainfolder + \"labels/\")\n",
    "os.mkdir(testfolder + \"images/\")\n",
    "os.mkdir(testfolder + \"labels/\")\n",
    "os.mkdir(validfolder + \"images/\")\n",
    "os.mkdir(validfolder + \"labels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\n",
    "annotations=[]\n",
    "annotations2=[]\n",
    "annot_csv=[]\n",
    "\n",
    "img_id=int(0)\n",
    "anno_id=int(0)\n",
    "\n",
    "rescaling_min = 0.20\n",
    "rescaling_max = 0.70\n",
    "\n",
    "# Ratios at which these values will be modified\n",
    "brightness_ratio = 0.05\n",
    "saturation_ratio = 0.05\n",
    "hue_ratio = 0.02\n",
    "\n",
    "for j in range(20000):\n",
    "    #create empty label file\n",
    "    with open(f'{trainfolder}labels/{img_id}.txt', 'w') as file:\n",
    "        pass\n",
    "    #select hramdomly how many objects will be in an image\n",
    "    num_objects = random.randint(0, 5)\n",
    "    #print(\"number of objects\",num_objects)\n",
    "    # Select random foreground images from the three folders, with replacement\n",
    "    fg_categories = random.choices(objects_list, k=num_objects)\n",
    "    \n",
    "    fg_files_selected = []\n",
    "    for category in fg_categories:\n",
    "        fg_files_selected.append([category,random.choice(fg_files[category])])\n",
    "    #print(\"seleccion\",fg_files_selected)\n",
    "    # Load the selected foreground images using Pillow\n",
    "    fg_imgs = []\n",
    "    for img in fg_files_selected:\n",
    "        folder = [f[0] for f in fg_folders if f[1] == img[0]][0]\n",
    "        fg_imgs.append([img[0],Image.open(folder + img[1]),folder+img[1]])\n",
    "\n",
    "    # Randomly resize and rotate the foreground images using Pillow's transform module\n",
    "    # img[0] is category, img[1] is image, img[2] is path\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "        angle = random.randint(-5, 5)\n",
    "        scale = random.uniform(rescaling_min, rescaling_max)\n",
    "        fg_img = fg_img.rotate(angle, resample=Image.BICUBIC, expand=True)\n",
    "        fg_img = fg_img.resize((int(fg_img.width * scale), int(fg_img.height * scale)))\n",
    "        fg_img = ImageEnhance.Brightness(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = ImageEnhance.Contrast(fg_img).enhance(random.uniform(0.9, 1.1))\n",
    "        fg_img = ImageEnhance.Color(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = fg_img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.0, 0.5)))\n",
    "\n",
    "\n",
    "        img[1] = fg_img\n",
    "\n",
    "    # Load the background image using Pillow\n",
    "    bg_files = os.listdir(bg_folder)\n",
    "    bg_file = random.choice(bg_files)\n",
    "    bg_img = Image.open(bg_folder + bg_file)\n",
    "\n",
    "    # Define the maximum overlap as a percentage\n",
    "    max_overlap_pct = 10\n",
    "\n",
    "    # Define an array to keep track of occupied areas\n",
    "    occupied = np.zeros((bg_img.height, bg_img.width))\n",
    "\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "\n",
    "        # Calculate the maximum overlap area\n",
    "        max_overlap_area = (fg_img.width * fg_img.height)\n",
    "\n",
    "        seg_img = img[1]\n",
    "\n",
    "\n",
    "        # Convert the image to a NumPy array\n",
    "        img_arr = np.array(seg_img)\n",
    "        # Create a binary mask of the non-transparent pixels\n",
    "        mask = img_arr[:, :, 3] != 0\n",
    "\n",
    "        # Convert the mask to a COCO format segmentation\n",
    "        segmentation = []\n",
    "        for i in range(mask.shape[0]):\n",
    "            for j in range(mask.shape[1]):\n",
    "                if mask[i, j]:\n",
    "                    segmentation.append(j)\n",
    "                    segmentation.append(i)\n",
    "        segmentation = [segmentation]\n",
    "\n",
    "        # Calculate the area of the segmentation\n",
    "        area = 0\n",
    "        for i in range(len(segmentation[0]) // 2):\n",
    "            x1 = segmentation[0][2 * i]\n",
    "            y1 = segmentation[0][2 * i + 1]\n",
    "            x2 = segmentation[0][(2 * i + 2) % len(segmentation[0])]\n",
    "            y2 = segmentation[0][(2 * i + 3) % len(segmentation[0])]\n",
    "            area += x1 * y2 - x2 * y1\n",
    "        area = abs(area) / 2\n",
    "        \n",
    "        # Draw the segmentation onto a copy of the original image\n",
    "        #image_copy = image.copy()\n",
    "        #cv2.fillPoly(image_copy, aux_segmentation, color=(0, 255, 0))\n",
    "\n",
    "        # Display the image with segmentation overlay\n",
    "        #cv2.imshow('Image with Segmentation', image_copy)\n",
    "        #cv2.waitKey(0)\n",
    "        #cv2.destroyAllWindows()\n",
    "\n",
    "        # Calculate the maximum allowed position for the top-left corner\n",
    "        max_x = bg_img.width - fg_img.width\n",
    "        max_y = bg_img.height - fg_img.height\n",
    "        area = fg_img.width * fg_img.height\n",
    "\n",
    "        # Generate a random location until an unoccupied area is found that meets the overlap limit\n",
    "        total_area = bg_img.width * bg_img.height\n",
    "        overlap_area = total_area\n",
    "        \n",
    "        while overlap_area / area > max_overlap_pct / 100:\n",
    "            try:\n",
    "                x = random.randint(0, max_x)\n",
    "                y = random.randint(0, max_y)\n",
    "            except:\n",
    "                x = random.randint(0, abs(max_x))\n",
    "                y = random.randint(0, abs(max_y))\n",
    "\n",
    "            # Calculate the overlap area\n",
    "            overlap_area = np.sum(occupied[y:y+fg_img.height, x:x+fg_img.width])\n",
    "\n",
    "            # Check if the area is unoccupied and the overlap limit is not exceeded\n",
    "            if (max_overlap_area) >= overlap_area/10:\n",
    "                break\n",
    "            if i==10:\n",
    "                continue\n",
    "        \n",
    "        for i in range(0, len(segmentation[0])):\n",
    "            if i % 2:\n",
    "                segmentation[0][i]=int(segmentation[0][i]+y)\n",
    "            else :\n",
    "                segmentation[0][i]=int(segmentation[0][i]+x)\n",
    "        # Update the occupied array\n",
    "        occupied[y:y+fg_img.height, x:x+fg_img.width] = 1\n",
    "\n",
    "        bg_img.paste(fg_img, (x, y), fg_img)\n",
    "        x_center_ann = (x+fg_img.width/2) / bg_img.width\n",
    "        y_center_ann = (y+fg_img.height/2) / bg_img.height\n",
    "        width_ann = fg_img.width / bg_img.width\n",
    "        height_ann = fg_img.height / bg_img.height\n",
    "        with open(f'{trainfolder}labels/{img_id}.txt', 'a') as f:\n",
    "            f.write(f\"{annotations_ID[img[0]]} {x_center_ann} {y_center_ann} {width_ann} {height_ann}\\n\")\n",
    "        annotations2.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": segmentation,\"area\": area,\"iscrowd\": 0})\n",
    "        annotations.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": [],\"area\": fg_img.height*fg_img.width,\"iscrowd\": 0})\n",
    "        annot_csv.append([\"TRAIN\", output_folder + str(img_id)+\".jpg\", img[0], x/bg_img.width, y/bg_img.height,\"\",\"\",(x+fg_img.width)/bg_img.width, (y+fg_img.height)/bg_img.height])\n",
    "        anno_id=anno_id+1\n",
    "        #draw = ImageDraw.Draw(bg_img)\n",
    "        #fdraw.rectangle((x, y, x+fg_img.width, y+fg_img.height), outline='red', width=3)\n",
    "    bg_img.save(f\"{trainfolder}images/\"+str(img_id)+\".jpg\", quality=100)\n",
    "    images.append({\"id\": img_id, \"file_name\": str(img_id)+\".jpg\",\"height\": bg_img.height,\"width\": bg_img.width})\n",
    "    img_id=img_id+1\n",
    "    #print(img_id)\n",
    "\n",
    "#making data.yaml\n",
    "data = dict(\n",
    "    train = f\"{trainfolder}images\",\n",
    "    val = f\"{validfolder}images\",\n",
    "    test = f\"{validfolder}images\",\n",
    "    nc = len(annotations_ID),\n",
    "    names = list(annotations_ID.keys())\n",
    "    )\n",
    "#storing\n",
    "with open(f'{output_folder}data.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
