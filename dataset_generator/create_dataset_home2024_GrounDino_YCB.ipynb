{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset for @HOME2024 using GroundingDino and SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageEnhance, ImageFilter, ImageFont\n",
    "from pycocotools import mask\n",
    "import json\n",
    "import yaml\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ultralytics\n",
    "import time\n",
    "import imutils\n",
    "import argparse\n",
    "\n",
    "#grounding imports----------------\n",
    "\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.vl_utils import create_positive_map_from_span\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathtofiles = \"/home/roborregos/Documents/YCB_dataset/ycb_2classes/\"\n",
    "pathtoimage = [f.path for f in os.scandir(pathtofiles) if f.is_dir()]\n",
    "new_width = 720\n",
    "\n",
    "for filepath in pathtoimage:\n",
    "    folder = filepath + \"/\"\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(folder + filename)\n",
    "        aspect_ratio = img.height / img.width\n",
    "        new_height = int(new_width * aspect_ratio)\n",
    "        img = img.resize((new_width, new_height))\n",
    "        img.save(folder + filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto label con Segment anyting y modelo de YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N5_135.jpg', 'N2_306.jpg', 'N5_222.jpg', 'N2_261.jpg', 'N5_216.jpg', 'N5_225.jpg', 'N3_123.jpg', 'N2_336.jpg', 'N5_267.jpg', 'N2_228.jpg', 'N3_294.jpg', 'N3_270.jpg', 'N2_126.jpg', 'N2_141.jpg', 'N2_309.jpg', 'N5_102.jpg', 'N5_54.jpg', 'N4_63.jpg', 'N1_345.jpg', 'N3_282.jpg', 'N5_180.jpg', 'N1_228.jpg', 'N3_249.jpg', 'N5_297.jpg', 'N5_261.jpg', 'N2_30.jpg', 'N4_111.jpg', 'N3_300.jpg', 'N5_240.jpg', 'N2_330.jpg', 'N3_261.jpg', 'N3_150.jpg', 'N1_108.jpg', 'N3_24.jpg', 'N4_123.jpg', 'N3_303.jpg', 'N3_108.jpg', 'N2_288.jpg', 'N5_306.jpg', 'N4_141.jpg', 'N5_63.jpg', 'N2_3.jpg', 'N1_84.jpg', 'N5_120.jpg', 'N5_162.jpg', 'N1_129.jpg', 'N1_261.jpg', 'N3_264.jpg', 'N4_261.jpg', 'N1_171.jpg', 'N4_234.jpg', 'N2_354.jpg', 'N5_18.jpg', 'N2_93.jpg', 'N2_243.jpg', 'N2_216.jpg', 'N5_288.jpg', 'N4_99.jpg', 'N4_282.jpg', 'N2_273.jpg', 'N4_48.jpg', 'N1_102.jpg', 'N2_144.jpg', 'N3_183.jpg', 'N4_114.jpg', 'N4_279.jpg', 'N3_126.jpg', 'N1_330.jpg', 'N5_204.jpg', 'N1_324.jpg', 'N2_327.jpg', 'N3_324.jpg', 'N2_75.jpg', 'N3_210.jpg', 'N1_222.jpg', 'N1_81.jpg', 'N4_312.jpg', 'N5_351.jpg', 'N5_285.jpg', 'N4_24.jpg', 'N4_15.jpg', 'N3_111.jpg', 'N5_42.jpg', 'N5_66.jpg', 'N5_309.jpg', 'N4_324.jpg', 'N3_36.jpg', 'N2_255.jpg', 'N4_339.jpg', 'N3_255.jpg', 'N3_90.jpg', 'N5_84.jpg', 'N2_192.jpg', 'N2_183.jpg', 'N1_195.jpg', 'N1_150.jpg', 'N1_153.jpg', 'N5_165.jpg', 'N4_204.jpg', 'N5_87.jpg', 'N5_117.jpg', 'N5_75.jpg', 'N2_213.jpg', 'N5_231.jpg', 'N5_72.jpg', 'N4_210.jpg', 'N3_138.jpg', 'N3_84.jpg', 'N1_336.jpg', 'N1_243.jpg', 'N2_171.jpg', 'N1_294.jpg', 'N1_99.jpg', 'N2_315.jpg', 'N1_105.jpg', 'N4_228.jpg', 'N4_318.jpg', 'N2_177.jpg', 'N4_165.jpg', 'N1_288.jpg', 'N5_90.jpg', 'N3_171.jpg', 'N4_102.jpg', 'N3_57.jpg', 'N1_303.jpg', 'N2_63.jpg', 'N4_183.jpg', 'N5_321.jpg', 'N3_12.jpg', 'N5_195.jpg', 'N2_234.jpg', 'N1_348.jpg', 'N3_327.jpg', 'N3_87.jpg', 'N1_318.jpg', 'N1_342.jpg', 'N3_54.jpg', 'N3_234.jpg', 'N5_192.jpg', 'N4_30.jpg', 'N5_123.jpg', 'N3_75.jpg', 'N2_162.jpg', 'N3_30.jpg', 'N5_21.jpg', 'N2_303.jpg', 'N4_3.jpg', 'N1_180.jpg', 'N2_117.jpg', 'N5_45.jpg', 'N3_318.jpg', 'N3_201.jpg', 'N4_39.jpg', 'N5_15.jpg', 'N2_57.jpg', 'N5_147.jpg', 'N1_306.jpg', 'N5_138.jpg', 'N5_9.jpg', 'N3_60.jpg', 'N3_222.jpg', 'N3_45.jpg', 'N3_192.jpg', 'N2_168.jpg', 'N3_225.jpg', 'N3_279.jpg', 'N1_126.jpg', 'N1_15.jpg', 'N4_51.jpg', 'N4_354.jpg', 'N1_87.jpg', 'N1_156.jpg', 'N1_138.jpg', 'N2_258.jpg', 'N1_192.jpg', 'N3_180.jpg', 'N3_27.jpg', 'N2_204.jpg', 'N2_60.jpg', 'N4_258.jpg', 'N3_72.jpg', 'N3_33.jpg', 'N2_87.jpg', 'N1_249.jpg', 'N1_201.jpg', 'N1_36.jpg', 'N3_342.jpg', 'N5_108.jpg', 'N3_288.jpg', 'N5_111.jpg', 'N4_18.jpg', 'N3_165.jpg', 'N5_126.jpg', 'N1_309.jpg', 'N2_165.jpg', 'N4_6.jpg', 'N3_228.jpg', 'N4_255.jpg', 'N1_357.jpg', 'N4_78.jpg', 'N4_171.jpg', 'N5_318.jpg', 'N3_207.jpg', 'N5_243.jpg', 'N4_273.jpg', 'N3_105.jpg', 'N4_36.jpg', 'N1_141.jpg', 'N4_135.jpg', 'N5_324.jpg', 'N3_3.jpg', 'N1_96.jpg', 'N2_297.jpg', 'N5_99.jpg', 'N4_126.jpg', 'N2_237.jpg', 'N4_207.jpg', 'N4_162.jpg', 'N2_48.jpg', 'N5_333.jpg', 'N4_57.jpg', 'N4_33.jpg', 'N5_159.jpg', 'N4_129.jpg', 'N5_189.jpg', 'N5_279.jpg', 'N4_189.jpg', 'N4_132.jpg', 'N2_66.jpg', 'N4_150.jpg', 'N2_111.jpg', 'N2_186.jpg', 'N1_78.jpg', 'N3_345.jpg', 'N2_333.jpg', 'N5_129.jpg', 'N1_315.jpg', 'N2_150.jpg', 'N3_9.jpg', 'N2_189.jpg', 'N2_270.jpg', 'N1_75.jpg', 'N5_144.jpg', 'N3_231.jpg', 'N2_264.jpg', 'N5_237.jpg', 'N2_231.jpg', 'N5_282.jpg', 'N5_357.jpg', 'N4_54.jpg', 'N5_6.jpg', 'N3_66.jpg', 'N1_135.jpg', 'N5_330.jpg', 'N5_348.jpg', 'N5_171.jpg', 'N3_162.jpg', 'N3_63.jpg', 'N1_165.jpg', 'N1_210.jpg', 'N4_348.jpg', 'N1_24.jpg', 'N5_207.jpg', 'N1_42.jpg', 'N4_9.jpg', 'N4_144.jpg', 'N4_12.jpg', 'N3_357.jpg', 'N4_168.jpg', 'N1_45.jpg', 'N2_225.jpg', 'N4_321.jpg', 'N1_120.jpg', 'N1_207.jpg', 'N5_93.jpg', 'N4_159.jpg', 'N3_354.jpg', 'N5_153.jpg', 'N4_231.jpg', 'N5_342.jpg', 'N4_246.jpg', 'N3_336.jpg', 'N4_42.jpg', 'N2_78.jpg', 'N3_114.jpg', 'N3_6.jpg', 'N1_72.jpg', 'N5_78.jpg', 'N5_60.jpg', 'N4_96.jpg', 'N5_264.jpg', 'N5_57.jpg', 'N2_282.jpg', 'N5_168.jpg', 'N1_27.jpg', 'N2_201.jpg', 'N3_258.jpg', 'N5_312.jpg', 'N5_141.jpg', 'N5_339.jpg', 'N5_303.jpg', 'N5_96.jpg', 'N3_120.jpg', 'N1_321.jpg', 'N3_102.jpg', 'N4_93.jpg', 'N4_351.jpg', 'N5_246.jpg', 'N3_351.jpg', 'N1_327.jpg', 'N4_120.jpg', 'N2_6.jpg', 'N1_12.jpg', 'N1_300.jpg', 'N5_228.jpg', 'N4_186.jpg', 'N2_96.jpg', 'N4_75.jpg', 'N5_273.jpg', 'N1_51.jpg', 'N3_195.jpg', 'N2_0.jpg', 'N1_6.jpg', 'N4_342.jpg', 'N4_243.jpg', 'N3_333.jpg', 'N2_156.jpg', 'N5_354.jpg', 'N4_66.jpg', 'N1_351.jpg', 'N2_207.jpg', 'N4_156.jpg', 'N2_222.jpg', 'N1_216.jpg', 'N4_192.jpg', 'N1_240.jpg', 'N5_300.jpg', 'N1_123.jpg', 'N3_153.jpg', 'N4_21.jpg', 'N5_81.jpg', 'N5_201.jpg', 'N4_0.jpg', 'N2_321.jpg', 'N2_69.jpg', 'N2_21.jpg', 'N4_84.jpg', 'N2_159.jpg', 'N2_123.jpg', 'N5_234.jpg', 'N3_21.jpg', 'N3_129.jpg', 'N2_174.jpg', 'N1_354.jpg', 'N2_339.jpg', 'N1_270.jpg', 'N4_267.jpg', 'N1_276.jpg', 'N4_327.jpg', 'N4_300.jpg', 'N3_96.jpg', 'N3_93.jpg', 'N4_213.jpg', 'N1_63.jpg', 'N4_306.jpg', 'N2_45.jpg', 'N5_213.jpg', 'N3_0.jpg', 'N4_108.jpg', 'N2_105.jpg', 'N3_42.jpg', 'N3_237.jpg', 'N4_330.jpg', 'N5_270.jpg', 'N5_156.jpg', 'N5_291.jpg', 'N1_231.jpg', 'N5_249.jpg', 'N4_153.jpg', 'N2_84.jpg', 'N1_234.jpg', 'N1_291.jpg', 'N2_9.jpg', 'N4_240.jpg', 'N2_129.jpg', 'N3_348.jpg', 'N2_246.jpg', 'N2_39.jpg', 'N4_252.jpg', 'N3_297.jpg', 'N1_18.jpg', 'N2_249.jpg', 'N5_0.jpg', 'N2_36.jpg', 'N3_186.jpg', 'N1_267.jpg', 'N2_138.jpg', 'N4_276.jpg', 'N5_219.jpg', 'N4_177.jpg', 'N1_198.jpg', 'N2_279.jpg', 'N5_114.jpg', 'N4_285.jpg', 'N4_60.jpg', 'N5_294.jpg', 'N2_195.jpg', 'N2_252.jpg', 'N2_108.jpg', 'N5_210.jpg', 'N2_120.jpg', 'N3_117.jpg', 'N1_285.jpg', 'N4_237.jpg', 'N3_78.jpg', 'N5_30.jpg', 'N1_144.jpg', 'N5_198.jpg', 'N1_312.jpg', 'N1_39.jpg', 'N3_18.jpg', 'N3_213.jpg', 'N2_294.jpg', 'N2_42.jpg', 'N3_273.jpg', 'N2_285.jpg', 'N1_219.jpg', 'N4_27.jpg', 'N1_333.jpg', 'N5_183.jpg', 'N3_39.jpg', 'N3_312.jpg', 'N1_117.jpg', 'N4_69.jpg', 'N2_12.jpg', 'N3_306.jpg', 'N3_315.jpg', 'N5_27.jpg', 'N2_276.jpg', 'N5_315.jpg', 'N4_294.jpg', 'N3_15.jpg', 'N1_33.jpg', 'N5_345.jpg', 'N3_219.jpg', 'N3_267.jpg', 'N1_147.jpg', 'N1_66.jpg', 'N3_168.jpg', 'N1_186.jpg', 'N2_318.jpg', 'N4_117.jpg', 'N3_276.jpg', 'N5_33.jpg', 'N4_147.jpg', 'N3_285.jpg', 'N3_144.jpg', 'N3_198.jpg', 'N2_291.jpg', 'N2_81.jpg', 'N3_189.jpg', 'N2_357.jpg', 'N4_309.jpg', 'N4_201.jpg', 'N5_39.jpg', 'N1_0.jpg', 'N4_225.jpg', 'N4_345.jpg', 'N2_267.jpg', 'N1_168.jpg', 'N4_297.jpg', 'N5_24.jpg', 'N1_339.jpg', 'N1_114.jpg', 'N2_33.jpg', 'N2_348.jpg', 'N1_93.jpg', 'N4_249.jpg', 'N2_99.jpg', 'N3_330.jpg', 'N4_87.jpg', 'N3_309.jpg', 'N2_210.jpg', 'N2_51.jpg', 'N5_12.jpg', 'N2_27.jpg', 'N2_300.jpg', 'N5_174.jpg', 'N2_240.jpg', 'N4_45.jpg', 'N5_177.jpg', 'N3_204.jpg', 'N1_21.jpg', 'N2_153.jpg', 'N1_48.jpg', 'N5_3.jpg', 'N2_345.jpg', 'N3_81.jpg', 'N1_246.jpg', 'N3_339.jpg', 'N1_273.jpg', 'N1_213.jpg', 'N2_15.jpg', 'N4_105.jpg', 'N2_114.jpg', 'N1_282.jpg', 'N3_147.jpg', 'N1_90.jpg', 'N1_237.jpg', 'N2_24.jpg', 'N2_18.jpg', 'N2_135.jpg', 'N2_312.jpg', 'N3_69.jpg', 'N2_342.jpg', 'N3_135.jpg', 'N4_219.jpg', 'N1_30.jpg', 'N1_9.jpg', 'N5_48.jpg', 'N2_72.jpg', 'N1_3.jpg', 'N3_51.jpg', 'N3_48.jpg', 'N4_180.jpg', 'N3_99.jpg', 'N3_321.jpg', 'N5_36.jpg', 'N1_255.jpg', 'N4_81.jpg', 'N3_174.jpg', 'N2_219.jpg', 'N5_51.jpg', 'N1_183.jpg', 'N5_105.jpg', 'N1_162.jpg', 'N1_177.jpg', 'N3_141.jpg', 'N3_216.jpg', 'N4_90.jpg', 'N5_276.jpg', 'N2_198.jpg', 'N4_198.jpg', 'N4_138.jpg', 'N5_252.jpg', 'N5_186.jpg', 'N1_159.jpg', 'N3_240.jpg', 'N2_180.jpg', 'N3_252.jpg', 'N4_270.jpg', 'N4_303.jpg', 'N1_279.jpg', 'N1_69.jpg', 'N1_297.jpg', 'N5_258.jpg', 'N3_243.jpg', 'N2_132.jpg', 'N4_333.jpg', 'N3_132.jpg', 'N4_357.jpg', 'N4_264.jpg', 'N1_204.jpg', 'N4_315.jpg', 'N2_54.jpg', 'N1_54.jpg', 'N4_72.jpg', 'N3_246.jpg', 'N1_252.jpg', 'N3_156.jpg', 'N4_291.jpg', 'N1_57.jpg', 'N5_69.jpg', 'N5_255.jpg', 'N1_111.jpg', 'N5_132.jpg', 'N4_195.jpg', 'N1_60.jpg', 'N5_327.jpg', 'N1_189.jpg', 'N1_258.jpg', 'N1_264.jpg', 'N4_174.jpg', 'N4_336.jpg', 'N3_177.jpg', 'N2_147.jpg', 'N4_216.jpg', 'N3_291.jpg', 'N1_174.jpg', 'N2_90.jpg', 'N1_132.jpg', 'N2_351.jpg', 'N4_222.jpg', 'N2_324.jpg', 'N5_336.jpg', 'N3_159.jpg', 'N5_150.jpg', 'N4_288.jpg', 'N2_102.jpg', 'N1_225.jpg']\n",
      "Processing image: N5_135.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roborregos/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:1052: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box: 329, 235, 411, 331\n",
      "Bounding box: 410, 151, 555, 309\n",
      "Bounding box: 159, 48, 574, 467\n",
      "Processing image: N2_306.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 327, 67, 415, 279\n",
      "Processing image: N5_222.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 315, 234, 397, 330\n",
      "Bounding box: 324, 300, 483, 447\n",
      "Bounding box: 159, 48, 575, 467\n",
      "Processing image: N2_261.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 69, 413, 284\n",
      "Bounding box: 192, 277, 431, 458\n",
      "Processing image: N5_216.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 331, 291, 497, 447\n",
      "Bounding box: 315, 235, 398, 331\n",
      "Bounding box: 158, 48, 575, 466\n",
      "Processing image: N5_225.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 314, 234, 397, 330\n",
      "Bounding box: 320, 304, 475, 446\n",
      "Bounding box: 159, 48, 575, 466\n",
      "Processing image: N3_123.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 330, 110, 420, 288\n",
      "Processing image: N2_336.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 65, 419, 278\n",
      "Processing image: N5_267.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 312, 228, 394, 323\n",
      "Bounding box: 229, 284, 399, 448\n",
      "Bounding box: 157, 47, 574, 465\n",
      "Processing image: N2_228.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 326, 71, 416, 288\n",
      "Bounding box: 312, 294, 530, 450\n",
      "Processing image: N3_294.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 312, 102, 401, 279\n",
      "Bounding box: 159, 234, 352, 426\n",
      "Processing image: N3_270.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 311, 105, 400, 282\n",
      "Bounding box: 188, 269, 400, 470\n",
      "Processing image: N2_126.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 343, 71, 432, 288\n",
      "Bounding box: 353, 185, 409, 259\n",
      "Processing image: N2_141.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 342, 72, 430, 289\n",
      "Processing image: N2_309.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 328, 66, 416, 279\n",
      "Bounding box: 159, 226, 328, 365\n",
      "Processing image: N5_102.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 358, 81, 536, 260\n",
      "Bounding box: 332, 232, 414, 326\n",
      "Bounding box: 159, 49, 574, 469\n",
      "Processing image: N5_54.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 330, 225, 412, 318\n",
      "Bounding box: 158, 48, 575, 468\n",
      "Processing image: N4_63.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 324, 177, 410, 294\n",
      "Processing image: N1_345.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 99, 423, 344\n",
      "Processing image: N3_282.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 311, 103, 400, 280\n",
      "Processing image: N5_180.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 385, 234, 555, 406\n",
      "Bounding box: 322, 237, 403, 333\n",
      "Bounding box: 158, 48, 575, 467\n",
      "Processing image: N1_228.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 324, 99, 417, 349\n",
      "Bounding box: 300, 344, 590, 460\n",
      "Processing image: N3_249.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 311, 107, 400, 285\n",
      "Bounding box: 245, 296, 439, 474\n",
      "Processing image: N5_297.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 185, 240, 350, 410\n",
      "Bounding box: 313, 224, 395, 318\n",
      "Bounding box: 158, 47, 575, 467\n",
      "Processing image: N5_261.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 312, 229, 394, 325\n",
      "Bounding box: 242, 291, 408, 450\n",
      "Bounding box: 158, 48, 575, 466\n",
      "Processing image: N2_30.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 342, 66, 429, 279\n",
      "Processing image: N4_111.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 324, 186, 409, 301\n",
      "Bounding box: 370, 105, 545, 276\n",
      "Processing image: N3_300.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 313, 101, 402, 278\n",
      "Bounding box: 157, 225, 340, 410\n",
      "Processing image: N5_240.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 313, 232, 395, 328\n",
      "Bounding box: 158, 47, 575, 468\n",
      "Processing image: N2_330.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 331, 65, 418, 278\n",
      "Processing image: N3_261.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 311, 106, 400, 283\n",
      "Bounding box: 208, 281, 417, 475\n",
      "Processing image: N3_150.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 326, 112, 416, 291\n",
      "Processing image: N1_108.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 345, 99, 437, 349\n",
      "Bounding box: 363, 212, 424, 305\n",
      "Processing image: N3_24.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 327, 98, 415, 276\n",
      "Processing image: N4_123.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 323, 188, 408, 303\n",
      "Bounding box: 389, 127, 555, 295\n",
      "Processing image: N3_303.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 313, 101, 402, 278\n",
      "Processing image: N3_108.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 107, 421, 286\n",
      "Processing image: N2_288.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 326, 67, 413, 281\n",
      "Bounding box: 153, 246, 370, 419\n",
      "Processing image: N5_306.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 314, 223, 396, 317\n",
      "Bounding box: 180, 227, 337, 391\n",
      "Bounding box: 158, 48, 575, 467\n",
      "Processing image: N4_141.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 321, 190, 406, 305\n",
      "Processing image: N5_63.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 331, 227, 413, 319\n",
      "Bounding box: 297, 70, 447, 210\n",
      "Bounding box: 159, 48, 573, 468\n",
      "Processing image: N2_3.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 337, 65, 424, 278\n",
      "Processing image: N1_84.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 346, 99, 437, 347\n",
      "Processing image: N5_120.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 387, 114, 554, 287\n",
      "Bounding box: 330, 234, 412, 329\n",
      "Bounding box: 159, 49, 573, 467\n",
      "Processing image: N5_162.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 236, 407, 333\n",
      "Bounding box: 408, 206, 561, 366\n",
      "Bounding box: 159, 48, 574, 470\n",
      "Processing image: N1_129.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 342, 99, 435, 350\n",
      "Bounding box: 351, 212, 411, 307\n",
      "Processing image: N1_261.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 323, 99, 415, 347\n",
      "Bounding box: 133, 334, 445, 466\n",
      "Processing image: N3_264.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 311, 105, 400, 283\n",
      "Bounding box: 201, 277, 412, 475\n",
      "Processing image: N4_261.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 304, 183, 389, 297\n",
      "Bounding box: 223, 294, 405, 474\n",
      "Processing image: N1_171.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 335, 98, 428, 350\n",
      "Processing image: N4_234.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 306, 187, 391, 301\n",
      "Bounding box: 299, 320, 453, 465\n",
      "Processing image: N2_354.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 335, 65, 423, 277\n",
      "Processing image: N5_18.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 197, 83, 373, 260\n",
      "Bounding box: 325, 221, 407, 314\n",
      "Bounding box: 159, 49, 574, 469\n",
      "Processing image: N2_93.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 346, 69, 434, 285\n",
      "Processing image: N2_243.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 70, 414, 286\n",
      "Bounding box: 255, 296, 465, 452\n",
      "Processing image: N2_216.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 328, 72, 417, 288\n",
      "Bounding box: 334, 279, 571, 454\n",
      "Processing image: N5_288.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 195, 254, 364, 426\n",
      "Bounding box: 312, 225, 394, 320\n",
      "Bounding box: 157, 47, 575, 467\n",
      "Processing image: N4_99.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 184, 410, 299\n",
      "Bounding box: 349, 88, 526, 257\n",
      "Processing image: N4_282.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 305, 180, 390, 294\n",
      "Bounding box: 184, 264, 369, 454\n",
      "Processing image: N2_273.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 69, 413, 283\n",
      "Bounding box: 168, 263, 405, 447\n",
      "Processing image: N4_48.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 323, 175, 408, 292\n",
      "Processing image: N1_102.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 345, 99, 438, 348\n",
      "Bounding box: 366, 212, 427, 305\n",
      "Processing image: N2_144.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 341, 72, 430, 289\n",
      "Processing image: N3_183.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 321, 112, 410, 291\n",
      "Bounding box: 388, 228, 588, 421\n",
      "Processing image: N4_114.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 324, 186, 409, 302\n",
      "Bounding box: 374, 109, 548, 281\n",
      "Processing image: N4_279.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 305, 180, 390, 295\n",
      "Bounding box: 189, 269, 374, 459\n",
      "Processing image: N3_126.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 330, 110, 420, 289\n",
      "Bounding box: 402, 135, 577, 286\n",
      "Processing image: N1_330.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 329, 99, 421, 344\n",
      "Processing image: N5_204.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 350, 273, 524, 440\n",
      "Bounding box: 318, 236, 399, 332\n",
      "Bounding box: 158, 47, 576, 466\n",
      "Processing image: N1_324.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 329, 99, 419, 345\n",
      "Processing image: N2_327.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 331, 66, 418, 278\n",
      "Processing image: N3_324.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 316, 99, 405, 276\n",
      "Processing image: N2_75.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 345, 68, 434, 283\n",
      "Processing image: N3_210.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 316, 111, 406, 289\n",
      "Bounding box: 339, 271, 551, 467\n",
      "Processing image: N1_222.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 99, 418, 349\n",
      "Processing image: N1_81.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 346, 99, 437, 347\n",
      "Processing image: N4_312.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 307, 175, 392, 291\n",
      "Bounding box: 167, 219, 319, 386\n",
      "Processing image: N5_351.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 320, 220, 403, 313\n",
      "Bounding box: 174, 134, 335, 303\n",
      "Processing image: N5_285.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 197, 259, 369, 430\n",
      "Bounding box: 312, 225, 394, 320\n",
      "Bounding box: 157, 47, 575, 466\n",
      "Processing image: N4_24.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 320, 173, 404, 290\n",
      "Processing image: N4_15.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 318, 172, 403, 289\n",
      "Processing image: N3_111.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 108, 421, 287\n",
      "Processing image: N5_42.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 329, 223, 411, 316\n",
      "Bounding box: 247, 68, 409, 224\n",
      "Bounding box: 157, 49, 575, 468\n",
      "Processing image: N5_66.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 331, 227, 413, 319\n",
      "Bounding box: 302, 67, 456, 213\n",
      "Bounding box: 158, 48, 573, 468\n",
      "Processing image: N5_309.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 315, 223, 397, 316\n",
      "Bounding box: 180, 223, 333, 384\n",
      "Bounding box: 158, 48, 575, 467\n",
      "Processing image: N4_324.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 309, 174, 394, 290\n",
      "Processing image: N3_36.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 328, 99, 417, 277\n",
      "Processing image: N2_255.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 70, 413, 285\n",
      "Bounding box: 210, 284, 444, 459\n",
      "Processing image: N4_339.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 312, 172, 396, 289\n",
      "Processing image: N3_255.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 311, 107, 401, 284\n",
      "Bounding box: 225, 289, 428, 476\n",
      "Processing image: N3_90.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 333, 105, 423, 284\n",
      "Processing image: N5_84.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 328, 65, 502, 234\n",
      "Bounding box: 332, 229, 414, 323\n",
      "Bounding box: 159, 48, 575, 467\n",
      "Processing image: N2_192.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 72, 421, 289\n",
      "Processing image: N2_183.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 334, 72, 423, 290\n",
      "Bounding box: 401, 238, 618, 407\n",
      "Processing image: N1_195.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 330, 99, 423, 350\n",
      "Processing image: N1_150.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 339, 99, 432, 350\n",
      "Processing image: N1_153.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 338, 99, 432, 350\n",
      "Processing image: N5_165.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 324, 237, 406, 333\n",
      "Bounding box: 404, 211, 561, 373\n",
      "Bounding box: 158, 48, 575, 469\n",
      "Processing image: N4_204.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 310, 191, 395, 304\n",
      "Bounding box: 344, 275, 533, 462\n",
      "Processing image: N5_87.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 229, 414, 323\n",
      "Bounding box: 333, 67, 509, 239\n",
      "Bounding box: 159, 48, 575, 468\n",
      "Processing image: N5_117.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 383, 107, 552, 282\n",
      "Bounding box: 331, 233, 413, 328\n",
      "Bounding box: 159, 49, 573, 468\n",
      "Processing image: N5_75.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 228, 414, 321\n",
      "Bounding box: 315, 64, 480, 224\n",
      "Processing image: N2_213.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 329, 72, 417, 289\n",
      "Bounding box: 340, 276, 580, 452\n",
      "Processing image: N5_231.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 312, 312, 458, 444\n",
      "Bounding box: 314, 233, 396, 329\n",
      "Bounding box: 159, 48, 574, 468\n",
      "Processing image: N5_72.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 331, 228, 413, 320\n",
      "Bounding box: 310, 65, 473, 219\n",
      "Bounding box: 160, 48, 574, 467\n",
      "Processing image: N4_210.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 309, 190, 394, 304\n",
      "Bounding box: 334, 285, 520, 467\n",
      "Processing image: N3_138.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 328, 111, 418, 290\n",
      "Processing image: N3_84.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 104, 422, 283\n",
      "Processing image: N1_336.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 331, 99, 422, 344\n",
      "Bounding box: 356, 209, 414, 302\n",
      "Processing image: N1_243.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 323, 99, 416, 349\n",
      "Processing image: N2_171.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 336, 72, 426, 290\n",
      "Bounding box: 422, 226, 612, 377\n",
      "Processing image: N1_294.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 324, 99, 415, 346\n",
      "Bounding box: 101, 314, 350, 426\n",
      "Processing image: N1_99.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 345, 99, 438, 348\n",
      "Bounding box: 368, 211, 429, 304\n",
      "Processing image: N2_315.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 329, 66, 416, 279\n",
      "Processing image: N1_105.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 345, 99, 437, 349\n",
      "Bounding box: 364, 211, 426, 305\n",
      "Processing image: N4_228.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 306, 188, 392, 302\n",
      "Bounding box: 306, 312, 472, 468\n",
      "Processing image: N4_318.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 308, 174, 393, 290\n",
      "Processing image: N2_177.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 335, 72, 424, 290\n",
      "Bounding box: 413, 231, 616, 392\n",
      "Processing image: N4_165.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 317, 191, 402, 306\n",
      "Bounding box: 403, 212, 563, 383\n",
      "Processing image: N1_288.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 324, 99, 415, 346\n",
      "Bounding box: 100, 317, 367, 437\n",
      "Processing image: N5_90.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 339, 68, 515, 243\n",
      "Bounding box: 332, 230, 414, 324\n",
      "Bounding box: 159, 48, 574, 467\n",
      "Processing image: N3_171.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 323, 113, 412, 291\n",
      "Bounding box: 407, 211, 587, 390\n",
      "Processing image: N4_102.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 184, 410, 300\n",
      "Bounding box: 355, 92, 531, 262\n",
      "Processing image: N3_57.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 331, 100, 421, 279\n",
      "Processing image: N1_303.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 99, 416, 345\n",
      "Processing image: N2_63.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 345, 67, 434, 282\n",
      "Processing image: N4_183.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 314, 191, 399, 306\n",
      "Bounding box: 377, 241, 560, 428\n",
      "Processing image: N5_321.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 316, 222, 398, 315\n",
      "Bounding box: 180, 205, 317, 352\n",
      "Processing image: N3_12.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 98, 413, 275\n",
      "Processing image: N5_195.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 363, 259, 538, 430\n",
      "Bounding box: 319, 236, 401, 333\n",
      "Bounding box: 158, 48, 576, 466\n",
      "Processing image: N2_234.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 71, 415, 287\n",
      "Bounding box: 297, 300, 506, 445\n",
      "Processing image: N1_348.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 333, 99, 424, 344\n",
      "Bounding box: 352, 210, 412, 301\n",
      "Processing image: N3_327.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 317, 99, 405, 276\n",
      "Processing image: N3_87.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 333, 105, 422, 283\n",
      "Processing image: N1_318.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 327, 99, 418, 345\n",
      "Processing image: N1_342.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 99, 423, 344\n",
      "Processing image: N3_54.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 331, 100, 421, 279\n",
      "Processing image: N3_234.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 312, 109, 402, 287\n",
      "Bounding box: 300, 306, 478, 465\n",
      "Processing image: N5_192.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 368, 253, 544, 426\n",
      "Bounding box: 158, 48, 575, 466\n",
      "Bounding box: 319, 237, 401, 333\n",
      "Processing image: N4_30.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 321, 174, 405, 290\n",
      "Bounding box: 219, 84, 387, 242\n",
      "Processing image: N5_123.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 392, 121, 555, 292\n",
      "Bounding box: 330, 234, 412, 329\n",
      "Bounding box: 159, 49, 573, 467\n",
      "Processing image: N3_75.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 103, 422, 281\n",
      "Processing image: N2_162.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 338, 72, 427, 290\n",
      "Processing image: N3_30.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 328, 99, 416, 276\n",
      "Processing image: N5_21.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 202, 80, 378, 255\n",
      "Bounding box: 326, 222, 408, 314\n",
      "Bounding box: 160, 49, 574, 467\n",
      "Processing image: N2_303.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 327, 67, 415, 280\n",
      "Bounding box: 156, 230, 339, 382\n",
      "Processing image: N4_3.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 317, 172, 400, 289\n",
      "Bounding box: 173, 116, 347, 286\n",
      "Processing image: N1_180.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 333, 98, 426, 350\n",
      "Processing image: N2_117.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 344, 71, 433, 288\n",
      "Bounding box: 358, 185, 414, 259\n",
      "Processing image: N5_45.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 329, 224, 411, 316\n",
      "Bounding box: 159, 49, 575, 467\n",
      "Bounding box: 255, 68, 413, 220\n",
      "Processing image: N3_318.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 315, 99, 404, 276\n",
      "Processing image: N3_201.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 317, 112, 407, 290\n",
      "Processing image: N4_39.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 322, 175, 406, 291\n",
      "Processing image: N5_15.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 221, 407, 314\n",
      "Bounding box: 158, 49, 574, 469\n",
      "Bounding box: 192, 87, 368, 264\n",
      "Processing image: N2_57.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 345, 67, 433, 281\n",
      "Processing image: N5_147.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 327, 236, 409, 332\n",
      "Bounding box: 423, 185, 551, 324\n",
      "Processing image: N1_306.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 326, 99, 417, 345\n",
      "Bounding box: 107, 308, 318, 402\n",
      "Processing image: N5_138.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 328, 235, 410, 331\n",
      "Bounding box: 413, 159, 555, 314\n",
      "Processing image: N5_9.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 324, 221, 406, 314\n",
      "Bounding box: 186, 98, 359, 275\n",
      "Bounding box: 159, 49, 573, 467\n",
      "Processing image: N3_60.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 101, 421, 280\n",
      "Processing image: N3_222.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 314, 110, 403, 288\n",
      "Bounding box: 319, 289, 519, 471\n",
      "Processing image: N3_45.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 330, 99, 419, 278\n",
      "Processing image: N3_192.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 319, 112, 408, 290\n",
      "Processing image: N2_168.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 337, 72, 426, 290\n",
      "Processing image: N3_225.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 313, 110, 403, 288\n",
      "Processing image: N3_279.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 311, 104, 400, 281\n",
      "Processing image: N1_126.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 343, 99, 435, 350\n",
      "Bounding box: 352, 212, 414, 306\n",
      "Processing image: N1_15.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 338, 99, 429, 344\n",
      "Processing image: N4_51.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 323, 175, 408, 293\n",
      "Processing image: N4_354.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 315, 172, 399, 289\n",
      "Bounding box: 165, 133, 334, 301\n",
      "Processing image: N1_87.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 346, 99, 438, 347\n",
      "Processing image: N1_156.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 338, 99, 431, 350\n",
      "Processing image: N1_138.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 341, 99, 434, 350\n",
      "Processing image: N2_258.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 69, 413, 284\n",
      "Bounding box: 201, 280, 437, 459\n",
      "Processing image: N1_192.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 331, 99, 424, 350\n",
      "Bounding box: 391, 317, 671, 440\n",
      "Processing image: N3_180.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 321, 113, 411, 291\n",
      "Bounding box: 393, 224, 588, 414\n",
      "Processing image: N3_27.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 328, 98, 416, 276\n",
      "Processing image: N2_204.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 331, 72, 419, 289\n",
      "Processing image: N2_60.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 345, 67, 434, 281\n",
      "Processing image: N4_258.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 304, 183, 390, 298\n",
      "Bounding box: 230, 299, 410, 474\n",
      "Processing image: N3_72.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 333, 103, 422, 281\n",
      "Processing image: N3_33.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 329, 99, 417, 277\n",
      "Processing image: N2_87.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 346, 68, 434, 284\n",
      "Processing image: N1_249.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 323, 99, 415, 348\n",
      "Bounding box: 182, 342, 476, 465\n",
      "Processing image: N1_201.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 329, 99, 422, 350\n",
      "Processing image: N1_36.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 342, 99, 433, 345\n",
      "Processing image: N3_342.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 319, 98, 408, 275\n",
      "Processing image: N5_108.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 369, 90, 544, 270\n",
      "Bounding box: 331, 232, 413, 327\n",
      "Bounding box: 158, 49, 573, 467\n",
      "Processing image: N3_288.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 312, 102, 401, 280\n",
      "Bounding box: 162, 242, 363, 440\n",
      "Processing image: N5_111.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 374, 95, 547, 274\n",
      "Bounding box: 331, 232, 413, 327\n",
      "Bounding box: 158, 49, 573, 468\n",
      "Processing image: N4_18.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 319, 173, 403, 290\n",
      "Processing image: N3_165.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 324, 113, 414, 291\n",
      "Processing image: N5_126.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 397, 127, 556, 297\n",
      "Bounding box: 330, 234, 412, 330\n",
      "Bounding box: 159, 49, 573, 468\n",
      "Processing image: N1_309.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 326, 99, 417, 345\n",
      "Processing image: N2_165.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 338, 72, 426, 290\n",
      "Processing image: N4_6.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 317, 172, 401, 289\n",
      "Bounding box: 176, 111, 352, 281\n",
      "Processing image: N3_228.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 313, 110, 402, 288\n",
      "Bounding box: 308, 299, 498, 469\n",
      "Processing image: N4_255.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 304, 184, 390, 298\n",
      "Bounding box: 238, 303, 415, 474\n",
      "Processing image: N1_357.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 335, 99, 426, 344\n",
      "Bounding box: 349, 210, 410, 302\n",
      "Processing image: N4_78.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 325, 181, 410, 296\n",
      "Bounding box: 315, 77, 481, 230\n",
      "Processing image: N4_171.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 316, 191, 401, 306\n",
      "Bounding box: 394, 221, 564, 399\n",
      "Processing image: N5_318.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 316, 222, 397, 315\n",
      "Bounding box: 178, 210, 321, 361\n",
      "Bounding box: 157, 48, 575, 467\n",
      "Processing image: N3_207.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 316, 112, 406, 289\n",
      "Bounding box: 344, 266, 558, 465\n",
      "Processing image: N5_243.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 313, 232, 394, 327\n",
      "Bounding box: 289, 312, 433, 445\n",
      "Processing image: N4_273.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 304, 181, 390, 296\n",
      "Bounding box: 197, 277, 384, 465\n",
      "Processing image: N3_105.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 332, 107, 422, 286\n",
      "Processing image: N4_36.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 321, 174, 406, 291\n",
      "Processing image: N1_141.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 341, 99, 433, 350\n",
      "Processing image: N4_135.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 322, 189, 407, 304\n",
      "Bounding box: 406, 156, 559, 312\n",
      "Processing image: N5_324.jpg\n",
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "Running model...\n",
      "Bounding box: 316, 221, 398, 315\n",
      "Bounding box: 182, 203, 314, 345\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 264\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m#run sam\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ran_sam \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     ran_sam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    267\u001b[0m mask, _, _ \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    268\u001b[0m     point_coords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    269\u001b[0m     point_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    270\u001b[0m     box\u001b[38;5;241m=\u001b[39msam_bounding_box,\n\u001b[1;32m    271\u001b[0m     multimask_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/predictor.py:60\u001b[0m, in \u001b[0;36mSamPredictor.set_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     57\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(input_image, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m input_image_torch\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_torch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/predictor.py:89\u001b[0m, in \u001b[0;36mSamPredictor.set_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(transformed_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m     88\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpreprocess(transformed_image)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_image_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:349\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    347\u001b[0m q_h, q_w \u001b[38;5;241m=\u001b[39m q_size\n\u001b[1;32m    348\u001b[0m k_h, k_w \u001b[38;5;241m=\u001b[39m k_size\n\u001b[0;32m--> 349\u001b[0m Rh \u001b[38;5;241m=\u001b[39m \u001b[43mget_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m Rw \u001b[38;5;241m=\u001b[39m get_rel_pos(q_w, k_w, rel_pos_w)\n\u001b[1;32m    352\u001b[0m B, _, dim \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pathtofiles = \"/home/roborregos/Documents/YCB_dataset/ycb_2classes/\" #path to images to process\n",
    "resultspath = \"/home/roborregos/Documents/YCB_dataset/ycb_precut\" #path to save results all ready processed and segmented images\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n",
    "\n",
    "def load_image(image_path):\n",
    "\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")  # load image\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    image, _ = transform(image_pil, None)  # 3, h, w\n",
    "    return image_pil, image\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, cpu_only=False):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_grounding_output(model, image, caption, box_threshold, text_threshold=None, with_logits=True, cpu_only=False, token_spans=None):\n",
    "    assert text_threshold is not None or token_spans is not None, \"text_threshould and token_spans should not be None at the same time!\"\n",
    "    caption = caption.lower()\n",
    "    caption = caption.strip()\n",
    "    if not caption.endswith(\".\"):\n",
    "        caption = caption + \".\"\n",
    "    device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        print(\"Running model...\")\n",
    "        outputs = model(image[None], captions=[caption])\n",
    "    logits = outputs[\"pred_logits\"].sigmoid()[0]  # (nq, 256)\n",
    "    boxes = outputs[\"pred_boxes\"][0]  # (nq, 4)\n",
    "\n",
    "    # filter output\n",
    "    if token_spans is None:\n",
    "        logits_filt = logits.cpu().clone()\n",
    "        boxes_filt = boxes.cpu().clone()\n",
    "        filt_mask = logits_filt.max(dim=1)[0] > box_threshold\n",
    "        logits_filt = logits_filt[filt_mask]  # num_filt, 256\n",
    "        boxes_filt = boxes_filt[filt_mask]  # num_filt, 4\n",
    "\n",
    "        # get phrase\n",
    "        tokenlizer = model.tokenizer\n",
    "        tokenized = tokenlizer(caption)\n",
    "        # build pred\n",
    "        pred_phrases = []\n",
    "        for logit, box in zip(logits_filt, boxes_filt):\n",
    "            pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenlizer)\n",
    "            if with_logits:\n",
    "                pred_phrases.append(pred_phrase + f\"({str(logit.max().item())[:4]})\")\n",
    "            else:\n",
    "                pred_phrases.append(pred_phrase)\n",
    "    else:\n",
    "        # given-phrase mode\n",
    "        positive_maps = create_positive_map_from_span(\n",
    "            model.tokenizer(text_prompt),\n",
    "            token_span=token_spans\n",
    "        ).to(image.device) # n_phrase, 256\n",
    "\n",
    "        logits_for_phrases = positive_maps @ logits.T # n_phrase, nq\n",
    "        all_logits = []\n",
    "        all_phrases = []\n",
    "        all_boxes = []\n",
    "        for (token_span, logit_phr) in zip(token_spans, logits_for_phrases):\n",
    "            # get phrase\n",
    "            phrase = ' '.join([caption[_s:_e] for (_s, _e) in token_span])\n",
    "            # get mask\n",
    "            filt_mask = logit_phr > box_threshold\n",
    "            # filt box\n",
    "            all_boxes.append(boxes[filt_mask])\n",
    "            # filt logits\n",
    "            all_logits.append(logit_phr[filt_mask])\n",
    "            if with_logits:\n",
    "                logit_phr_num = logit_phr[filt_mask]\n",
    "                all_phrases.extend([phrase + f\"({str(logit.item())[:4]})\" for logit in logit_phr_num])\n",
    "            else:\n",
    "                all_phrases.extend([phrase for _ in range(len(filt_mask))])\n",
    "        boxes_filt = torch.cat(all_boxes, dim=0).cpu()\n",
    "        pred_phrases = all_phrases\n",
    "\n",
    "\n",
    "    return boxes_filt, pred_phrases\n",
    "\n",
    "# cfg\n",
    "config_file = \"/home/roborregos/Documents/home-vision/dataset_generator/groundingdino/config/GroundingDINO_SwinT_OGC.py\"  # change the path of the model config file\n",
    "\n",
    "#wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
    "checkpoint_path = \"/home/roborregos/Documents/home-vision/dataset_generator/groundingdino_swint_ogc.pth\"  # change the path of the model\n",
    "text_prompt = \"bannana\"\n",
    "output_dir = resultspath\n",
    "box_threshold = 0.3\n",
    "text_threshold = 0.25\n",
    "token_spans = None\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "sam_model = \"h\"\n",
    "\n",
    "#use sam model\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
    "if sam_model ==\"h\":\n",
    "  sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "  model_type = \"vit_h\"\n",
    "else:\n",
    "  sam_checkpoint = \"sam_vit_l_0b3195.pth\"\n",
    "  model_type = \"vit_l\"\n",
    "\n",
    "device = \"cuda\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "images=[]\n",
    "annotations=[]\n",
    "categories=[]\n",
    "\n",
    "img_id=0\n",
    "anno_id=0\n",
    "\n",
    "#check if results directory exists, else create it\n",
    "if not os.path.exists(resultspath):\n",
    "  os.makedirs(resultspath)\n",
    "\n",
    "#make a list of all the directories in the path\n",
    "pathtoimage = [f.path for f in os.scandir(pathtofiles) if f.is_dir()]\n",
    "\n",
    "#pathtoimage = os.listdir(pathtofiles)\n",
    "\n",
    "for filepath in pathtoimage:\n",
    "    imgPaths = os.listdir(filepath)\n",
    "    print(imgPaths)\n",
    "\n",
    "    i=0\n",
    "\n",
    "    for imgPath in imgPaths:\n",
    "        print(f\"Processing image: {imgPath}\")\n",
    "        img = imutils.resize(cv2.imread(f\"{filepath}/{imgPath}\"))\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "    #------------------------start grounding----------------------------------------------\n",
    "        #image_path = args.image_path\n",
    "\n",
    "        # load image\n",
    "        image_pil, image = load_image(f\"{filepath}/{imgPath}\")\n",
    "\n",
    "        # load model\n",
    "        model = load_model(config_file, checkpoint_path, cpu_only=False)\n",
    "\n",
    "        # set the text_threshold to None if token_spans is set.\n",
    "        if token_spans is not None:\n",
    "            text_threshold = None\n",
    "            print(\"Using token_spans. Set the text_threshold to None.\")\n",
    "\n",
    "        # run model\n",
    "        boxes_filt, pred_phrases = get_grounding_output(\n",
    "            model, image, text_prompt, box_threshold, text_threshold, cpu_only=False, token_spans=eval(f\"{token_spans}\")\n",
    "        )\n",
    "\n",
    "        #found bb dimensions\n",
    "\n",
    "        size = image_pil.size\n",
    "        pred_dict = {\n",
    "            \"boxes\": boxes_filt,\n",
    "            \"size\": [size[1], size[0]],  # H,W\n",
    "            \"labels\": pred_phrases,\n",
    "        }\n",
    "\n",
    "        H, W = pred_dict[\"size\"]\n",
    "        boxes = pred_dict[\"boxes\"]\n",
    "        labels = pred_dict[\"labels\"]\n",
    "        assert len(boxes) == len(labels), \"boxes and labels must have same length\"\n",
    "\n",
    "        draw = ImageDraw.Draw(image_pil)\n",
    "        mask = Image.new(\"L\", image_pil.size, 0)\n",
    "        mask_draw = ImageDraw.Draw(mask)\n",
    "\n",
    "        #change pil image to cv2 image\n",
    "        img = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
    "        img2 = img.copy()\n",
    "        # draw boxes and masks\n",
    "        x0_max = 0\n",
    "        y0_max = 0\n",
    "        x1_min = np.max(np.array(img))\n",
    "        y1_min = np.max(np.array(img))\n",
    "        for box, label in zip(boxes, labels):\n",
    "            # from 0..1 to 0..W, 0..H\n",
    "            box = box * torch.Tensor([W, H, W, H])\n",
    "            # from xywh to xyxy\n",
    "            box[:2] -= box[2:] / 2\n",
    "            box[2:] += box[:2]\n",
    "            # random color\n",
    "            color = tuple(np.random.randint(0, 255, size=1).tolist())\n",
    "            # draw\n",
    "            padding = 10\n",
    "            x0, y0, x1, y1 = box\n",
    "            x0, y0, x1, y1 = int(x0)-padding, int(y0)-padding, int(x1)+padding, int(y1)+padding\n",
    "\n",
    "            #validate if the bounding box is inside the image\n",
    "            if x0 < 0:\n",
    "                x0 = 0\n",
    "            if y0 < 0:\n",
    "                y0 = 0\n",
    "            if x1 > W:\n",
    "                x1 = W\n",
    "            if y1 > H:\n",
    "                y1 = H\n",
    "                \n",
    "            #draw rectangles\n",
    "            cv2.rectangle(img2, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "            draw.rectangle([x0, y0, x1, y1], outline=color, width=6)\n",
    "            # draw.text((x0, y0), str(label), fill=color)\n",
    "\n",
    "            font = ImageFont.load_default()\n",
    "            if hasattr(font, \"getbbox\"):\n",
    "                bbox = draw.textbbox((x0, y0), str(label), font)\n",
    "            else:\n",
    "                w, h = draw.textsize(str(label), font)\n",
    "                bbox = (x0, y0, w + x0, y0 + h)\n",
    "            # bbox = draw.textbbox((x0, y0), str(label))\n",
    "            draw.rectangle(bbox, fill=color)\n",
    "            draw.text((x0, y0), str(label), fill=\"white\")\n",
    "\n",
    "            mask_draw.rectangle([x0, y0, x1, y1], fill=255, width=6)\n",
    "        \n",
    "    # ----------------End grounding ---------------------------------------------------------   \n",
    "        \n",
    "    # ----------------Start SAM--------------------------------------------------------------  \n",
    "            \n",
    "            class_name = filepath.split(\"/\")[-1]\n",
    "            #print x0, y0, x1, y1\n",
    "            print(f\"Bounding box: {x0}, {y0}, {x1}, {y1}\")\n",
    "            \n",
    "            #obtener el mas pequeo de los bounding boxes\n",
    "            \n",
    "            if x0 > x0_max and y0 > y0_max and x1 < x1_min and y1 < y1_min:\n",
    "                x0_max = x0\n",
    "                y0_max = y0\n",
    "                x1_min = x1\n",
    "                y1_min = y1\n",
    "                \n",
    "            sam_bounding_box = np.array([x0, y0, x1, y1])\n",
    "            ran_sam = False\n",
    "            #run sam\n",
    "            if ran_sam == False:\n",
    "                predictor.set_image(img)\n",
    "                ran_sam = True\n",
    "\n",
    "            mask, _, _ = predictor.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=sam_bounding_box,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "\n",
    "            mask, _, _ = predictor.predict(box=sam_bounding_box, multimask_output=False)\n",
    "\n",
    "            #Make png mask\n",
    "            contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # Your call to find the contours\n",
    "\n",
    "            # threshold input image using otsu thresholding as mask and refine with morphology\n",
    "            ret, pngmask = cv2.threshold(mask[0].astype(np.uint8), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU) \n",
    "            kernel = np.ones((9,9), np.uint8)\n",
    "            pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_CLOSE, kernel)\n",
    "            pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_OPEN, kernel)\n",
    "            result = img.copy()\n",
    "            result = cv2.cvtColor(result, cv2.COLOR_BGR2BGRA)\n",
    "            result[:, :, 3] = pngmask                           \n",
    "\n",
    "    # ----------------End SAM-----------------------------------------------------------------  \n",
    "            #cv2.imwrite(f\"{resultspath}/groundingcv2_{imgPath}\", img2)\n",
    "\n",
    "            #image_pil.save(f\"{resultspath}/grounding_{imgPath}\")\n",
    "\n",
    "            if not os.path.exists(f\"{resultspath}/{class_name}\"):\n",
    "                os.mkdir(f\"{resultspath}/{class_name}\")\n",
    "\n",
    "            file_path = f\"{resultspath}/{class_name}/{imgPath[:-4]}.png\"\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "                if x0_max == x0 and y0_max == y0 and x1_min == x1 and y1_min == y1:\n",
    "                    cv2.imwrite(file_path, result)\n",
    "            else:\n",
    "                cv2.imwrite(file_path, result)\n",
    "            i=i+1\n",
    "            ran_sam = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gatos..png started\n",
      "aqui\n",
      "gatos..png done\n",
      "gatos._2.png started\n",
      "aqui\n",
      "gatos._2.png done\n",
      "gatos._1.png started\n",
      "aqui\n",
      "gatos._1.png done\n",
      "jarra3..png started\n",
      "aqui\n",
      "jarra3..png done\n",
      "jarra2..png started\n",
      "aqui\n",
      "jarra2..png done\n",
      "jarra1._2.png started\n",
      "aqui\n",
      "jarra1._2.png done\n",
      "jarra1._1.png started\n",
      "aqui\n",
      "jarra1._1.png done\n",
      "jarra1..png started\n",
      "aqui\n",
      "jarra1..png done\n",
      "lata.png started\n",
      "aqui\n",
      "lata.png done\n",
      "manzana1..png started\n",
      "aqui\n",
      "manzana1..png done\n",
      "manzana2._1.png started\n",
      "aqui\n",
      "manzana2._1.png done\n",
      "manzana1._2.png started\n",
      "aqui\n",
      "manzana1._2.png done\n",
      "manzana2._2.png started\n",
      "aqui\n",
      "manzana2._2.png done\n",
      "manzana1._1.png started\n",
      "aqui\n",
      "manzana1._1.png done\n",
      "manzana2..png started\n",
      "aqui\n",
      "manzana2..png done\n",
      "platano3..png started\n",
      "aqui\n",
      "platano3..png done\n",
      "platano3._1.png started\n",
      "aqui\n",
      "platano3._1.png done\n",
      "platano1._1.png started\n",
      "aqui\n",
      "platano1._1.png done\n",
      "platano2..png started\n",
      "aqui\n",
      "platano2..png done\n",
      "platano2._1.png started\n",
      "aqui\n",
      "platano2._1.png done\n",
      "platano1..png started\n",
      "aqui\n",
      "platano1..png done\n",
      "silla.png started\n",
      "aqui\n",
      "silla.png done\n",
      "silla_1.png started\n",
      "aqui\n",
      "silla_1.png done\n",
      "silla_2.png started\n",
      "aqui\n",
      "silla_2.png done\n",
      "all done\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "datasetpath = \"/home/jabv/Desktop/prueba/res/\"\n",
    "resultspath = \"/home/jabv/Desktop/prueba/DS_res/\"\n",
    "\n",
    "#create a result folder if it doesn't exist\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n",
    "    os.makedirs(resultspath+\"gatos/\")\n",
    "    os.makedirs(resultspath+\"jarra/\")\n",
    "    os.makedirs(resultspath+\"lata/\")\n",
    "    os.makedirs(resultspath+\"manzana/\")\n",
    "    os.makedirs(resultspath+\"platano/\")\n",
    "    os.makedirs(resultspath+\"silla/\")\n",
    "\n",
    "\n",
    "fg_folders = [\n",
    "    (\"gatos/\"),\n",
    "    (\"jarra/\"),\n",
    "    (\"lata/\"),\n",
    "    (\"manzana/\"),\n",
    "    (\"platano/\"),\n",
    "    (\"silla/\")\n",
    "]\n",
    "\n",
    "for folder in fg_folders:\n",
    "    for filename in os.listdir(f\"{datasetpath}{folder}\"):\n",
    "        try:\n",
    "            print(f\"{filename} started\")\n",
    "            myImage = Image.open(datasetpath+folder+filename)\n",
    "            black = Image.new('RGBA', myImage.size)\n",
    "            myImage = Image.composite(myImage, black, myImage)\n",
    "            #print(\"aqui\")\n",
    "            myCroppedImage = myImage.crop(myImage.getbbox())\n",
    "            myCroppedImage.save(f\"{resultspath}{folder}{filename}\")\n",
    "            print(f\"{filename} done\")\n",
    "        except:\n",
    "            print(f\"{filename} failed\")\n",
    "            continue\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the three folders containing the images\n",
    "datasetPath = \"/home/jabv/Desktop/prueba/DS_res\"\n",
    "fg_folders = [\n",
    "    (f\"{datasetPath}/gatos/\",\"gatos\" ),\n",
    "    (f\"{datasetPath}/jarra/\",\"jarra\" ),\n",
    "    (f\"{datasetPath}/lata/\",\"lata\" ),\n",
    "    (f\"{datasetPath}/manzana/\",\"manzana\" ),\n",
    "    (f\"{datasetPath}/platano/\",\"platano\" ),\n",
    "    (f\"{datasetPath}/silla/\",\"silla\" )\n",
    "\n",
    "]\n",
    "bg_folder = \"/home/jabv/Desktop/prueba/bg/\"\n",
    "output_folder = \"/home/jabv/Desktop/prueba/ds_final/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gatos': 0, 'jarra': 1, 'lata': 2, 'manzana': 3, 'platano': 4, 'silla': 5}\n",
      "[{'id': 0, 'name': 'gatos'}, {'id': 1, 'name': 'jarra'}, {'id': 2, 'name': 'lata'}, {'id': 3, 'name': 'manzana'}, {'id': 4, 'name': 'platano'}, {'id': 5, 'name': 'silla'}]\n"
     ]
    }
   ],
   "source": [
    "objects_list = [\"gatos\", \"jarra\", \"lata\", \"manzana\", \"platano\", \"silla\"]\n",
    "annotations_ID = {}\n",
    "categories = []\n",
    "for i, object in enumerate(objects_list):\n",
    "    annotations_ID[object] = i\n",
    "    categories.append({\"id\": i, \"name\": object})\n",
    "\n",
    "print(annotations_ID)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of files in each of the three folders\n",
    "fg_files = {}\n",
    "for folder, category in fg_folders:\n",
    "    fg_files[category] = os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "trainfolder = output_folder + \"train/\"\n",
    "testfolder = output_folder + \"test/\"\n",
    "validfolder = output_folder + \"valid/\"\n",
    "os.mkdir(trainfolder)\n",
    "os.mkdir(testfolder)\n",
    "os.mkdir(validfolder)\n",
    "os.mkdir(trainfolder + \"images/\")\n",
    "os.mkdir(trainfolder + \"labels/\")\n",
    "os.mkdir(testfolder + \"images/\")\n",
    "os.mkdir(testfolder + \"labels/\")\n",
    "os.mkdir(validfolder + \"images/\")\n",
    "os.mkdir(validfolder + \"labels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\n",
    "annotations=[]\n",
    "annotations2=[]\n",
    "annot_csv=[]\n",
    "\n",
    "img_id=int(0)\n",
    "anno_id=int(0)\n",
    "\n",
    "rescaling_min = 0.20\n",
    "rescaling_max = 0.70\n",
    "\n",
    "# Ratios at which these values will be modified\n",
    "brightness_ratio = 0.05\n",
    "saturation_ratio = 0.05\n",
    "hue_ratio = 0.02\n",
    "\n",
    "for j in range(20000):\n",
    "    #create empty label file\n",
    "    with open(f'{trainfolder}labels/{img_id}.txt', 'w') as file:\n",
    "        pass\n",
    "    #select hramdomly how many objects will be in an image\n",
    "    num_objects = random.randint(0, 5)\n",
    "    #print(\"number of objects\",num_objects)\n",
    "    # Select random foreground images from the three folders, with replacement\n",
    "    fg_categories = random.choices(objects_list, k=num_objects)\n",
    "    \n",
    "    fg_files_selected = []\n",
    "    for category in fg_categories:\n",
    "        fg_files_selected.append([category,random.choice(fg_files[category])])\n",
    "    #print(\"seleccion\",fg_files_selected)\n",
    "    # Load the selected foreground images using Pillow\n",
    "    fg_imgs = []\n",
    "    for img in fg_files_selected:\n",
    "        folder = [f[0] for f in fg_folders if f[1] == img[0]][0]\n",
    "        fg_imgs.append([img[0],Image.open(folder + img[1]),folder+img[1]])\n",
    "\n",
    "    # Randomly resize and rotate the foreground images using Pillow's transform module\n",
    "    # img[0] is category, img[1] is image, img[2] is path\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "        angle = random.randint(-5, 5)\n",
    "        scale = random.uniform(rescaling_min, rescaling_max)\n",
    "        fg_img = fg_img.rotate(angle, resample=Image.BICUBIC, expand=True)\n",
    "        fg_img = fg_img.resize((int(fg_img.width * scale), int(fg_img.height * scale)))\n",
    "        fg_img = ImageEnhance.Brightness(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = ImageEnhance.Contrast(fg_img).enhance(random.uniform(0.9, 1.1))\n",
    "        fg_img = ImageEnhance.Color(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = fg_img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.0, 0.5)))\n",
    "\n",
    "\n",
    "        img[1] = fg_img\n",
    "\n",
    "    # Load the background image using Pillow\n",
    "    bg_files = os.listdir(bg_folder)\n",
    "    bg_file = random.choice(bg_files)\n",
    "    bg_img = Image.open(bg_folder + bg_file)\n",
    "\n",
    "    # Define the maximum overlap as a percentage\n",
    "    max_overlap_pct = 10\n",
    "\n",
    "    # Define an array to keep track of occupied areas\n",
    "    occupied = np.zeros((bg_img.height, bg_img.width))\n",
    "\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "\n",
    "        # Calculate the maximum overlap area\n",
    "        max_overlap_area = (fg_img.width * fg_img.height)\n",
    "\n",
    "        seg_img = img[1]\n",
    "\n",
    "\n",
    "        # Convert the image to a NumPy array\n",
    "        img_arr = np.array(seg_img)\n",
    "        # Create a binary mask of the non-transparent pixels\n",
    "        mask = img_arr[:, :, 3] != 0\n",
    "\n",
    "        # Convert the mask to a COCO format segmentation\n",
    "        segmentation = []\n",
    "        for i in range(mask.shape[0]):\n",
    "            for j in range(mask.shape[1]):\n",
    "                if mask[i, j]:\n",
    "                    segmentation.append(j)\n",
    "                    segmentation.append(i)\n",
    "        segmentation = [segmentation]\n",
    "\n",
    "        # Calculate the area of the segmentation\n",
    "        area = 0\n",
    "        for i in range(len(segmentation[0]) // 2):\n",
    "            x1 = segmentation[0][2 * i]\n",
    "            y1 = segmentation[0][2 * i + 1]\n",
    "            x2 = segmentation[0][(2 * i + 2) % len(segmentation[0])]\n",
    "            y2 = segmentation[0][(2 * i + 3) % len(segmentation[0])]\n",
    "            area += x1 * y2 - x2 * y1\n",
    "        area = abs(area) / 2\n",
    "        \n",
    "        # Draw the segmentation onto a copy of the original image\n",
    "        #image_copy = image.copy()\n",
    "        #cv2.fillPoly(image_copy, aux_segmentation, color=(0, 255, 0))\n",
    "\n",
    "        # Display the image with segmentation overlay\n",
    "        #cv2.imshow('Image with Segmentation', image_copy)\n",
    "        #cv2.waitKey(0)\n",
    "        #cv2.destroyAllWindows()\n",
    "\n",
    "        # Calculate the maximum allowed position for the top-left corner\n",
    "        max_x = bg_img.width - fg_img.width\n",
    "        max_y = bg_img.height - fg_img.height\n",
    "        area = fg_img.width * fg_img.height\n",
    "\n",
    "        # Generate a random location until an unoccupied area is found that meets the overlap limit\n",
    "        total_area = bg_img.width * bg_img.height\n",
    "        overlap_area = total_area\n",
    "        \n",
    "        while overlap_area / area > max_overlap_pct / 100:\n",
    "            try:\n",
    "                x = random.randint(0, max_x)\n",
    "                y = random.randint(0, max_y)\n",
    "            except:\n",
    "                x = random.randint(0, abs(max_x))\n",
    "                y = random.randint(0, abs(max_y))\n",
    "\n",
    "            # Calculate the overlap area\n",
    "            overlap_area = np.sum(occupied[y:y+fg_img.height, x:x+fg_img.width])\n",
    "\n",
    "            # Check if the area is unoccupied and the overlap limit is not exceeded\n",
    "            if (max_overlap_area) >= overlap_area/10:\n",
    "                break\n",
    "            if i==10:\n",
    "                continue\n",
    "        \n",
    "        for i in range(0, len(segmentation[0])):\n",
    "            if i % 2:\n",
    "                segmentation[0][i]=int(segmentation[0][i]+y)\n",
    "            else :\n",
    "                segmentation[0][i]=int(segmentation[0][i]+x)\n",
    "        # Update the occupied array\n",
    "        occupied[y:y+fg_img.height, x:x+fg_img.width] = 1\n",
    "\n",
    "        bg_img.paste(fg_img, (x, y), fg_img)\n",
    "        x_center_ann = (x+fg_img.width/2) / bg_img.width\n",
    "        y_center_ann = (y+fg_img.height/2) / bg_img.height\n",
    "        width_ann = fg_img.width / bg_img.width\n",
    "        height_ann = fg_img.height / bg_img.height\n",
    "        with open(f'{trainfolder}labels/{img_id}.txt', 'a') as f:\n",
    "            f.write(f\"{annotations_ID[img[0]]} {x_center_ann} {y_center_ann} {width_ann} {height_ann}\\n\")\n",
    "        annotations2.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": segmentation,\"area\": area,\"iscrowd\": 0})\n",
    "        annotations.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": [],\"area\": fg_img.height*fg_img.width,\"iscrowd\": 0})\n",
    "        annot_csv.append([\"TRAIN\", output_folder + str(img_id)+\".jpg\", img[0], x/bg_img.width, y/bg_img.height,\"\",\"\",(x+fg_img.width)/bg_img.width, (y+fg_img.height)/bg_img.height])\n",
    "        anno_id=anno_id+1\n",
    "        #draw = ImageDraw.Draw(bg_img)\n",
    "        #fdraw.rectangle((x, y, x+fg_img.width, y+fg_img.height), outline='red', width=3)\n",
    "    bg_img.save(f\"{trainfolder}images/\"+str(img_id)+\".jpg\", quality=100)\n",
    "    images.append({\"id\": img_id, \"file_name\": str(img_id)+\".jpg\",\"height\": bg_img.height,\"width\": bg_img.width})\n",
    "    img_id=img_id+1\n",
    "    #print(img_id)\n",
    "\n",
    "#making data.yaml\n",
    "data = dict(\n",
    "    train = f\"{trainfolder}images\",\n",
    "    val = f\"{validfolder}images\",\n",
    "    test = f\"{validfolder}images\",\n",
    "    nc = len(annotations_ID),\n",
    "    names = list(annotations_ID.keys())\n",
    "    )\n",
    "#storing\n",
    "with open(f'{output_folder}data.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SplitTrainValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "validation = 0.1\n",
    "test = 0.1\n",
    "\n",
    "# Assumes test has 100% of data\n",
    "output_folder = \"/home/jabv/Desktop/prueba/ds_final/\"\n",
    "trainfolder = output_folder + \"train/\"\n",
    "trainfolderimgs = trainfolder + \"images/\"\n",
    "trainfolderlabels = trainfolder + \"labels/\"\n",
    "testfolder = output_folder + \"test/\"\n",
    "testfolderimgs = testfolder + \"images/\"\n",
    "testfolderlabels = testfolder + \"labels/\"\n",
    "validfolder = output_folder + \"valid/\"\n",
    "validfolderimgs = validfolder + \"images/\"\n",
    "validfolderlabels = validfolder + \"labels/\"\n",
    "\n",
    "fullSize = len(os.listdir(trainfolderimgs))\n",
    "validSize = int(fullSize * validation)\n",
    "testSize = int(fullSize * test)\n",
    "\n",
    "for i in range(validSize):\n",
    "    filelist = os.listdir(trainfolderimgs)\n",
    "    #randomize file list, to not pick files in order\n",
    "    random.shuffle(filelist)\n",
    "    filetomove = filelist[i]\n",
    "    #take out .jpg, .png, etc\n",
    "    filetomovename = filetomove[:-4]\n",
    "    #move images\n",
    "    shutil.move(f\"{trainfolderimgs}{filetomove}\", f\"{validfolderimgs}{filetomove}\")\n",
    "    #move labels\n",
    "    shutil.move(f\"{trainfolderlabels}{filetomovename}.txt\", f\"{validfolderlabels}{filetomovename}.txt\")\n",
    "for i in range(testSize):\n",
    "    filetomove = os.listdir(trainfolderimgs)[i]\n",
    "    #take out .jpg, .png, etc\n",
    "    filetomovename = filetomove[:-4]\n",
    "    #move images\n",
    "    shutil.move(f\"{trainfolderimgs}{filetomove}\", f\"{testfolderimgs}{filetomove}\")\n",
    "    #move labels\n",
    "    shutil.move(f\"{trainfolderlabels}{filetomovename}.txt\", f\"{testfolderlabels}{filetomovename}.txt\")\n",
    "\n",
    "#Validation\n",
    "print(f\"Train size is now: {len(os.listdir(trainfolderimgs))}\")\n",
    "print(f\"Validation size is now: {len(os.listdir(validfolderimgs))}\")\n",
    "print(f\"Test size is now: {len(os.listdir(testfolderimgs))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
