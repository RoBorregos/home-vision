{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageEnhance, ImageFilter\n",
    "from pycocotools import mask\n",
    "import json\n",
    "import yaml\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ultralytics\n",
    "import time\n",
    "import imutils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto label con Segment anyting y modelo de YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathtoimage = \"dataset_modelo\" #path to images to process\n",
    "resultspath = \"DS_letras_cut\" #path to save results all ready processed and segmented images\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n",
    "\n",
    "def generate_boxes_confidences_classids_v8(outs, threshold):\n",
    "\t\tboxes = []\n",
    "\t\tconfidences = []\n",
    "\t\tclassids = []\n",
    "\n",
    "\t\tfor out in outs:\n",
    "\t\t\t\tfor box in out.boxes:\n",
    "\t\t\t\t\tx1, y1, x2, y2 = [round(x) for x in box.xyxy[0].tolist()]\n",
    "\t\t\t\t\tclass_id = box.cls[0].item()\n",
    "\t\t\t\t\tprob = round(box.conf[0].item(), 2)\n",
    "\t\t\t\t\tif prob > threshold:\n",
    "\t\t\t\t\t\t# Append to list\n",
    "\t\t\t\t\t\tboxes.append([x1, y1, x2-x1, y2-y1])\n",
    "\t\t\t\t\t\tconfidences.append(float(prob))\n",
    "\t\t\t\t\t\tclassids.append(class_id)\n",
    "\t\n",
    "\t\treturn boxes, confidences, classids\n",
    "\n",
    "def yolov8_warmup(model, repetitions=1, verbose=False):\n",
    "    # Warmup model\n",
    "    startTime = time.time()\n",
    "    # create an empty frame to warmup the model\n",
    "    for i in range(repetitions):\n",
    "        warmupFrame = np.zeros((360, 640, 3), dtype=np.uint8)\n",
    "        model.predict(source=warmupFrame, verbose=verbose)\n",
    "    print(f\"Model warmed up in {time.time() - startTime} seconds\")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "\n",
    "#prevtime = time.time()\n",
    "model = ultralytics.YOLO(\"/home/jabv/Desktop/larc30000_yolov87/weights/letras_colores_v2.pt\") #change to your model path\n",
    "yolov8_warmup(model, 10, False)\n",
    "#print(f\"Model loaded in {time.time() - prevtime} seconds\")\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "sam_model = \"h\"\n",
    "\n",
    "#use sam model\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
    "if sam_model ==\"h\":\n",
    "  sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "  model_type = \"vit_h\"\n",
    "else:\n",
    "  sam_checkpoint = \"sam_vit_l_0b3195.pth\"\n",
    "  model_type = \"vit_l\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Creating annotation in COCO format\n",
    "#{\"id\": 0, \"file_name\": \"0.jpg\", \"height\": 480, \"width\": 736}\n",
    "images=[]\n",
    "annotations=[]\n",
    "categories=[]\n",
    "\n",
    "img_id=0\n",
    "anno_id=0\n",
    "\n",
    "#check if results directory exists, else create it\n",
    "if not os.path.exists(resultspath):\n",
    "  os.makedirs(resultspath)\n",
    "\n",
    "\n",
    "imgPaths = os.listdir(pathtoimage)\n",
    "print(imgPaths)\n",
    "\n",
    "i=0\n",
    "\n",
    "for imgPath in imgPaths:\n",
    "    print(f\"Processing image: {imgPath}\")\n",
    "    img = imutils.resize(cv2.imread(f\"{pathtoimage}/{imgPath}\"))\n",
    "    if img is None:\n",
    "        continue\n",
    "    #results = model(img)\n",
    "    results = model(img, verbose=False)\n",
    "    boxes, confidences, classids = generate_boxes_confidences_classids_v8(results, 0.85)\n",
    "    for i in range(len(boxes)):\n",
    "        x,y,w,h = boxes[i]\n",
    "        class_name = model.model.names[classids[i]]\n",
    "        ran_sam = False\n",
    "\n",
    "        sam_bounding_box = (np.array([x, y, x+w, y+h]))\n",
    "\n",
    "        #run sam\n",
    "        if ran_sam == False:\n",
    "            predictor.set_image(img)\n",
    "            ran_sam = True\n",
    "\n",
    "        mask, _, _ = predictor.predict(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            box=sam_bounding_box,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # Your call to find the contours\n",
    "        # threshold input image using otsu thresholding as mask and refine with morphology\n",
    "        ret, pngmask = cv2.threshold(mask[0].astype(np.uint8), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU) \n",
    "        kernel = np.ones((9,9), np.uint8)\n",
    "        pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_CLOSE, kernel)\n",
    "        pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # put mask into alpha channel of result\n",
    "        result = img.copy()\n",
    "        result = cv2.cvtColor(result, cv2.COLOR_BGR2BGRA)\n",
    "        result[:, :, 3] = pngmask\n",
    "        #cv2.imwrite(f\"{resultspath}/{cutobject}_{i}.png\", result)\n",
    "        #to save with same name as original file\n",
    "        # if already exists, save with _1, _2, etc\n",
    "\n",
    "        if not os.path.exists(f\"{resultspath}/{class_name}\"):\n",
    "            os.mkdir(f\"{resultspath}/{class_name}\")\n",
    "\n",
    "        if os.path.exists(f\"{resultspath}/{class_name}/{imgPath[:-4]}.png\"):\n",
    "            if os.path.exists(f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\"):\n",
    "                print(\"File already exists, saving with _2\")\n",
    "                cv2.imwrite(f\"{resultspath}/{class_name}/{imgPath[:-4]}_2.png\", result)\n",
    "            print(\"File already exists, saving with _1\")\n",
    "            temppath = f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\"\n",
    "            print(temppath)\n",
    "            cv2.imwrite(f\"{resultspath}/{class_name}/{imgPath[:-4]}_1.png\", result)\n",
    "\n",
    "        cv2.imwrite(f\"{resultspath}/{class_name}/{imgPath[:-4]}.png\", result)\n",
    "        i=i+1\n",
    "        ran_sam = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "datasetpath = \"DS_letras_cut/\"\n",
    "resultspath = \"DS_letras/\"\n",
    "\n",
    "#create a result folder if it doesn't exist\n",
    "if not os.path.exists(resultspath):\n",
    "    os.makedirs(resultspath)\n",
    "    os.makedirs(resultspath+\"A/\")\n",
    "    os.makedirs(resultspath+\"B/\")\n",
    "    os.makedirs(resultspath+\"C/\")\n",
    "    os.makedirs(resultspath+\"D/\")\n",
    "    os.makedirs(resultspath+\"E/\")\n",
    "    os.makedirs(resultspath+\"F/\")\n",
    "    os.makedirs(resultspath+\"G/\")\n",
    "    os.makedirs(resultspath+\"H/\")\n",
    "    os.makedirs(resultspath+\"I/\")\n",
    "    os.makedirs(resultspath+\"verde/\")\n",
    "    os.makedirs(resultspath+\"amarillo/\")\n",
    "    os.makedirs(resultspath+\"azul/\")\n",
    "    os.makedirs(resultspath+\"rojo/\")\n",
    "\n",
    "fg_folders = [\n",
    "    (\"A/\"),\n",
    "    (\"B/\"),\n",
    "    (\"C/\"),\n",
    "    (\"D/\"),\n",
    "    (\"E/\"),\n",
    "    (\"F/\"),\n",
    "    (\"G/\"),\n",
    "    (\"H/\"),\n",
    "    (\"I/\"),\n",
    "    (\"verde/\"),\n",
    "    (\"amarillo/\"),\n",
    "    (\"azul/\"),\n",
    "    (\"rojo/\")\n",
    "]\n",
    "\n",
    "for folder in fg_folders:\n",
    "    for filename in os.listdir(f\"{datasetpath}{folder}\"):\n",
    "        try:\n",
    "            print(f\"{filename} started\")\n",
    "            myImage = Image.open(datasetpath+folder+filename)\n",
    "            black = Image.new('RGBA', myImage.size)\n",
    "            myImage = Image.composite(myImage, black, myImage)\n",
    "            print(\"aqui\")\n",
    "            myCroppedImage = myImage.crop(myImage.getbbox())\n",
    "            myCroppedImage.save(f\"{resultspath}{folder}{filename}\")\n",
    "            print(f\"{filename} done\")\n",
    "        except:\n",
    "            print(f\"{filename} failed\")\n",
    "            continue\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the three folders containing the images\n",
    "datasetPath = \"/home/jabv/Desktop/DS_letras_cropped\"\n",
    "fg_folders = [\n",
    "    (f\"{datasetPath}/A/\",\"A\" ),\n",
    "    (f\"{datasetPath}/B/\",\"B\" ),\n",
    "    (f\"{datasetPath}/C/\",\"C\" ),\n",
    "    (f\"{datasetPath}/D/\",\"D\" ),\n",
    "    (f\"{datasetPath}/E/\",\"E\" ),\n",
    "    (f\"{datasetPath}/F/\",\"F\" ),\n",
    "    (f\"{datasetPath}/G/\",\"G\" ),\n",
    "    (f\"{datasetPath}/H/\",\"H\" ),\n",
    "    (f\"{datasetPath}/I/\",\"I\" ),\n",
    "    (f\"{datasetPath}/verde/\",\"verde\" ),\n",
    "    (f\"{datasetPath}/azul/\",\"azul\" ),\n",
    "    (f\"{datasetPath}/amarillo/\",\"amarillo\" ),\n",
    "    (f\"{datasetPath}/rojo/\",\"rojo\" )\n",
    "]\n",
    "bg_folder = \"/home/jabv/Desktop/bg_imgs/\"\n",
    "output_folder = \"/home/jabv/Desktop/DS_letras/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'verde': 9, 'azul': 10, 'amarillo': 11, 'rojo': 12}\n",
      "[{'id': 0, 'name': 'A'}, {'id': 1, 'name': 'B'}, {'id': 2, 'name': 'C'}, {'id': 3, 'name': 'D'}, {'id': 4, 'name': 'E'}, {'id': 5, 'name': 'F'}, {'id': 6, 'name': 'G'}, {'id': 7, 'name': 'H'}, {'id': 8, 'name': 'I'}, {'id': 9, 'name': 'verde'}, {'id': 10, 'name': 'azul'}, {'id': 11, 'name': 'amarillo'}, {'id': 12, 'name': 'rojo'}]\n"
     ]
    }
   ],
   "source": [
    "objects_list = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"verde\", \"azul\", \"amarillo\", \"rojo\"]\n",
    "annotations_ID = {}\n",
    "categories = []\n",
    "for i, object in enumerate(objects_list):\n",
    "    annotations_ID[object] = i\n",
    "    categories.append({\"id\": i, \"name\": object})\n",
    "\n",
    "print(annotations_ID)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of files in each of the three folders\n",
    "fg_files = {}\n",
    "for folder, category in fg_folders:\n",
    "    fg_files[category] = os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(output_folder)\n",
    "trainfolder = output_folder + \"train/\"\n",
    "testfolder = output_folder + \"test/\"\n",
    "validfolder = output_folder + \"valid/\"\n",
    "os.mkdir(trainfolder)\n",
    "os.mkdir(testfolder)\n",
    "os.mkdir(validfolder)\n",
    "os.mkdir(trainfolder + \"images/\")\n",
    "os.mkdir(trainfolder + \"labels/\")\n",
    "os.mkdir(testfolder + \"images/\")\n",
    "os.mkdir(testfolder + \"labels/\")\n",
    "os.mkdir(validfolder + \"images/\")\n",
    "os.mkdir(validfolder + \"labels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\n",
    "annotations=[]\n",
    "annotations2=[]\n",
    "annot_csv=[]\n",
    "\n",
    "img_id=int(0)\n",
    "anno_id=int(0)\n",
    "\n",
    "rescaling_min = 0.20\n",
    "rescaling_max = 0.70\n",
    "\n",
    "# Ratios at which these values will be modified\n",
    "brightness_ratio = 0.05\n",
    "saturation_ratio = 0.05\n",
    "hue_ratio = 0.02\n",
    "\n",
    "for j in range(20000):\n",
    "    #create empty label file\n",
    "    with open(f'{trainfolder}labels/{img_id}.txt', 'w') as file:\n",
    "        pass\n",
    "    #select hramdomly how many objects will be in an image\n",
    "    num_objects = random.randint(0, 5)\n",
    "    #print(\"number of objects\",num_objects)\n",
    "    # Select random foreground images from the three folders, with replacement\n",
    "    fg_categories = random.choices(objects_list, k=num_objects)\n",
    "    \n",
    "    fg_files_selected = []\n",
    "    for category in fg_categories:\n",
    "        fg_files_selected.append([category,random.choice(fg_files[category])])\n",
    "    #print(\"seleccion\",fg_files_selected)\n",
    "    # Load the selected foreground images using Pillow\n",
    "    fg_imgs = []\n",
    "    for img in fg_files_selected:\n",
    "        folder = [f[0] for f in fg_folders if f[1] == img[0]][0]\n",
    "        fg_imgs.append([img[0],Image.open(folder + img[1]),folder+img[1]])\n",
    "\n",
    "    # Randomly resize and rotate the foreground images using Pillow's transform module\n",
    "    # img[0] is category, img[1] is image, img[2] is path\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "        angle = random.randint(-5, 5)\n",
    "        scale = random.uniform(rescaling_min, rescaling_max)\n",
    "        fg_img = fg_img.rotate(angle, resample=Image.BICUBIC, expand=True)\n",
    "        fg_img = fg_img.resize((int(fg_img.width * scale), int(fg_img.height * scale)))\n",
    "        fg_img = ImageEnhance.Brightness(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = ImageEnhance.Contrast(fg_img).enhance(random.uniform(0.9, 1.1))\n",
    "        fg_img = ImageEnhance.Color(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = fg_img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.0, 0.5)))\n",
    "\n",
    "\n",
    "        img[1] = fg_img\n",
    "\n",
    "    # Load the background image using Pillow\n",
    "    bg_files = os.listdir(bg_folder)\n",
    "    bg_file = random.choice(bg_files)\n",
    "    bg_img = Image.open(bg_folder + bg_file)\n",
    "\n",
    "    # Define the maximum overlap as a percentage\n",
    "    max_overlap_pct = 10\n",
    "\n",
    "    # Define an array to keep track of occupied areas\n",
    "    occupied = np.zeros((bg_img.height, bg_img.width))\n",
    "\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "\n",
    "        # Calculate the maximum overlap area\n",
    "        max_overlap_area = (fg_img.width * fg_img.height)\n",
    "\n",
    "        seg_img = img[1]\n",
    "\n",
    "\n",
    "        # Convert the image to a NumPy array\n",
    "        img_arr = np.array(seg_img)\n",
    "        # Create a binary mask of the non-transparent pixels\n",
    "        mask = img_arr[:, :, 3] != 0\n",
    "\n",
    "        # Convert the mask to a COCO format segmentation\n",
    "        segmentation = []\n",
    "        for i in range(mask.shape[0]):\n",
    "            for j in range(mask.shape[1]):\n",
    "                if mask[i, j]:\n",
    "                    segmentation.append(j)\n",
    "                    segmentation.append(i)\n",
    "        segmentation = [segmentation]\n",
    "\n",
    "        # Calculate the area of the segmentation\n",
    "        area = 0\n",
    "        for i in range(len(segmentation[0]) // 2):\n",
    "            x1 = segmentation[0][2 * i]\n",
    "            y1 = segmentation[0][2 * i + 1]\n",
    "            x2 = segmentation[0][(2 * i + 2) % len(segmentation[0])]\n",
    "            y2 = segmentation[0][(2 * i + 3) % len(segmentation[0])]\n",
    "            area += x1 * y2 - x2 * y1\n",
    "        area = abs(area) / 2\n",
    "        \n",
    "        # Draw the segmentation onto a copy of the original image\n",
    "        #image_copy = image.copy()\n",
    "        #cv2.fillPoly(image_copy, aux_segmentation, color=(0, 255, 0))\n",
    "\n",
    "        # Display the image with segmentation overlay\n",
    "        #cv2.imshow('Image with Segmentation', image_copy)\n",
    "        #cv2.waitKey(0)\n",
    "        #cv2.destroyAllWindows()\n",
    "\n",
    "        # Calculate the maximum allowed position for the top-left corner\n",
    "        max_x = bg_img.width - fg_img.width\n",
    "        max_y = bg_img.height - fg_img.height\n",
    "        area = fg_img.width * fg_img.height\n",
    "\n",
    "        # Generate a random location until an unoccupied area is found that meets the overlap limit\n",
    "        total_area = bg_img.width * bg_img.height\n",
    "        overlap_area = total_area\n",
    "        \n",
    "        while overlap_area / area > max_overlap_pct / 100:\n",
    "            try:\n",
    "                x = random.randint(0, max_x)\n",
    "                y = random.randint(0, max_y)\n",
    "            except:\n",
    "                x = random.randint(0, abs(max_x))\n",
    "                y = random.randint(0, abs(max_y))\n",
    "\n",
    "            # Calculate the overlap area\n",
    "            overlap_area = np.sum(occupied[y:y+fg_img.height, x:x+fg_img.width])\n",
    "\n",
    "            # Check if the area is unoccupied and the overlap limit is not exceeded\n",
    "            if (max_overlap_area) >= overlap_area/10:\n",
    "                break\n",
    "            if i==10:\n",
    "                continue\n",
    "        \n",
    "        for i in range(0, len(segmentation[0])):\n",
    "            if i % 2:\n",
    "                segmentation[0][i]=int(segmentation[0][i]+y)\n",
    "            else :\n",
    "                segmentation[0][i]=int(segmentation[0][i]+x)\n",
    "        # Update the occupied array\n",
    "        occupied[y:y+fg_img.height, x:x+fg_img.width] = 1\n",
    "\n",
    "        bg_img.paste(fg_img, (x, y), fg_img)\n",
    "        x_center_ann = (x+fg_img.width/2) / bg_img.width\n",
    "        y_center_ann = (y+fg_img.height/2) / bg_img.height\n",
    "        width_ann = fg_img.width / bg_img.width\n",
    "        height_ann = fg_img.height / bg_img.height\n",
    "        with open(f'{trainfolder}labels/{img_id}.txt', 'a') as f:\n",
    "            f.write(f\"{annotations_ID[img[0]]} {x_center_ann} {y_center_ann} {width_ann} {height_ann}\\n\")\n",
    "        annotations2.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": segmentation,\"area\": area,\"iscrowd\": 0})\n",
    "        annotations.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": [],\"area\": fg_img.height*fg_img.width,\"iscrowd\": 0})\n",
    "        annot_csv.append([\"TRAIN\", output_folder + str(img_id)+\".jpg\", img[0], x/bg_img.width, y/bg_img.height,\"\",\"\",(x+fg_img.width)/bg_img.width, (y+fg_img.height)/bg_img.height])\n",
    "        anno_id=anno_id+1\n",
    "        #draw = ImageDraw.Draw(bg_img)\n",
    "        #fdraw.rectangle((x, y, x+fg_img.width, y+fg_img.height), outline='red', width=3)\n",
    "    bg_img.save(f\"{trainfolder}images/\"+str(img_id)+\".jpg\", quality=100)\n",
    "    images.append({\"id\": img_id, \"file_name\": str(img_id)+\".jpg\",\"height\": bg_img.height,\"width\": bg_img.width})\n",
    "    img_id=img_id+1\n",
    "    #print(img_id)\n",
    "\n",
    "#making data.yaml\n",
    "data = dict(\n",
    "    train = f\"{trainfolder}images\",\n",
    "    val = f\"{validfolder}images\",\n",
    "    test = f\"{validfolder}images\",\n",
    "    nc = len(annotations_ID),\n",
    "    names = list(annotations_ID.keys())\n",
    "    )\n",
    "#storing\n",
    "with open(f'{output_folder}data.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
